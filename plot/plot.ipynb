{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pyasn\n",
    "\n",
    "from ipaddress import ip_network\n",
    "from clickhouse_driver import Client\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from scripts.utils.file_utils import load_json\n",
    "from scripts.analysis.analysis import compute_error_threshold_cdfs, every_tier_result_and_errors, get_all_bgp_prefixes, is_same_bgp_prefix\n",
    "from scripts.utils.plot_utils import plot_multiple_cdf, homogenize_legend, plot_save, plot_multiple_error_bars, plot_scatter_multiple\n",
    "from scripts.utils.clickhouse_utils import get_min_rtt_per_src_dst_query_ping_table\n",
    "from scripts.utils.helpers import haversine, rtt_to_km\n",
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = load_json(ANCHORS_FILE)\n",
    "\n",
    "probes = load_json(PROBES_FILE)\n",
    "\n",
    "all_probes = load_json(PROBES_AND_ANCHORS_FILE)\n",
    "\n",
    "removed_probes = load_json(REMOVED_PROBES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing probes to anchors\n",
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n",
      "711\n"
     ]
    }
   ],
   "source": [
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "\n",
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "    errors_threshold_probes_to_anchors)\n",
    "\n",
    "Ys = error_threshold_cdfs_p_to_a\n",
    "print(len(error_threshold_cdfs_p_to_a[0]))\n",
    "labels = [\"All VPs\"]\n",
    "labels.extend([f\"VPs > {t} km\" for t in THRESHOLD_DISTANCES if t > 0])\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\", legend_size=12)\n",
    "\n",
    "ofile = CBG_THRESHOLD_PROBES_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0 no geolocation 33\n",
      "Threshold 0 no geolocation 7\n",
      "Threshold 0 no geolocation 5\n",
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n"
     ]
    }
   ],
   "source": [
    "Ys = []\n",
    "labels = []\n",
    "results_file = [VP_SELECTION_ALGORITHM_PROBES_1_FILE, VP_SELECTION_ALGORITHM_PROBES_3_FILE, VP_SELECTION_ALGORITHM_PROBES_10_FILE]\n",
    "index = [1, 3, 10]\n",
    "\n",
    "for i, file in enumerate(results_file):\n",
    "    n_vps = index[i]\n",
    "    errors_threshold_vp_selection_algorithm = load_json(\n",
    "        results_file[i])\n",
    "    error_threshold_cdfs_p_to_a_vp_selection, circles_threshold_cdfs_p_to_a_vp_selection, _ = compute_error_threshold_cdfs(\n",
    "        errors_threshold_vp_selection_algorithm)\n",
    "    Ys.append(list(error_threshold_cdfs_p_to_a_vp_selection[0]))\n",
    "    labels.append(f\"{n_vps} closest VP (RTT)\")\n",
    "    if n_vps == 10:\n",
    "        # Take the baseline where 10 VPs are used to geolocate a target\n",
    "        error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "            errors_threshold_probes_to_anchors, errors_threshold_vp_selection_algorithm)\n",
    "        Ys.append(list(error_threshold_cdfs_p_to_a[0]))\n",
    "        labels.append(\"All VPs\")\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                        \"Geolocation error (km)\",\n",
    "                        \"CDF of targets\",\n",
    "                        xscale=\"log\",\n",
    "                        yscale=\"linear\",\n",
    "                        legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = CBG_THRESHOLD_VP_SELECTION_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_code_2_to_country():\n",
    "    country_by_iso_2 = {}\n",
    "    continent_by_iso_2 = {}\n",
    "    # Continent_Name,Continent_Code,Country_Name,Two_Letter_Country_Code,Three_Letter_Country_Code,Country_Number\n",
    "    with open(COUNTRIES_CSV_FILE) as f:\n",
    "        reader = csv.reader(f, delimiter=\",\", quotechar='\"')\n",
    "        next(reader, None)\n",
    "        for line in reader:\n",
    "            continent_code = line[1]\n",
    "            country_name = line[2].split(\",\")[0]\n",
    "            country_iso_code_2 = line[3]\n",
    "            country_by_iso_2[country_iso_code_2] = country_name\n",
    "            continent_by_iso_2[country_iso_code_2] = continent_code\n",
    "    return continent_by_iso_2, country_by_iso_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_per_country = {}\n",
    "for anchor in anchors:\n",
    "    if \"address_v4\" in anchor and \"geometry\" in anchor and \"coordinates\" in anchor[\"geometry\"]:\n",
    "        ip_v4_address = anchor[\"address_v4\"]\n",
    "        if ip_v4_address is None:\n",
    "            continue\n",
    "        country = anchor[\"country_code\"]\n",
    "        ip_per_country[ip_v4_address] = country\n",
    "\n",
    "country_per_ip = {}\n",
    "for ip, country in ip_per_country.items():\n",
    "    country_per_ip.setdefault(country, []).append(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n",
      "[('Venezuela', (2192.9140091405034, 1, 1)), ('Pakistan', (1543.1755865726761, 1, 1)), ('Colombia', (1434.2320893110607, 1, 1)), ('Mozambique', (832.4551341422323, 1, 1)), ('Kuwait', (717.8898014224587, 1, 1)), ('Peru', (383.09155748407676, 2, 2)), ('Ecuador', (263.85158220593513, 1, 1)), ('Argentina', (221.1132724306607, 4, 4)), ('Hungary', (161.44000325140453, 1, 1)), ('Macedonia', (132.45677655906775, 1, 1)), ('Paraguay', (117.97642875966719, 1, 1)), ('Ghana', (98.31229536436996, 2, 2)), ('Thailand', (93.312454164549, 2, 2)), ('Kenya', (61.11453915894795, 1, 1)), ('Iraq', (54.572811751748695, 1, 1)), ('Poland', (49.969744441399314, 8, 8)), ('Belarus', (43.57770007033401, 1, 1)), ('Dominican Republic', (42.13216173978507, 2, 2)), ('Belgium', (36.693758379143034, 6, 6)), ('United Kingdom', (30.11516867734334, 34, 34)), ('Bulgaria', (29.21677461892106, 4, 4)), ('Italy', (28.05099202145557, 18, 18)), ('Croatia', (27.712028850785746, 1, 1)), ('Japan', (26.677772494424328, 9, 9)), ('Austria', (26.656457822224873, 16, 16)), ('Hong Kong', (26.26910261256225, 2, 2)), ('Chile', (22.855788071461195, 5, 5)), ('Brazil', (21.098721357502143, 10, 10)), ('Sweden', (18.744614215494046, 12, 12)), ('Bahrain', (17.79008834720065, 1, 1)), ('Finland', (16.207622427864646, 8, 8)), ('South Africa', (16.184973218259923, 6, 6)), ('Ireland', (15.55543638240952, 1, 1)), ('Uruguay', (15.233032905336387, 2, 2)), ('Germany', (14.206063127767582, 98, 98)), ('Turkey', (12.956640031304456, 8, 8)), ('United Arab Emirates', (11.839363051672427, 9, 9)), ('Latvia', (11.783213318782138, 2, 2)), ('Nepal', (11.750643963102501, 1, 1)), ('Spain', (11.586997095194093, 9, 9)), ('Luxembourg', (11.402506492262782, 7, 7)), ('Trinidad and Tobago', (11.318134473456936, 1, 1)), ('Switzerland', (10.750089482812738, 27, 27)), ('Singapore', (10.431626958300843, 16, 16)), ('Russian Federation', (10.374153937417868, 19, 19)), ('United States of America', (9.789495753196185, 99, 99)), ('China', (9.698641430918185, 2, 2)), ('Netherlands', (9.573161862276665, 43, 43)), ('India', (8.885183726334462, 6, 6)), ('Tanzania', (8.841181675586071, 1, 1)), ('Kazakhstan', (8.67353831563491, 13, 13)), ('Lithuania', (8.50031646370264, 5, 5)), ('Korea', (7.976306339881375, 3, 3)), ('Israel', (7.668583067843381, 2, 2)), ('Ukraine', (6.696703692941085, 10, 10)), ('Denmark', (6.4986094307266, 8, 8)), ('Czech Republic', (6.2962196724400075, 11, 11)), ('Vietnam', (5.893317419519709, 1, 1)), ('Romania', (5.688135886177724, 5, 5)), ('Serbia', (5.590193609488846, 1, 1)), ('Taiwan', (5.5775229019789165, 3, 3)), ('Saudi Arabia', (5.165651353675621, 3, 3)), ('Iran', (5.103856174420272, 7, 7)), ('Malaysia', (4.847537323241495, 3, 3)), ('Bangladesh', (4.2872401896931995, 1, 1)), ('Norway', (4.078295385641459, 5, 5)), ('Slovenia', (4.0142290695653156, 3, 3)), ('Mongolia', (3.66712874465885, 1, 1)), ('Canada', (3.5894850993185523, 17, 17)), ('Armenia', (3.379778961007782, 1, 1)), ('Panama', (2.7278754075999987, 1, 1)), ('France', (2.6343337447482904, 39, 40)), ('Uganda', (2.4371575436569373, 1, 1)), ('Australia', (2.3354490574777684, 13, 13)), ('Iceland', (1.907919264407635, 1, 1)), ('Moldova', (1.8658295814792256, 1, 1)), ('Greece', (1.738851828660383, 4, 4)), ('Qatar', (1.5076274923059634, 1, 1)), ('Indonesia', (1.355718509632829, 5, 5)), ('Albania', (1.333501361693806, 1, 1)), ('Mauritius', (1.2333398308454366, 2, 2)), ('Mexico', (1.1665092421473662, 3, 3)), ('Portugal', (1.1321987716135997, 3, 3)), ('Maldives', (1.1303591534657769, 1, 1)), ('Burkina Faso', (1.0854467397438248, 1, 1)), ('New Zealand', (0.8355222903806723, 4, 4)), ('Bosnia and Herzegovina', (0.5508332838388663, 2, 2)), ('New Caledonia', (0.380227844671447, 1, 1)), ('Philippines', (0.28654722772090385, 2, 2)), ('Estonia', (0.24929684092108637, 3, 3)), ('Costa Rica', (0.2455098339810597, 1, 1)), ('Georgia', (0.24285056600895522, 2, 2))]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute results per continent\n",
    "\"\"\"\n",
    "\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "\n",
    "continent_by_iso_2, country_by_iso_2 = iso_code_2_to_country()\n",
    "\n",
    "_, _, error_per_ip = compute_error_threshold_cdfs(errors_threshold_probes_to_anchors)\n",
    "\n",
    "error_per_continent_cdf = {}\n",
    "error_per_country_cdf = {}\n",
    "\n",
    "# Match the anchors of the second replicated paper\n",
    "anchors_second = list(set(load_json(ANCHORS_SECOND_PAPER_FILE)))\n",
    "for ip, error in error_per_ip.items():\n",
    "    if ip not in anchors_second:\n",
    "        continue\n",
    "    country = ip_per_country[ip]\n",
    "    continent = continent_by_iso_2[country]\n",
    "    error_per_continent_cdf.setdefault(continent, []).append(error)\n",
    "    error_per_country_cdf.setdefault(country, []).append(error)\n",
    "\n",
    "error_per_country_cdf_med = {country_by_iso_2[x]: (np.median(error_per_country_cdf[x]),\n",
    "                                                len(error_per_country_cdf[x]), len(country_per_ip[x])) for x in error_per_country_cdf}\n",
    "\n",
    "\n",
    "error_per_country_cdf_med_sorted = sorted(\n",
    "    error_per_country_cdf_med.items(), key=lambda x: x[1][0], reverse=True)\n",
    "print(error_per_country_cdf_med_sorted)\n",
    "\n",
    "Ys = [list(error_per_continent_cdf[c])\n",
    "        for c in error_per_continent_cdf]\n",
    "labels = [\n",
    "    f\"{c} ({len(error_per_continent_cdf[c])})\" for c in error_per_continent_cdf]\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = CBG_THRESHOLD_CONTINENT_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n",
      "10 3051555\n",
      "100 4529640\n",
      "300 2918748\n",
      "500 2309946\n",
      "1000 2639067\n"
     ]
    }
   ],
   "source": [
    "round_based_algorithm_results = load_json(ROUND_BASED_ALGORITHM_FILE)\n",
    "\n",
    "round_based_algorithm_results = {int(x):round_based_algorithm_results[x] for x in round_based_algorithm_results}\n",
    "\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "    errors_threshold_probes_to_anchors)\n",
    "\n",
    "Ys_error = [error_threshold_cdfs_p_to_a[0]]\n",
    "Ys_n_vps = []\n",
    "\n",
    "labels_error = [\"All VPs\"]\n",
    "labels_n_vps = []\n",
    "\n",
    "\n",
    "for tier1_vps, results in sorted(round_based_algorithm_results.items()):\n",
    "    tier1_vps = int(tier1_vps)\n",
    "    error_cdf = [r[1] for r in results if r[1] is not None]\n",
    "    n_vps_cdf = [r[2] + tier1_vps for r in results if r[2] is not None]\n",
    "    label = f\"{tier1_vps} VPs\"\n",
    "    labels_error.append(label)\n",
    "    labels_n_vps.append(label)\n",
    "    Ys_error.append(error_cdf)\n",
    "    Ys_n_vps.append(n_vps_cdf)\n",
    "    print(tier1_vps, 3 * sum(n_vps_cdf))\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys_error, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels_error)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = ROUND_ALGORITHM_ERROR_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_n_vps_probes = load_json(ACCURACY_VS_N_VPS_PROBES_FILE)\n",
    "accuracy_vs_n_vps_probes = {\n",
    "    int(x): accuracy_vs_n_vps_probes[x] for x in accuracy_vs_n_vps_probes}\n",
    "X = sorted([x for x in sorted(accuracy_vs_n_vps_probes.keys())])\n",
    "Ys = [accuracy_vs_n_vps_probes[i] for i in X]\n",
    "Ys_med = [[np.median(x) for x in Ys]]\n",
    "Ys_err = [[np.std(x) for x in Ys]]\n",
    "\n",
    "\"\"\"\n",
    "Fig 3.a of the paper\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plot_multiple_error_bars(X, Ys_med, Ys_err,\n",
    "                                    xmin=10, xmax=10500, ymin=1, ymax=10000,\n",
    "                                    xlabel=\"Number of VPs\",\n",
    "                                    ylabel=\"Geolocation error (km)\",\n",
    "                                    xscale=\"log\",\n",
    "                                    yscale=\"log\",\n",
    "                                    labels=[\n",
    "                                        \"\"\n",
    "                                    ],\n",
    "\n",
    "                                    )\n",
    "\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = FIG_3A_FILE\n",
    "plot_save(ofile, is_tight_layout=True)\n",
    "\n",
    "\"\"\"\n",
    "Fig 3.b of the paper\n",
    "\"\"\"\n",
    "\n",
    "subset_sizes = [100, 500, 1000, 2000]\n",
    "\n",
    "labels = [f\"{s} VPs\" for s in subset_sizes]\n",
    "\n",
    "Ys = [accuracy_vs_n_vps_probes[i] for i in subset_sizes]\n",
    "print(min(accuracy_vs_n_vps_probes[100]),\n",
    "        max(accuracy_vs_n_vps_probes[100]))\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of median error\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = FIG_3B_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_info_geo = load_json(IP_INFO_GEO_FILE)\n",
    "mm_geo = load_json(MAXMIND_GEO_FILE)\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n",
      "[711, 709, 711]\n"
     ]
    }
   ],
   "source": [
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "    errors_threshold_probes_to_anchors)\n",
    "\n",
    "maxmind_error = {}\n",
    "ip_info_error = {}\n",
    "for i, anchor in enumerate(sorted(anchors, key=lambda x: x[\"address_v4\"])):\n",
    "    ip = anchor[\"address_v4\"]\n",
    "    if ip in removed_probes:\n",
    "        continue\n",
    "\n",
    "    if \"geometry\" not in anchor:\n",
    "        continue\n",
    "\n",
    "    long, lat = anchor[\"geometry\"][\"coordinates\"]\n",
    "    if ip in mm_geo:\n",
    "        error = haversine(mm_geo[ip], (lat, long))\n",
    "        maxmind_error[ip] = error\n",
    "\n",
    "    if ip in ip_info_geo:\n",
    "        ipinfo_lat, ipinfo_long = ip_info_geo[ip][\"loc\"].split(\",\")\n",
    "        ipinfo_lat, ipinfo_long = float(ipinfo_lat), float(ipinfo_long)\n",
    "        error = haversine((ipinfo_lat, ipinfo_long), (lat, long))\n",
    "        ip_info_error[ip] = error\n",
    "\n",
    "Ys = [error_threshold_cdfs_p_to_a[0], list(\n",
    "    maxmind_error.values()), list(ip_info_error.values())]\n",
    "print([len(Y) for Y in Ys])\n",
    "labels = [\"All VPs\", \"Maxmind (Free)\", \"IPinfo\"]\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "\n",
    "ofile = GEO_DATABASE_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cdf error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier1 Failed\n",
      "723\n",
      "723\n",
      "723\n",
      "723\n",
      "207\n",
      "77 targets are geolocated at street lvl using CBG 0.10650069156293222\n",
      "17 targets are geolocated at street lvl using tech 0.02351313969571231\n",
      "tier 1 median error = 29.368729465989418\n",
      "tier 2 median error = 34.41749100849208\n",
      "tier 3 median error = 27.917440080911355\n",
      "closest landmark distance median = 3.316187115363793\n",
      "filtered tier 1 median error = 24.701434871210086\n",
      "filtered tier 2 median error = 27.976445217281974\n",
      "filtered tier 3 median error = 22.686067574789096\n",
      "filtered closest landmark distance median = 2.8434411415239755\n",
      "17 targets are geolocated at street lvl out of 723 or 2.351313969571231%\n",
      "207 targets has a landmark at street lvl out of 723 or 28.630705394190873%\n"
     ]
    }
   ],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "error1 = []\n",
    "error2 = []\n",
    "error3 = []\n",
    "error4 = []\n",
    "\n",
    "filtered_error1 = []\n",
    "filtered_error2 = []\n",
    "filtered_error3 = []\n",
    "filtered_error4 = []\n",
    "for _, d in data.items():\n",
    "    errors = every_tier_result_and_errors(d)\n",
    "    error1.append(errors['error1'])\n",
    "    error2.append(errors['error2'])\n",
    "    error3.append(errors['error3'])\n",
    "    error4.append(errors['error4'])\n",
    "    if d['tier1:done'] and 'tier2:landmarks' in d and len(d['tier2:landmarks']) > 0:\n",
    "        filtered_error1.append(errors['error1'])\n",
    "        filtered_error2.append(errors['error2'])\n",
    "        filtered_error3.append(errors['error3'])\n",
    "        filtered_error4.append(errors['error4'])\n",
    "\n",
    "print(len(error1))\n",
    "print(len(error2))\n",
    "print(len(error3))\n",
    "print(len(error4))\n",
    "print(len([i for i in error4 if i <= 1]))\n",
    "\n",
    "street_lvl_count_cbg = 0\n",
    "street_lvl_count_tech = 0\n",
    "for e in error1:\n",
    "    if e <= 1:\n",
    "        street_lvl_count_cbg += 1\n",
    "for e in error3:\n",
    "    if e <= 1:\n",
    "        street_lvl_count_tech += 1\n",
    "print(f\"{street_lvl_count_cbg} targets are geolocated at street lvl using CBG {street_lvl_count_cbg/len(error1)}\")\n",
    "print(f\"{street_lvl_count_tech} targets are geolocated at street lvl using tech {street_lvl_count_tech/len(error3)}\")\n",
    "\n",
    "median1 = np.median(error1)\n",
    "median2 = np.median(error2)\n",
    "median3 = np.median(error3)\n",
    "median4 = np.median(error4)\n",
    "\n",
    "print(f\"tier 1 median error = {median1}\")\n",
    "print(f\"tier 2 median error = {median2}\")\n",
    "print(f\"tier 3 median error = {median3}\")\n",
    "print(f\"closest landmark distance median = {median4}\")\n",
    "\n",
    "fmedian1 = np.median(filtered_error1)\n",
    "fmedian2 = np.median(filtered_error2)\n",
    "fmedian3 = np.median(filtered_error3)\n",
    "fmedian4 = np.median(filtered_error4)\n",
    "\n",
    "print(f\"filtered tier 1 median error = {fmedian1}\")\n",
    "print(f\"filtered tier 2 median error = {fmedian2}\")\n",
    "print(f\"filtered tier 3 median error = {fmedian3}\")\n",
    "print(f\"filtered closest landmark distance median = {fmedian4}\")\n",
    "\n",
    "less_then_1 = 0\n",
    "less_then_1_lm = 0\n",
    "for e in error3:\n",
    "    if e <= 1:\n",
    "        less_then_1 += 1\n",
    "for e in error4:\n",
    "    if e <= 1:\n",
    "        less_then_1_lm += 1\n",
    "print(f\"{less_then_1} targets are geolocated at street lvl out of {len(error3)} or {less_then_1*100/len(error3)}%\")\n",
    "print(f\"{less_then_1_lm} targets has a landmark at street lvl out of {len(error4)} or {less_then_1_lm*100/len(error4)}%\")\n",
    "\n",
    "\n",
    "plot_multiple_cdf([error3, error1, error4], 10000, 0.1, None, 'Geolocation error (km)',\n",
    "                    'CDF of targets', [\"Street Level\", \"CBG\", \"Closest Landmark\"], xscale=\"log\")\n",
    "plt.legend(fontsize=\"14\")\n",
    "plot_save(CLOSE_LANDMARK_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cdf landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44833 no r1 found out of 143601\n"
     ]
    }
   ],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "values = []\n",
    "all_traceroutes_count = 0\n",
    "no_r1_traceroutes_count = 0\n",
    "\n",
    "for _, d in data.items():\n",
    "    good = 0\n",
    "    bad = 0\n",
    "\n",
    "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "        if f in d:\n",
    "            for t in d[f]:\n",
    "                if t[4] < 0:\n",
    "                    bad += 1\n",
    "                else:\n",
    "                    good += 1\n",
    "\n",
    "                all_traceroutes_count += 1\n",
    "                if t[3] == None:\n",
    "                    no_r1_traceroutes_count += 1\n",
    "\n",
    "\n",
    "\n",
    "    if good != 0 or bad != 0:\n",
    "        values.append(bad/(bad+good))\n",
    "\n",
    "print(f\"{no_r1_traceroutes_count} no r1 found out of {all_traceroutes_count}\")\n",
    "plot_multiple_cdf([values], 10000, 0, 1,\n",
    "                    'Fraction of landmarks with\\nD1 + D2 < 0', 'CDF of targets', None)\n",
    "plot_save(INVALID_RTT_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tier 1 median duration = 0.20766615867614746\n",
      "tier 2 median duration = 394.77880704402924\n",
      "tier 3 median duration = 736.932727098465\n",
      "Street Level median duration = 1210.4626953601837\n"
     ]
    }
   ],
   "source": [
    "data= load_json(ANALYZABLE_FILE)\n",
    "\n",
    "time1 = []\n",
    "time2 = []\n",
    "time3 = []\n",
    "values = []\n",
    "for _, d in data.items():\n",
    "    if d['tier1:done'] and 'tier1:duration' in d:\n",
    "        time1.append(d['tier1:duration'])\n",
    "    if d['tier2:done'] and 'tier2:duration' in d:\n",
    "        time2.append(d['tier2:duration'])\n",
    "    if d['tier3:done'] and 'tier3:duration' in d:\n",
    "        time3.append(d['tier3:duration'])\n",
    "        values.append(d['tier1:duration'] +\n",
    "                        d['tier2:duration']+d['tier3:duration'])\n",
    "\n",
    "median1 = np.median(time1)\n",
    "median2 = np.median(time2)\n",
    "median3 = np.median(time3)\n",
    "median = np.median(values)\n",
    "\n",
    "print(f\"tier 1 median duration = {median1}\")\n",
    "print(f\"tier 2 median duration = {median2}\")\n",
    "print(f\"tier 3 median duration = {median3}\")\n",
    "print(f\"Street Level median duration = {median}\")\n",
    "\n",
    "plot_multiple_cdf([values], 1000, None, None,\n",
    "                    'Time to geolocate a target (sec)', 'CDF of targets', None)\n",
    "plot_save(TIME_TO_GEOLOCATE_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measured distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measured Distance vs Distance median correlation = 0.07190527319632016\n",
      "Measured Distance vs Distance min correlation = -1.0\n",
      "Measured Distance vs Distance max correlation = 1.0\n"
     ]
    }
   ],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "correlations = []\n",
    "mdvd = {}\n",
    "scater_plot_data = {}\n",
    "for target_ip, d in data.items():\n",
    "    tmp_landmarks = {}\n",
    "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "        if f in d:\n",
    "            for t in d[f]:\n",
    "                # if t[3] == None or t[4]<0:\n",
    "                if t[4] < 0:\n",
    "                    continue\n",
    "                landmarks_ip = t[2]\n",
    "                measured_distance = rtt_to_km(t[4], 4/9, 300)\n",
    "                distance = haversine(\n",
    "                    (t[5], t[6]), (d['lat_c'], d['lon_c']))\n",
    "                if landmarks_ip not in tmp_landmarks:\n",
    "                    tmp_landmarks[landmarks_ip] = (\n",
    "                        measured_distance, distance)\n",
    "                if measured_distance < tmp_landmarks[landmarks_ip][0]:\n",
    "                    tmp_landmarks[landmarks_ip] = (\n",
    "                        measured_distance, distance)\n",
    "    if len(tmp_landmarks) != 0:\n",
    "        tmp_dict = {'md': [], 'd': []}\n",
    "        for k, v in tmp_landmarks.items():\n",
    "            all_diff = True\n",
    "            for i in range(len(tmp_dict['d'])):\n",
    "                if v[1] == tmp_dict['d'][i]:\n",
    "                    all_diff = False\n",
    "            if all_diff:\n",
    "                tmp_dict['md'].append(v[0])\n",
    "                tmp_dict['d'].append(v[1])\n",
    "        if len(tmp_dict['md']) > 1:\n",
    "            correlation = pearsonr(tmp_dict['md'], tmp_dict['d'])[0]\n",
    "            tmp_dict['correlation'] = correlation\n",
    "            correlations.append(correlation)\n",
    "            mdvd[d['target_ip']] = tmp_dict\n",
    "        if len(tmp_dict['md']) >= 5:  # and len(tmp_dict['md']) <= 15:\n",
    "            error = every_tier_result_and_errors(d)\n",
    "            if error['error3'] < 45:\n",
    "                scater_plot_data[target_ip] = {\n",
    "                    'geo_loc_data': d, 'error_data': error, 'mdvd_data': tmp_dict}\n",
    "\n",
    "medianc = np.median(correlations)\n",
    "minc = min(correlations)\n",
    "maxc = max(correlations)\n",
    "\n",
    "print(f\"Measured Distance vs Distance median correlation = {medianc}\")\n",
    "print(f\"Measured Distance vs Distance min correlation = {minc}\")\n",
    "print(f\"Measured Distance vs Distance max correlation = {maxc}\")\n",
    "\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "x3 = []\n",
    "x4 = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "y4 = []\n",
    "for _, d in scater_plot_data.items():\n",
    "    if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 1:\n",
    "        if len(x1) == 0:\n",
    "            x1 = d['mdvd_data']['d']\n",
    "            y1 = d['mdvd_data']['md']\n",
    "        if len(x1) > len(d['mdvd_data']['d']):\n",
    "            x1 = d['mdvd_data']['d']\n",
    "            y1 = d['mdvd_data']['md']\n",
    "    if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 6 and d['error_data']['error3'] > 4:\n",
    "        if len(x2) == 0:\n",
    "            x2 = d['mdvd_data']['d']\n",
    "            y2 = d['mdvd_data']['md']\n",
    "        if len(x2) > len(d['mdvd_data']['d']):\n",
    "            x2 = d['mdvd_data']['d']\n",
    "            y2 = d['mdvd_data']['md']\n",
    "    if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 11 and d['error_data']['error3'] > 9:\n",
    "        if len(x3) == 0:\n",
    "            x3 = d['mdvd_data']['d']\n",
    "            y3 = d['mdvd_data']['md']\n",
    "        if len(x3) > len(d['mdvd_data']['d']):\n",
    "            x3 = d['mdvd_data']['d']\n",
    "            y3 = d['mdvd_data']['md']\n",
    "    if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 41 and d['error_data']['error3'] > 39:\n",
    "        if len(x4) == 0:\n",
    "            x4 = d['mdvd_data']['d']\n",
    "            y4 = d['mdvd_data']['md']\n",
    "        if len(x4) > len(d['mdvd_data']['d']):\n",
    "            x4 = d['mdvd_data']['d']\n",
    "            y4 = d['mdvd_data']['md']\n",
    "\n",
    "list_color = ['r', 'b', 'g', 'y']\n",
    "list_mak = ['o', '*', 'x', '+']\n",
    "list_lab = ['< 1 km error', '5 km error', '10 km error', '40 km error']\n",
    "plot_scatter_multiple([x1, x2, x3, x4], [y1, y2, y3, y4], None, None, 1, None, \"log\", \"log\",\n",
    "                        'Geographical distance (km)', 'Measured distance (km)', list_mak, list_color, [10, 10, 10, 10])\n",
    "plt.legend(list_lab, fontsize=\"14\")\n",
    "plot_save(SCATTER_DISTANCE_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier1 Failed\n"
     ]
    }
   ],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "pop_data = load_json(POPULATION_CITY_FILE)\n",
    "\n",
    "dens_lst = []\n",
    "error_lst = []\n",
    "for d in pop_data:\n",
    "    ip = d['target_ip']\n",
    "    if ip not in data:\n",
    "        continue\n",
    "    pop = d['density']\n",
    "    dens_lst.append(pop)\n",
    "    errors = every_tier_result_and_errors(data[ip])\n",
    "    error_lst.append(errors['error3'])\n",
    "\n",
    "fig, ax = plot_scatter_multiple([error_lst], [dens_lst], 0.1, 10000, 0.1, 100000, \"log\",\n",
    "                                \"log\", 'Error distance (km)', 'Population Density (people/km²)', [\"x\"], [\"b\"], [10])\n",
    "degree = 1\n",
    "coef = np.polyfit(error_lst, dens_lst, deg=degree)\n",
    "xseq = np.linspace(0, 10000, num=100)\n",
    "yseq = [0 for i in range(len(xseq))]\n",
    "for i in range(len(coef)):\n",
    "    power = len(coef) - i - 1\n",
    "    yseq = [(xseq[j]**power)*coef[i]+yseq[j] for j in range(len(xseq))]\n",
    "ax.plot(xseq, yseq, color=\"k\", lw=2.5)\n",
    "plot_save(SCATTER_DENSITY_FILE, is_tight_layout=True)\n",
    "\n",
    "plot_multiple_cdf([dens_lst], 10000, None, None,\n",
    "                    'Population Density (people/km²)', 'CDF of targets', None, xscale=\"log\")\n",
    "plot_save(CDF_DENSITY_FILE, is_tight_layout=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review-fXCvvitn-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
