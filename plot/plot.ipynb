{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from clickhouse_driver import Client\n",
    "\n",
    "from scripts.utils.file_utils import load_json\n",
    "from scripts.analysis.analysis import compute_error_threshold_cdfs, every_tier_result_and_errors\n",
    "from scripts.utils.plot_utils import plot_multiple_cdf, homogenize_legend, plot_save, plot_multiple_error_bars\n",
    "from scripts.utils.clickhouse_utils import get_min_rtt_per_src_dst_query_ping_table\n",
    "from scripts.utils.helpers import haversine\n",
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = load_json(ANCHORS_FILE)\n",
    "\n",
    "probes = load_json(PROBES_FILE)\n",
    "\n",
    "all_probes = load_json(PROBES_AND_ANCHORS_FILE)\n",
    "\n",
    "removed_probes = load_json(REMOVED_PROBES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing probes to anchors\n",
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n",
      "711\n"
     ]
    }
   ],
   "source": [
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "\n",
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "    errors_threshold_probes_to_anchors)\n",
    "\n",
    "Ys = error_threshold_cdfs_p_to_a\n",
    "print(len(error_threshold_cdfs_p_to_a[0]))\n",
    "labels = [\"All VPs\"]\n",
    "labels.extend([f\"VPs > {t} km\" for t in THRESHOLD_DISTANCES if t > 0])\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\", legend_size=12)\n",
    "\n",
    "ofile = CBG_THRESHOLD_PROBES_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0 no geolocation 33\n",
      "Threshold 0 no geolocation 7\n",
      "Threshold 0 no geolocation 5\n",
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n"
     ]
    }
   ],
   "source": [
    "Ys = []\n",
    "labels = []\n",
    "results_file = [VP_SELECTION_ALGORITHM_PROBES_1_FILE, VP_SELECTION_ALGORITHM_PROBES_3_FILE, VP_SELECTION_ALGORITHM_PROBES_10_FILE]\n",
    "index = [1, 3, 10]\n",
    "\n",
    "for i, file in enumerate(results_file):\n",
    "    n_vps = index[i]\n",
    "    errors_threshold_vp_selection_algorithm = load_json(\n",
    "        results_file[i])\n",
    "    error_threshold_cdfs_p_to_a_vp_selection, circles_threshold_cdfs_p_to_a_vp_selection, _ = compute_error_threshold_cdfs(\n",
    "        errors_threshold_vp_selection_algorithm)\n",
    "    Ys.append(list(error_threshold_cdfs_p_to_a_vp_selection[0]))\n",
    "    labels.append(f\"{n_vps} closest VP (RTT)\")\n",
    "    if n_vps == 10:\n",
    "        # Take the baseline where 10 VPs are used to geolocate a target\n",
    "        error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "            errors_threshold_probes_to_anchors, errors_threshold_vp_selection_algorithm)\n",
    "        Ys.append(list(error_threshold_cdfs_p_to_a[0]))\n",
    "        labels.append(\"All VPs\")\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                        \"Geolocation error (km)\",\n",
    "                        \"CDF of targets\",\n",
    "                        xscale=\"log\",\n",
    "                        yscale=\"linear\",\n",
    "                        legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = CBG_THRESHOLD_VP_SELECTION_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_code_2_to_country():\n",
    "    country_by_iso_2 = {}\n",
    "    continent_by_iso_2 = {}\n",
    "    # Continent_Name,Continent_Code,Country_Name,Two_Letter_Country_Code,Three_Letter_Country_Code,Country_Number\n",
    "    with open(COUNTRIES_CSV_FILE) as f:\n",
    "        reader = csv.reader(f, delimiter=\",\", quotechar='\"')\n",
    "        next(reader, None)\n",
    "        for line in reader:\n",
    "            continent_code = line[1]\n",
    "            country_name = line[2].split(\",\")[0]\n",
    "            country_iso_code_2 = line[3]\n",
    "            country_by_iso_2[country_iso_code_2] = country_name\n",
    "            continent_by_iso_2[country_iso_code_2] = continent_code\n",
    "    return continent_by_iso_2, country_by_iso_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_per_country = {}\n",
    "for anchor in anchors:\n",
    "    if \"address_v4\" in anchor and \"geometry\" in anchor and \"coordinates\" in anchor[\"geometry\"]:\n",
    "        ip_v4_address = anchor[\"address_v4\"]\n",
    "        if ip_v4_address is None:\n",
    "            continue\n",
    "        country = anchor[\"country_code\"]\n",
    "        ip_per_country[ip_v4_address] = country\n",
    "\n",
    "country_per_ip = {}\n",
    "for ip, country in ip_per_country.items():\n",
    "    country_per_ip.setdefault(country, []).append(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n",
      "[('Venezuela', (2192.9140091405034, 1, 1)), ('Pakistan', (1543.1755865726761, 1, 1)), ('Colombia', (1434.2320893110607, 1, 1)), ('Mozambique', (832.4551341422323, 1, 1)), ('Kuwait', (717.8898014224587, 1, 1)), ('Peru', (383.09155748407676, 2, 2)), ('Ecuador', (263.85158220593513, 1, 1)), ('Argentina', (221.1132724306607, 4, 4)), ('Hungary', (161.44000325140453, 1, 1)), ('Macedonia', (132.45677655906775, 1, 1)), ('Paraguay', (117.97642875966719, 1, 1)), ('Ghana', (98.31229536436996, 2, 2)), ('Thailand', (93.312454164549, 2, 2)), ('Kenya', (61.11453915894795, 1, 1)), ('Iraq', (54.572811751748695, 1, 1)), ('Poland', (49.969744441399314, 8, 8)), ('Belarus', (43.57770007033401, 1, 1)), ('Dominican Republic', (42.13216173978507, 2, 2)), ('Belgium', (36.693758379143034, 6, 6)), ('United Kingdom', (30.11516867734334, 34, 34)), ('Bulgaria', (29.21677461892106, 4, 4)), ('Italy', (28.05099202145557, 18, 18)), ('Croatia', (27.712028850785746, 1, 1)), ('Japan', (26.677772494424328, 9, 9)), ('Austria', (26.656457822224873, 16, 16)), ('Hong Kong', (26.26910261256225, 2, 2)), ('Chile', (22.855788071461195, 5, 5)), ('Brazil', (21.098721357502143, 10, 10)), ('Sweden', (18.744614215494046, 12, 12)), ('Bahrain', (17.79008834720065, 1, 1)), ('Finland', (16.207622427864646, 8, 8)), ('South Africa', (16.184973218259923, 6, 6)), ('Ireland', (15.55543638240952, 1, 1)), ('Uruguay', (15.233032905336387, 2, 2)), ('Germany', (14.206063127767582, 98, 98)), ('Turkey', (12.956640031304456, 8, 8)), ('United Arab Emirates', (11.839363051672427, 9, 9)), ('Latvia', (11.783213318782138, 2, 2)), ('Nepal', (11.750643963102501, 1, 1)), ('Spain', (11.586997095194093, 9, 9)), ('Luxembourg', (11.402506492262782, 7, 7)), ('Trinidad and Tobago', (11.318134473456936, 1, 1)), ('Switzerland', (10.750089482812738, 27, 27)), ('Singapore', (10.431626958300843, 16, 16)), ('Russian Federation', (10.374153937417868, 19, 19)), ('United States of America', (9.789495753196185, 99, 99)), ('China', (9.698641430918185, 2, 2)), ('Netherlands', (9.573161862276665, 43, 43)), ('India', (8.885183726334462, 6, 6)), ('Tanzania', (8.841181675586071, 1, 1)), ('Kazakhstan', (8.67353831563491, 13, 13)), ('Lithuania', (8.50031646370264, 5, 5)), ('Korea', (7.976306339881375, 3, 3)), ('Israel', (7.668583067843381, 2, 2)), ('Ukraine', (6.696703692941085, 10, 10)), ('Denmark', (6.4986094307266, 8, 8)), ('Czech Republic', (6.2962196724400075, 11, 11)), ('Vietnam', (5.893317419519709, 1, 1)), ('Romania', (5.688135886177724, 5, 5)), ('Serbia', (5.590193609488846, 1, 1)), ('Taiwan', (5.5775229019789165, 3, 3)), ('Saudi Arabia', (5.165651353675621, 3, 3)), ('Iran', (5.103856174420272, 7, 7)), ('Malaysia', (4.847537323241495, 3, 3)), ('Bangladesh', (4.2872401896931995, 1, 1)), ('Norway', (4.078295385641459, 5, 5)), ('Slovenia', (4.0142290695653156, 3, 3)), ('Mongolia', (3.66712874465885, 1, 1)), ('Canada', (3.5894850993185523, 17, 17)), ('Armenia', (3.379778961007782, 1, 1)), ('Panama', (2.7278754075999987, 1, 1)), ('France', (2.6343337447482904, 39, 40)), ('Uganda', (2.4371575436569373, 1, 1)), ('Australia', (2.3354490574777684, 13, 13)), ('Iceland', (1.907919264407635, 1, 1)), ('Moldova', (1.8658295814792256, 1, 1)), ('Greece', (1.738851828660383, 4, 4)), ('Qatar', (1.5076274923059634, 1, 1)), ('Indonesia', (1.355718509632829, 5, 5)), ('Albania', (1.333501361693806, 1, 1)), ('Mauritius', (1.2333398308454366, 2, 2)), ('Mexico', (1.1665092421473662, 3, 3)), ('Portugal', (1.1321987716135997, 3, 3)), ('Maldives', (1.1303591534657769, 1, 1)), ('Burkina Faso', (1.0854467397438248, 1, 1)), ('New Zealand', (0.8355222903806723, 4, 4)), ('Bosnia and Herzegovina', (0.5508332838388663, 2, 2)), ('New Caledonia', (0.380227844671447, 1, 1)), ('Philippines', (0.28654722772090385, 2, 2)), ('Estonia', (0.24929684092108637, 3, 3)), ('Costa Rica', (0.2455098339810597, 1, 1)), ('Georgia', (0.24285056600895522, 2, 2))]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute results per continent\n",
    "\"\"\"\n",
    "\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "\n",
    "continent_by_iso_2, country_by_iso_2 = iso_code_2_to_country()\n",
    "\n",
    "_, _, error_per_ip = compute_error_threshold_cdfs(errors_threshold_probes_to_anchors)\n",
    "\n",
    "error_per_continent_cdf = {}\n",
    "error_per_country_cdf = {}\n",
    "\n",
    "# Match the anchors of the second replicated paper\n",
    "anchors_second = list(set(load_json(ANCHORS_SECOND_PAPER_FILE)))\n",
    "for ip, error in error_per_ip.items():\n",
    "    if ip not in anchors_second:\n",
    "        continue\n",
    "    country = ip_per_country[ip]\n",
    "    continent = continent_by_iso_2[country]\n",
    "    error_per_continent_cdf.setdefault(continent, []).append(error)\n",
    "    error_per_country_cdf.setdefault(country, []).append(error)\n",
    "\n",
    "error_per_country_cdf_med = {country_by_iso_2[x]: (np.median(error_per_country_cdf[x]),\n",
    "                                                len(error_per_country_cdf[x]), len(country_per_ip[x])) for x in error_per_country_cdf}\n",
    "\n",
    "\n",
    "error_per_country_cdf_med_sorted = sorted(\n",
    "    error_per_country_cdf_med.items(), key=lambda x: x[1][0], reverse=True)\n",
    "print(error_per_country_cdf_med_sorted)\n",
    "\n",
    "Ys = [list(error_per_continent_cdf[c])\n",
    "        for c in error_per_continent_cdf]\n",
    "labels = [\n",
    "    f\"{c} ({len(error_per_continent_cdf[c])})\" for c in error_per_continent_cdf]\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = CBG_THRESHOLD_CONTINENT_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_n_vps_probes = load_json(ACCURACY_VS_N_VPS_PROBES_FILE)\n",
    "accuracy_vs_n_vps_probes = {\n",
    "    int(x): accuracy_vs_n_vps_probes[x] for x in accuracy_vs_n_vps_probes}\n",
    "X = sorted([x for x in sorted(accuracy_vs_n_vps_probes.keys())])\n",
    "Ys = [accuracy_vs_n_vps_probes[i] for i in X]\n",
    "Ys_med = [[np.median(x) for x in Ys]]\n",
    "Ys_err = [[np.std(x) for x in Ys]]\n",
    "\n",
    "\"\"\"\n",
    "Fig 3.a of the paper\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plot_multiple_error_bars(X, Ys_med, Ys_err,\n",
    "                                    xmin=10, xmax=10500, ymin=1, ymax=10000,\n",
    "                                    xlabel=\"Number of VPs\",\n",
    "                                    ylabel=\"Geolocation error (km)\",\n",
    "                                    xscale=\"log\",\n",
    "                                    yscale=\"log\",\n",
    "                                    labels=[\n",
    "                                        \"\"\n",
    "                                    ],\n",
    "\n",
    "                                    )\n",
    "\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = FIG_3A_FILE\n",
    "plot_save(ofile, is_tight_layout=True)\n",
    "\n",
    "\"\"\"\n",
    "Fig 3.b of the paper\n",
    "\"\"\"\n",
    "\n",
    "subset_sizes = [100, 500, 1000, 2000]\n",
    "\n",
    "labels = [f\"{s} VPs\" for s in subset_sizes]\n",
    "\n",
    "Ys = [accuracy_vs_n_vps_probes[i] for i in subset_sizes]\n",
    "print(min(accuracy_vs_n_vps_probes[100]),\n",
    "        max(accuracy_vs_n_vps_probes[100]))\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of median error\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = FIG_3B_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n",
      "10 3051555\n",
      "100 4529640\n",
      "300 2918748\n",
      "500 2309946\n",
      "1000 2639067\n"
     ]
    }
   ],
   "source": [
    "round_based_algorithm_results = load_json(ROUND_BASED_ALGORITHM_FILE)\n",
    "\n",
    "round_based_algorithm_results = {\n",
    "int(x): round_based_algorithm_results[x] for x in round_based_algorithm_results}\n",
    "\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "errors_threshold_probes_to_anchors)\n",
    "\n",
    "Ys_error = [error_threshold_cdfs_p_to_a[0]]\n",
    "Ys_n_vps = []\n",
    "\n",
    "labels_error = [\"All VPs\"]\n",
    "labels_n_vps = []\n",
    "\n",
    "for tier1_vps, results in sorted(round_based_algorithm_results.items()):\n",
    "        tier1_vps = int(tier1_vps)\n",
    "        error_cdf = [r[1] for r in results if r[1] is not None]\n",
    "        n_vps_cdf = [r[2] + tier1_vps for r in results if r[2] is not None]\n",
    "        label = f\"{tier1_vps} VPs\"\n",
    "        labels_error.append(label)\n",
    "        labels_n_vps.append(label)\n",
    "        Ys_error.append(error_cdf)\n",
    "        Ys_n_vps.append(n_vps_cdf)\n",
    "        print(tier1_vps, 3 * sum(n_vps_cdf))\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys_error, 10000, 1, 10000,\n",
    "                        \"Geolocation error (km)\",\n",
    "                        \"CDF of targets\",\n",
    "                        xscale=\"log\",\n",
    "                        yscale=\"linear\",\n",
    "                        legend=labels_error)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = ROUND_ALGORITHM_ERROR_FILE\n",
    "plot_save(ofile, is_tight_layout=True)\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys_n_vps, 10000, 10, 10000,\n",
    "                        \"Vantage points\",\n",
    "                        \"CDF of representatives\",\n",
    "                        xscale=\"log\",\n",
    "                        yscale=\"linear\",\n",
    "                        legend=labels_n_vps)\n",
    "homogenize_legend(ax, \"upper left\")\n",
    "ofile = ROUND_BASED_ALGORITHM_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\milo2\\\\Desktop\\\\review\\\\datasets\\\\measurements\\\\street_level\\\\final_all_res.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m load_json(FINAL_ANALYZABLE_FILE)\n\u001b[0;32m      3\u001b[0m query \u001b[39m=\u001b[39m get_min_rtt_per_src_dst_query_ping_table(\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgeolocation_replication\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtargets_to_landmarks_pings\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1000000\u001b[39m)\n\u001b[0;32m      5\u001b[0m client \u001b[39m=\u001b[39m Client(\u001b[39m'\u001b[39m\u001b[39m127.0.0.1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\review\\scripts\\utils\\file_utils.py:5\u001b[0m, in \u001b[0;36mload_json\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_json\u001b[39m(file):\n\u001b[1;32m----> 5\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m         \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\milo2\\\\Desktop\\\\review\\\\datasets\\\\measurements\\\\street_level\\\\final_all_res.json'"
     ]
    }
   ],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "query = get_min_rtt_per_src_dst_query_ping_table(\n",
    "    'geolocation_replication', 'targets_to_landmarks_pings', '', 1000000)\n",
    "client = Client('127.0.0.1')\n",
    "db_table = client.execute(query)\n",
    "rtts = []\n",
    "remove_dict = {}\n",
    "print(len(db_table))\n",
    "for l in db_table:\n",
    "    rtts.append(l[2])\n",
    "    remove_dict[(l[0], l[1])] = l[2]\n",
    "print(len(rtts))\n",
    "plot_multiple_cdf([rtts], 10000, 0, None, 'Min RTT (ms)',\n",
    "                    'CDF of (landmark, target) pairs', None)\n",
    "plot_save(CLOSE_LANDMARK_FILE, is_tight_layout=True)\n",
    "\n",
    "plot_multiple_cdf([rtts], 10000, 0.1, None, 'Min RTT (ms)',\n",
    "                    'CDF of (landmark, target) pairs', None, xscale=\"log\")\n",
    "plot_save(CLOSE_LANDMARK_LOG_FILE, is_tight_layout=True)\n",
    "\n",
    "error1 = []\n",
    "error2 = []\n",
    "error3 = []\n",
    "error4 = []\n",
    "error1ms = []\n",
    "error2ms = []\n",
    "error5ms = []\n",
    "error10ms = []\n",
    "\n",
    "for _, d in data.items():\n",
    "    errors = every_tier_result_and_errors(d)\n",
    "    error1.append(errors['error1'])\n",
    "    error2.append(errors['error2'])\n",
    "    error3.append(errors['error3'])\n",
    "    error4.append(errors['error4'])\n",
    "    err1ms = 50000\n",
    "    err2ms = 50000\n",
    "    err5ms = 50000\n",
    "    err10ms = 50000\n",
    "    for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
    "        if f in d:\n",
    "            for l_ip, _, l_lat, l_lon in d[f]:\n",
    "                dist = haversine((l_lat, l_lon), (d['lat_c'], d['lon_c']))\n",
    "                key_rtt = (l_ip, d['target_ip'])\n",
    "                if dist < err1ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 1):\n",
    "                    err1ms = dist\n",
    "                if dist < err2ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 2):\n",
    "                    err2ms = dist\n",
    "                if dist < err5ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 5):\n",
    "                    err5ms = dist\n",
    "                if dist < err10ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 10):\n",
    "                    err10ms = dist\n",
    "    if err1ms != 50000:\n",
    "        error1ms.append(err1ms)\n",
    "    else:\n",
    "        error1ms.append(error1[-1])\n",
    "    if err2ms != 50000:\n",
    "        error2ms.append(err2ms)\n",
    "    else:\n",
    "        error2ms.append(error1[-1])\n",
    "    if err5ms != 50000:\n",
    "        error5ms.append(err5ms)\n",
    "    else:\n",
    "        error5ms.append(error1[-1])\n",
    "    if err10ms != 50000:\n",
    "        error10ms.append(err10ms)\n",
    "    else:\n",
    "        error10ms.append(error1[-1])\n",
    "\n",
    "plot_multiple_cdf([error3, error4, error1ms, error2ms, error5ms, error10ms], 10000, 0, None, 'Geolocation error (km)', 'CDF of targets', [\n",
    "                    \"Street Level\", \"Closest landmark unfiltered\", \"Closest landmark <= 1ms\", \"Closest landmark <= 2ms\", \"Closest landmark <= 5ms\", \"Closest landmark <= 10ms\"])\n",
    "plt.legend(fontsize=\"10\")\n",
    "plot_save(CLOSE_LANDMARK_FILE_2, is_tight_layout=True)\n",
    "\n",
    "plot_multiple_cdf([error3, error4, error1ms, error2ms, error5ms, error10ms], 10000, 0.1, None, 'Geolocation error (km)', 'CDF of targets', [\n",
    "                    \"Street Level\", \"Closest landmark unfiltered\", \"Closest landmark <= 1ms\", \"Closest landmark <= 2ms\", \"Closest landmark <= 5ms\", \"Closest landmark <= 10ms\"], xscale=\"log\")\n",
    "plt.legend(fontsize=\"10\")\n",
    "plot_save(CLOSE_LANDMARK_LOG_FILE_2, is_tight_layout=True)\n",
    "\n",
    "for i in [1, 5, 10, 40, 9999999999]:\n",
    "    c = len([j for j in error1ms if j <= i])\n",
    "    print(f\"{c} targets with landmarks (ping <= {i}) or {c/len(error1ms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_info_geo = load_json(IP_INFO_GEO_FILE)\n",
    "mm_geo = load_json(MAXMIND_GEO_FILE)\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0 no geolocation 0\n",
      "Threshold 40 no geolocation 0\n",
      "Threshold 100 no geolocation 0\n",
      "Threshold 500 no geolocation 0\n",
      "Threshold 1000 no geolocation 1\n",
      "[711, 709, 711]\n"
     ]
    }
   ],
   "source": [
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "    errors_threshold_probes_to_anchors)\n",
    "\n",
    "maxmind_error = {}\n",
    "ip_info_error = {}\n",
    "for i, anchor in enumerate(sorted(anchors, key=lambda x: x[\"address_v4\"])):\n",
    "    ip = anchor[\"address_v4\"]\n",
    "    if ip in removed_probes:\n",
    "        continue\n",
    "\n",
    "    if \"geometry\" not in anchor:\n",
    "        continue\n",
    "\n",
    "    long, lat = anchor[\"geometry\"][\"coordinates\"]\n",
    "    if ip in mm_geo:\n",
    "        error = haversine(mm_geo[ip], (lat, long))\n",
    "        maxmind_error[ip] = error\n",
    "\n",
    "    if ip in ip_info_geo:\n",
    "        ipinfo_lat, ipinfo_long = ip_info_geo[ip][\"loc\"].split(\",\")\n",
    "        ipinfo_lat, ipinfo_long = float(ipinfo_lat), float(ipinfo_long)\n",
    "        error = haversine((ipinfo_lat, ipinfo_long), (lat, long))\n",
    "        ip_info_error[ip] = error\n",
    "\n",
    "Ys = [error_threshold_cdfs_p_to_a[0], list(\n",
    "    maxmind_error.values()), list(ip_info_error.values())]\n",
    "print([len(Y) for Y in Ys])\n",
    "labels = [\"All VPs\", \"Maxmind (Free)\", \"IPinfo\"]\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "\n",
    "ofile = GEO_DATABASE_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cdf error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier1 Failed\n",
      "723\n",
      "723\n",
      "723\n",
      "723\n",
      "207\n",
      "77 targets are geolocated at street lvl using CBG 0.10650069156293222\n",
      "17 targets are geolocated at street lvl using tech 0.02351313969571231\n",
      "tier 1 median error = 29.368729465989418\n",
      "tier 2 median error = 34.41749100849208\n",
      "tier 3 median error = 27.917440080911355\n",
      "closest landmark distance median = 3.316187115363793\n",
      "filtered tier 1 median error = 24.701434871210086\n",
      "filtered tier 2 median error = 27.976445217281974\n",
      "filtered tier 3 median error = 22.686067574789096\n",
      "filtered closest landmark distance median = 2.8434411415239755\n",
      "17 targets are geolocated at street lvl out of 723 or 2.351313969571231%\n",
      "207 targets has a landmark at street lvl out of 723 or 28.630705394190873%\n"
     ]
    }
   ],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "error1 = []\n",
    "error2 = []\n",
    "error3 = []\n",
    "error4 = []\n",
    "\n",
    "filtered_error1 = []\n",
    "filtered_error2 = []\n",
    "filtered_error3 = []\n",
    "filtered_error4 = []\n",
    "for _, d in data.items():\n",
    "    errors = every_tier_result_and_errors(d)\n",
    "    error1.append(errors['error1'])\n",
    "    error2.append(errors['error2'])\n",
    "    error3.append(errors['error3'])\n",
    "    error4.append(errors['error4'])\n",
    "    if d['tier1:done'] and 'tier2:landmarks' in d and len(d['tier2:landmarks']) > 0:\n",
    "        filtered_error1.append(errors['error1'])\n",
    "        filtered_error2.append(errors['error2'])\n",
    "        filtered_error3.append(errors['error3'])\n",
    "        filtered_error4.append(errors['error4'])\n",
    "\n",
    "print(len(error1))\n",
    "print(len(error2))\n",
    "print(len(error3))\n",
    "print(len(error4))\n",
    "print(len([i for i in error4 if i <= 1]))\n",
    "\n",
    "street_lvl_count_cbg = 0\n",
    "street_lvl_count_tech = 0\n",
    "for e in error1:\n",
    "    if e <= 1:\n",
    "        street_lvl_count_cbg += 1\n",
    "for e in error3:\n",
    "    if e <= 1:\n",
    "        street_lvl_count_tech += 1\n",
    "print(f\"{street_lvl_count_cbg} targets are geolocated at street lvl using CBG {street_lvl_count_cbg/len(error1)}\")\n",
    "print(f\"{street_lvl_count_tech} targets are geolocated at street lvl using tech {street_lvl_count_tech/len(error3)}\")\n",
    "\n",
    "median1 = np.median(error1)\n",
    "median2 = np.median(error2)\n",
    "median3 = np.median(error3)\n",
    "median4 = np.median(error4)\n",
    "\n",
    "print(f\"tier 1 median error = {median1}\")\n",
    "print(f\"tier 2 median error = {median2}\")\n",
    "print(f\"tier 3 median error = {median3}\")\n",
    "print(f\"closest landmark distance median = {median4}\")\n",
    "\n",
    "fmedian1 = np.median(filtered_error1)\n",
    "fmedian2 = np.median(filtered_error2)\n",
    "fmedian3 = np.median(filtered_error3)\n",
    "fmedian4 = np.median(filtered_error4)\n",
    "\n",
    "print(f\"filtered tier 1 median error = {fmedian1}\")\n",
    "print(f\"filtered tier 2 median error = {fmedian2}\")\n",
    "print(f\"filtered tier 3 median error = {fmedian3}\")\n",
    "print(f\"filtered closest landmark distance median = {fmedian4}\")\n",
    "\n",
    "less_then_1 = 0\n",
    "less_then_1_lm = 0\n",
    "for e in error3:\n",
    "    if e <= 1:\n",
    "        less_then_1 += 1\n",
    "for e in error4:\n",
    "    if e <= 1:\n",
    "        less_then_1_lm += 1\n",
    "print(f\"{less_then_1} targets are geolocated at street lvl out of {len(error3)} or {less_then_1*100/len(error3)}%\")\n",
    "print(f\"{less_then_1_lm} targets has a landmark at street lvl out of {len(error4)} or {less_then_1_lm*100/len(error4)}%\")\n",
    "\n",
    "\n",
    "plot_multiple_cdf([error3, error1, error4], 10000, 0.1, None, 'Geolocation error (km)',\n",
    "                    'CDF of targets', [\"Street Level\", \"CBG\", \"Closest Landmark\"], xscale=\"log\")\n",
    "plt.legend(fontsize=\"14\")\n",
    "plot_save(CLOSE_LANDMARK_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min dist landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_min_dist_landmark(data):\n",
    "    without_landmarks_count = 0\n",
    "    with_landmarks_count = 0\n",
    "    distances = []\n",
    "    for _, d in data.items():\n",
    "        landmarks = []\n",
    "        if 'tier2:landmarks' in d:\n",
    "            for l in d['tier2:landmarks']:\n",
    "                landmarks.append(l)\n",
    "        if 'tier3:landmarks' in d:\n",
    "            for l in d['tier3:landmarks']:\n",
    "                landmarks.append(l)\n",
    "        if len(landmarks) == 0:\n",
    "            without_landmarks_count += 1\n",
    "        else:\n",
    "            with_landmarks_count += 1\n",
    "            mindist = haversine((d['lat_c'], d['lon_c']),\n",
    "                                (landmarks[0][2], landmarks[0][3]))\n",
    "            for l in landmarks[1:]:\n",
    "                dist = haversine((d['lat_c'], d['lon_c']), (l[2], l[3]))\n",
    "                mindist = min(mindist, dist)\n",
    "            distances.append(mindist)\n",
    "\n",
    "    median = np.median(distances)\n",
    "    print(f\"Median distance to nearest landmark = {median}\")\n",
    "\n",
    "    plot_multiple_cdf([distances], 10000, 0.1, None,\n",
    "                      'Distance to the nearest landmark (km)', 'CDF of targets', None)\n",
    "    plot_save(\"./fig/distance_to_landmark.pdf\", is_tight_layout=True)\n",
    "\n",
    "    plot_multiple_cdf([distances], 10000, 0.1, None,\n",
    "                      'Distance to the nearest landmark (km)', 'CDF of targets', None, xscale=\"log\")\n",
    "    plot_save(\"./fig/distance_to_landmark_log.pdf\", is_tight_layout=True)\n",
    "\n",
    "    print(f\"{with_landmarks_count} targets with at least one landmark\")\n",
    "    print(f\"{without_landmarks_count} targets without any landmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cdf landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_landmarks(data):\n",
    "    valid_landmarks_count = 0\n",
    "    unvalid_landmarks_count = 0\n",
    "    values = []\n",
    "    same_asn_lst = []\n",
    "    same_24_lst = []\n",
    "    same_bgp_lst = []\n",
    "    all_traceroutes_count = 0\n",
    "    no_r1_traceroutes_count = 0\n",
    "    asndb = pyasn.pyasn(IP_TO_ASN_FILE_PATH)\n",
    "    distances_to_landmarks = []\n",
    "    all_landmarks = []\n",
    "    bgp_prefixes = get_all_bgp_prefixes()\n",
    "    for _, d in data.items():\n",
    "        good = 0\n",
    "        bad = 0\n",
    "        same_asn = 0\n",
    "        diff_asn = 0\n",
    "        same_bgp = 0\n",
    "        diff_bgp = 0\n",
    "        same_24 = 0\n",
    "        diff_24 = 0\n",
    "        all_landmarks.append(0)\n",
    "        if \"tier2:cdn_count\" in d and \"tier2:landmark_count\" in d and \"tier2:failed_header_test_count\" in d:\n",
    "            all_landmarks[-1] += d['tier2:landmark_count'] + \\\n",
    "                d['tier2:cdn_count'] + d['tier2:failed_header_test_count']\n",
    "            valid_landmarks_count += d['tier2:landmark_count']\n",
    "            unvalid_landmarks_count += d['tier2:cdn_count'] + \\\n",
    "                d['tier2:failed_header_test_count']\n",
    "        if \"tier3:cdn_count\" in d and \"tier3:landmark_count\" in d and \"tier3:failed_header_test_count\" in d:\n",
    "            all_landmarks[-1] += d['tier3:landmark_count'] + \\\n",
    "                d['tier3:cdn_count'] + d['tier3:failed_header_test_count']\n",
    "            valid_landmarks_count += d['tier3:landmark_count']\n",
    "            unvalid_landmarks_count += d['tier3:cdn_count'] + \\\n",
    "                d['tier3:failed_header_test_count']\n",
    "        for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "            if f in d:\n",
    "                for t in d[f]:\n",
    "                    if t[4] < 0:\n",
    "                        bad += 1\n",
    "                    else:\n",
    "                        good += 1\n",
    "\n",
    "                    all_traceroutes_count += 1\n",
    "                    if t[3] == None:\n",
    "                        no_r1_traceroutes_count += 1\n",
    "\n",
    "                    ipt = t[1]\n",
    "                    ipl = t[2]\n",
    "                    asnt = asndb.lookup(ipt)[0]\n",
    "                    asnl = asndb.lookup(ipl)[0]\n",
    "                    if asnl != None and asnt != None:\n",
    "                        if asnt == asnl:\n",
    "                            same_asn += 1\n",
    "                        else:\n",
    "                            diff_asn += 1\n",
    "                    nt = ip_network(ipt+\"/24\", strict=False).network_address\n",
    "                    nl = ip_network(ipl+\"/24\", strict=False).network_address\n",
    "                    if nt == nl:\n",
    "                        same_24 += 1\n",
    "                    else:\n",
    "                        diff_24 += 1\n",
    "\n",
    "                    if is_same_bgp_prefix(ipt, ipl, bgp_prefixes):\n",
    "                        same_bgp += 1\n",
    "                    else:\n",
    "                        diff_bgp += 1\n",
    "        distances = []\n",
    "        for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
    "            target_geo = (d['lat_c'], d['lon_c'])\n",
    "            if f in d:\n",
    "                for l in d[f]:\n",
    "                    landmark_geo = (l[2], l[3])\n",
    "                    distances.append(haversine(target_geo, landmark_geo))\n",
    "        distances_to_landmarks.append(distances)\n",
    "\n",
    "        if same_asn != 0 or diff_asn != 0:\n",
    "            same_asn_lst.append(same_asn/(same_asn+diff_asn))\n",
    "\n",
    "        if same_24 != 0 or diff_24 != 0:\n",
    "            same_24_lst.append(same_24/(same_24+diff_24))\n",
    "            if same_24 != 0:\n",
    "                print(\n",
    "                    f\"Found {d['target_ip']} with a landmark in the same /24\")\n",
    "        if same_bgp != 0 or diff_bgp != 0:\n",
    "            same_bgp_lst.append(same_bgp/(diff_bgp+same_bgp))\n",
    "\n",
    "        if good != 0 or bad != 0:\n",
    "            values.append(bad/(bad+good))\n",
    "\n",
    "    print(f\"{no_r1_traceroutes_count} no r1 found out of {all_traceroutes_count}\")\n",
    "    plot_multiple_cdf([values], 10000, 0, 1,\n",
    "                      'Fraction of landmarks with\\nD1 + D2 < 0', 'CDF of targets', None)\n",
    "    plot_save(\"./fig/invalid_rtt.pdf\", is_tight_layout=True)\n",
    "\n",
    "    # plot_multiple_cdf([error3, error1, error4, error2], 10000, 0.1, None, 'Error distance (km)', 'CDF of error distance', [\"Street Level\", \"CBG\", \"Closest Landmarks\", \"Tier 2 estimation\"])\n",
    "    only_outside_asn = 0\n",
    "    for x in same_asn_lst:\n",
    "        if x == 0:\n",
    "            only_outside_asn += 1\n",
    "    only_outside_24 = 0\n",
    "    for x in same_24_lst:\n",
    "        if x == 0:\n",
    "            only_outside_24 += 1\n",
    "    only_outside_bgp = 0\n",
    "    for x in same_bgp_lst:\n",
    "        if x == 0:\n",
    "            only_outside_bgp += 1\n",
    "\n",
    "    print(f\"{valid_landmarks_count} total valid landmarks\")\n",
    "    print(f\"{unvalid_landmarks_count} unvalid landmarks\")\n",
    "    print(f\"{(valid_landmarks_count*100)/(valid_landmarks_count+unvalid_landmarks_count)}% valid landmarks\")\n",
    "\n",
    "    print(f\"{only_outside_asn} targets has all its landmarks outside its AS out of {len(same_asn_lst)} {only_outside_asn*100/(len(same_asn_lst))}%\")\n",
    "    print(f\"{only_outside_24} targets has all its landmarks outside its /24 out of {len(same_24_lst)} {only_outside_24*100/(len(same_24_lst))}%\")\n",
    "    print(f\"{only_outside_bgp} targets has all its landmarks outside its BGP prefix out of {len(same_bgp_lst)} {only_outside_bgp*100/(len(same_bgp_lst))}%\")\n",
    "\n",
    "    plot_multiple_cdf([same_asn_lst, same_bgp_lst, same_24_lst], 10000, None, None,\n",
    "                      'Fraction of landmarks and targets\\nsharing network', 'CDF of targets', ['ASN', 'BGP prefix', '/24'])\n",
    "    plt.legend(fontsize=\"14\")\n",
    "    plot_save(\"./fig/landmarks_targets_network.pdf\", is_tight_layout=True)\n",
    "\n",
    "    landmarks_all = []\n",
    "    landmarks_less_1 = []\n",
    "    landmarks_less_5 = []\n",
    "    landmarks_less_10 = []\n",
    "    landmarks_less_40 = []\n",
    "    total_count_ping = 0\n",
    "    for landmark_distances in distances_to_landmarks:\n",
    "        # if len(landmark_distances) == 0:\n",
    "        #     continue\n",
    "        landmarks_all.append(len(landmark_distances))\n",
    "        landmarks_less_1.append(len([i for i in landmark_distances if i <= 1]))\n",
    "        landmarks_less_5.append(len([i for i in landmark_distances if i <= 5]))\n",
    "        landmarks_less_10.append(\n",
    "            len([i for i in landmark_distances if i <= 10]))\n",
    "        landmarks_less_40.append(\n",
    "            len([i for i in landmark_distances if i <= 40]))\n",
    "        total_count_ping += len([i for i in landmark_distances if i <= 40])\n",
    "\n",
    "    print(f\"{total_count_ping} ping measurement to do\")\n",
    "\n",
    "    lm_a_0 = len([i for i in all_landmarks if i > 0])\n",
    "    lmv_a_0 = len([i for i in landmarks_all if i > 0])\n",
    "    lm1_0 = len([i for i in landmarks_less_1 if i > 0])\n",
    "    lm5_0 = len([i for i in landmarks_less_5 if i > 0])\n",
    "    lm10_0 = len([i for i in landmarks_less_10 if i > 0])\n",
    "    lm40_0 = len([i for i in landmarks_less_40 if i > 0])\n",
    "\n",
    "    lm1_1 = len([i for i in landmarks_less_1 if i >= 1])\n",
    "    print(lm1_1)\n",
    "\n",
    "    len_all = len(data)\n",
    "    print(f\"{lm_a_0} target have potentail landmarks or {lm_a_0/len_all}\")\n",
    "    print(f\"{lmv_a_0} target have valid landmarks or {lmv_a_0/len_all}\")\n",
    "    print(f\"{lm1_0} target with a landmark within 1 km or {lm1_0/len_all}\")\n",
    "    print(f\"{lm5_0} target with a landmark within 5 km or {lm5_0/len_all}\")\n",
    "    print(f\"{lm10_0} target with a landmark within 10 km or {lm10_0/len_all}\")\n",
    "    print(f\"{lm40_0} target with a landmark within 40 km or {lm40_0/len_all}\")\n",
    "\n",
    "    plot_multiple_cdf([all_landmarks, landmarks_all, landmarks_less_1, landmarks_less_5, landmarks_less_10, landmarks_less_40], 10000, 1, 0, 'Number of landmarks', 'CDF of targets', [\n",
    "                      'All potential landmarks', 'All valid landmarks', 'Landmarks within 1 km', 'Landmarks within 5 km', 'Landmarks within 10 km', 'Landmarks within 40 km'])\n",
    "    plt.legend(fontsize=\"14\")\n",
    "    plot_save(\"./fig/landmarks_count_per_target.pdf\", is_tight_layout=True)\n",
    "\n",
    "    plot_multiple_cdf([landmarks_less_1, landmarks_less_5, landmarks_less_10, landmarks_less_40], 10000, 1, 0, 'Number of landmarks',\n",
    "                      'CDF of targets', ['Landmarks within 1 km', 'Landmarks within 5 km', 'Landmarks within 10 km', 'Landmarks within 40 km'])\n",
    "    plt.legend(fontsize=\"14\")\n",
    "    plot_save(\"./fig/landmarks_count_per_target_40.pdf\", is_tight_layout=True)\n",
    "\n",
    "    plot_multiple_cdf([all_landmarks, landmarks_all, landmarks_less_1, landmarks_less_5, landmarks_less_10, landmarks_less_40], 10000, 0.8, 0, 'Number of landmarks', 'CDF of targets', [\n",
    "                      'All potential landmarks', 'All valid landmarks', 'Landmarks within 1 km', 'Landmarks within 5 km', 'Landmarks within 10 km', 'Landmarks within 40 km'], xscale=\"log\")\n",
    "    plt.legend(fontsize=\"14\")\n",
    "    plot_save(\"./fig/landmarks_count_per_target_log.pdf\", is_tight_layout=True)\n",
    "\n",
    "    plot_multiple_cdf([landmarks_less_1, landmarks_less_5, landmarks_less_10, landmarks_less_40], 10000, 1, 0, 'Number of landmarks', 'CDF of targets', [\n",
    "                      'Landmarks within 1 km', 'Landmarks within 5 km', 'Landmarks within 10 km', 'Landmarks within 40 km'], xscale=\"log\")\n",
    "    plt.legend(fontsize=\"14\")\n",
    "    plot_save(\"./fig/landmarks_count_per_target_40_log.pdf\",\n",
    "              is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_time_needed_to_geoloc(data):\n",
    "    time1 = []\n",
    "    time2 = []\n",
    "    time3 = []\n",
    "    values = []\n",
    "    for _, d in data.items():\n",
    "        if d['tier1:done'] and 'tier1:duration' in d:\n",
    "            time1.append(d['tier1:duration'])\n",
    "        if d['tier2:done'] and 'tier2:duration' in d:\n",
    "            time2.append(d['tier2:duration'])\n",
    "        if d['tier3:done'] and 'tier3:duration' in d:\n",
    "            time3.append(d['tier3:duration'])\n",
    "            values.append(d['tier1:duration'] +\n",
    "                          d['tier2:duration']+d['tier3:duration'])\n",
    "\n",
    "    median1 = np.median(time1)\n",
    "    median2 = np.median(time2)\n",
    "    median3 = np.median(time3)\n",
    "    median = np.median(values)\n",
    "\n",
    "    print(f\"tier 1 median duration = {median1}\")\n",
    "    print(f\"tier 2 median duration = {median2}\")\n",
    "    print(f\"tier 3 median duration = {median3}\")\n",
    "    print(f\"Street Level median duration = {median}\")\n",
    "\n",
    "    plot_multiple_cdf([values], 1000, None, None,\n",
    "                      'Time to geolocate a target (sec)', 'CDF of targets', None)\n",
    "    plot_save(\"./fig/cdf_time_to_geolocate.pdf\", is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measured distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measured_distance_vs_distance(data):\n",
    "    correlations = []\n",
    "    mdvd = {}\n",
    "    scater_plot_data = {}\n",
    "    for target_ip, d in data.items():\n",
    "        tmp_landmarks = {}\n",
    "        for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "            if f in d:\n",
    "                for t in d[f]:\n",
    "                    # if t[3] == None or t[4]<0:\n",
    "                    if t[4] < 0:\n",
    "                        continue\n",
    "                    landmarks_ip = t[2]\n",
    "                    measured_distance = rtt_to_km(t[4], 4/9, 300)\n",
    "                    distance = haversine(\n",
    "                        (t[5], t[6]), (d['lat_c'], d['lon_c']))\n",
    "                    if landmarks_ip not in tmp_landmarks:\n",
    "                        tmp_landmarks[landmarks_ip] = (\n",
    "                            measured_distance, distance)\n",
    "                    if measured_distance < tmp_landmarks[landmarks_ip][0]:\n",
    "                        tmp_landmarks[landmarks_ip] = (\n",
    "                            measured_distance, distance)\n",
    "        if len(tmp_landmarks) != 0:\n",
    "            tmp_dict = {'md': [], 'd': []}\n",
    "            for k, v in tmp_landmarks.items():\n",
    "                all_diff = True\n",
    "                for i in range(len(tmp_dict['d'])):\n",
    "                    if v[1] == tmp_dict['d'][i]:\n",
    "                        all_diff = False\n",
    "                if all_diff:\n",
    "                    tmp_dict['md'].append(v[0])\n",
    "                    tmp_dict['d'].append(v[1])\n",
    "            if len(tmp_dict['md']) > 1:\n",
    "                correlation = pearsonr(tmp_dict['md'], tmp_dict['d'])[0]\n",
    "                tmp_dict['correlation'] = correlation\n",
    "                correlations.append(correlation)\n",
    "                mdvd[d['target_ip']] = tmp_dict\n",
    "            if len(tmp_dict['md']) >= 5:  # and len(tmp_dict['md']) <= 15:\n",
    "                error = every_tier_result_and_errors(d)\n",
    "                if error['error3'] < 45:\n",
    "                    scater_plot_data[target_ip] = {\n",
    "                        'geo_loc_data': d, 'error_data': error, 'mdvd_data': tmp_dict}\n",
    "\n",
    "    medianc = np.median(correlations)\n",
    "    minc = min(correlations)\n",
    "    maxc = max(correlations)\n",
    "\n",
    "    print(f\"Measured Distance vs Distance median correlation = {medianc}\")\n",
    "    print(f\"Measured Distance vs Distance min correlation = {minc}\")\n",
    "    print(f\"Measured Distance vs Distance max correlation = {maxc}\")\n",
    "\n",
    "    plot_multiple_cdf([correlations], 10000, -1, 1,\n",
    "                      'Correlation Coef MD Vs D', 'CDF of Correlation Coef', None)\n",
    "    plot_save(\"./fig/cdf_md_vs_d.pdf\", is_tight_layout=True)\n",
    "\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    x3 = []\n",
    "    x4 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    y3 = []\n",
    "    y4 = []\n",
    "    for _, d in scater_plot_data.items():\n",
    "        if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 1:\n",
    "            if len(x1) == 0:\n",
    "                x1 = d['mdvd_data']['d']\n",
    "                y1 = d['mdvd_data']['md']\n",
    "            if len(x1) > len(d['mdvd_data']['d']):\n",
    "                x1 = d['mdvd_data']['d']\n",
    "                y1 = d['mdvd_data']['md']\n",
    "        if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 6 and d['error_data']['error3'] > 4:\n",
    "            if len(x2) == 0:\n",
    "                x2 = d['mdvd_data']['d']\n",
    "                y2 = d['mdvd_data']['md']\n",
    "            if len(x2) > len(d['mdvd_data']['d']):\n",
    "                x2 = d['mdvd_data']['d']\n",
    "                y2 = d['mdvd_data']['md']\n",
    "        if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 11 and d['error_data']['error3'] > 9:\n",
    "            if len(x3) == 0:\n",
    "                x3 = d['mdvd_data']['d']\n",
    "                y3 = d['mdvd_data']['md']\n",
    "            if len(x3) > len(d['mdvd_data']['d']):\n",
    "                x3 = d['mdvd_data']['d']\n",
    "                y3 = d['mdvd_data']['md']\n",
    "        if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 41 and d['error_data']['error3'] > 39:\n",
    "            if len(x4) == 0:\n",
    "                x4 = d['mdvd_data']['d']\n",
    "                y4 = d['mdvd_data']['md']\n",
    "            if len(x4) > len(d['mdvd_data']['d']):\n",
    "                x4 = d['mdvd_data']['d']\n",
    "                y4 = d['mdvd_data']['md']\n",
    "\n",
    "    list_color = ['r', 'b', 'g', 'y']\n",
    "    list_mak = ['o', '*', 'x', '+']\n",
    "    list_lab = ['< 1 km error', '5 km error', '10 km error', '40 km error']\n",
    "    plot_scatter_multiple([x1, x2, x3, x4], [y1, y2, y3, y4], None, None, 1, None, \"log\", \"log\",\n",
    "                          'Geographical distance (km)', 'Measured distance (km)', list_mak, list_color, [10, 10, 10, 10])\n",
    "    plt.legend(list_lab, fontsize=\"14\")\n",
    "    plot_save(\"./fig/scater_md_vs_d.pdf\", is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot(data):\n",
    "    with open(POPULATION_CITY_FILE_PATH, 'r') as json_file:\n",
    "        pop_data = json.load(json_file)\n",
    "\n",
    "    dens_lst = []\n",
    "    error_lst = []\n",
    "    for d in pop_data:\n",
    "        ip = d['target_ip']\n",
    "        pop = d['density']\n",
    "        dens_lst.append(pop)\n",
    "        errors = every_tier_result_and_errors(data[ip])\n",
    "        error_lst.append(errors['error3'])\n",
    "\n",
    "    fig, ax = plot_scatter_multiple([error_lst], [dens_lst], 0.1, 10000, 0.1, 100000, \"log\",\n",
    "                                    \"log\", 'Error distance (km)', 'Population Density (people/km²)', [\"x\"], [\"b\"], [10])\n",
    "    degree = 1\n",
    "    coef = np.polyfit(error_lst, dens_lst, deg=degree)\n",
    "    xseq = np.linspace(0, 10000, num=100)\n",
    "    yseq = [0 for i in range(len(xseq))]\n",
    "    for i in range(len(coef)):\n",
    "        power = len(coef) - i - 1\n",
    "        yseq = [(xseq[j]**power)*coef[i]+yseq[j] for j in range(len(xseq))]\n",
    "    ax.plot(xseq, yseq, color=\"k\", lw=2.5)\n",
    "    plot_save(\"./fig/scater_density.pdf\", is_tight_layout=True)\n",
    "\n",
    "    plot_multiple_cdf([dens_lst], 10000, None, None,\n",
    "                      'Population Density (people/km²)', 'CDF of targets', None, xscale=\"log\")\n",
    "    plot_save(\"./fig/cdf_density.pdf\", is_tight_layout=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review-fXCvvitn-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
