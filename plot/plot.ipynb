{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot\n",
    "Plot all the figures of the replication paper  \n",
    "To do after analysis/million_scale.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from scripts.utils.file_utils import load_json\n",
    "from scripts.analysis.analysis import compute_error_threshold_cdfs, every_tier_result_and_errors\n",
    "from scripts.utils.plot_utils import plot_multiple_cdf, homogenize_legend, plot_save, plot_multiple_error_bars, plot_scatter_multiple\n",
    "from scripts.utils.helpers import haversine, rtt_to_km\n",
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs number of VPs and subset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vs_n_vps_probes = load_json(ACCURACY_VS_N_VPS_PROBES_FILE)\n",
    "accuracy_vs_n_vps_probes = {\n",
    "    int(x): accuracy_vs_n_vps_probes[x] for x in accuracy_vs_n_vps_probes}\n",
    "X = sorted([x for x in sorted(accuracy_vs_n_vps_probes.keys())])\n",
    "Ys = [accuracy_vs_n_vps_probes[i] for i in X]\n",
    "Ys_med = [[np.median(x) for x in Ys]]\n",
    "Ys_err = [[np.std(x) for x in Ys]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2.a of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_multiple_error_bars(X, Ys_med, Ys_err,\n",
    "                                    xmin=10, xmax=10500, ymin=1, ymax=10000,\n",
    "                                    xlabel=\"Number of VPs\",\n",
    "                                    ylabel=\"Geolocation error (km)\",\n",
    "                                    xscale=\"log\",\n",
    "                                    yscale=\"log\",\n",
    "                                    labels=[\n",
    "                                        \"\"\n",
    "                                    ],\n",
    "\n",
    "                                    )\n",
    "\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = ACCURACY_VS_NB_VPS_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2.b of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sizes = [100, 500, 1000, 2000]\n",
    "\n",
    "labels = [f\"{s} VPs\" for s in subset_sizes]\n",
    "\n",
    "Ys = [accuracy_vs_n_vps_probes[i] for i in subset_sizes]\n",
    "\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of median error\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = ACCURACY_VS_SUBSET_SIZES_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBG with VPs threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2.c of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "\n",
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "    errors_threshold_probes_to_anchors)\n",
    "\n",
    "Ys = error_threshold_cdfs_p_to_a\n",
    "print(len(error_threshold_cdfs_p_to_a[0]))\n",
    "labels = [\"All VPs\"]\n",
    "labels.extend([f\"VPs > {t} km\" for t in THRESHOLD_DISTANCES if t > 0])\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\", legend_size=12)\n",
    "\n",
    "ofile = CBG_THRESHOLD_PROBES_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBG performance with original VP selection algorithm and new VP selection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3.a of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys = []\n",
    "labels = []\n",
    "results_file = [VP_SELECTION_ALGORITHM_PROBES_1_FILE, VP_SELECTION_ALGORITHM_PROBES_3_FILE, VP_SELECTION_ALGORITHM_PROBES_10_FILE]\n",
    "index = [1, 3, 10]\n",
    "\n",
    "for i, file in enumerate(results_file):\n",
    "    n_vps = index[i]\n",
    "    errors_threshold_vp_selection_algorithm = load_json(\n",
    "        results_file[i])\n",
    "    error_threshold_cdfs_p_to_a_vp_selection, circles_threshold_cdfs_p_to_a_vp_selection, _ = compute_error_threshold_cdfs(\n",
    "        errors_threshold_vp_selection_algorithm)\n",
    "    Ys.append(list(error_threshold_cdfs_p_to_a_vp_selection[0]))\n",
    "    labels.append(f\"{n_vps} closest VP (RTT)\")\n",
    "    if n_vps == 10:\n",
    "        # Take the baseline where 10 VPs are used to geolocate a target\n",
    "        error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "            errors_threshold_probes_to_anchors, errors_threshold_vp_selection_algorithm)\n",
    "        Ys.append(list(error_threshold_cdfs_p_to_a[0]))\n",
    "        labels.append(\"All VPs\")\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                        \"Geolocation error (km)\",\n",
    "                        \"CDF of targets\",\n",
    "                        xscale=\"log\",\n",
    "                        yscale=\"linear\",\n",
    "                        legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = CBG_THRESHOLD_VP_SELECTION_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3.b of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_based_algorithm_results = load_json(ROUND_BASED_ALGORITHM_FILE)\n",
    "\n",
    "round_based_algorithm_results = {int(x):round_based_algorithm_results[x] for x in round_based_algorithm_results}\n",
    "\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "    errors_threshold_probes_to_anchors)\n",
    "\n",
    "Ys_error = [error_threshold_cdfs_p_to_a[0]]\n",
    "Ys_n_vps = []\n",
    "\n",
    "labels_error = [\"All VPs\"]\n",
    "labels_n_vps = []\n",
    "\n",
    "\n",
    "for tier1_vps, results in sorted(round_based_algorithm_results.items()):\n",
    "    tier1_vps = int(tier1_vps)\n",
    "    error_cdf = [r[1] for r in results if r[1] is not None]\n",
    "    n_vps_cdf = [r[2] + tier1_vps for r in results if r[2] is not None]\n",
    "    label = f\"{tier1_vps} VPs\"\n",
    "    labels_error.append(label)\n",
    "    labels_n_vps.append(label)\n",
    "    Ys_error.append(error_cdf)\n",
    "    Ys_n_vps.append(n_vps_cdf)\n",
    "    print(tier1_vps, 3 * sum(n_vps_cdf))\n",
    "\n",
    "fig, ax = plot_multiple_cdf(Ys_error, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels_error)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = ROUND_ALGORITHM_ERROR_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error per continent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 4 of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_code_2_to_country():\n",
    "    country_by_iso_2 = {}\n",
    "    continent_by_iso_2 = {}\n",
    "    # Continent_Name,Continent_Code,Country_Name,Two_Letter_Country_Code,Three_Letter_Country_Code,Country_Number\n",
    "    with open(COUNTRIES_CSV_FILE) as f:\n",
    "        reader = csv.reader(f, delimiter=\",\", quotechar='\"')\n",
    "        next(reader, None)\n",
    "        for line in reader:\n",
    "            continent_code = line[1]\n",
    "            country_name = line[2].split(\",\")[0]\n",
    "            country_iso_code_2 = line[3]\n",
    "            country_by_iso_2[country_iso_code_2] = country_name\n",
    "            continent_by_iso_2[country_iso_code_2] = continent_code\n",
    "    return continent_by_iso_2, country_by_iso_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = load_json(ANCHORS_FILE)\n",
    "ip_per_country = {}\n",
    "for anchor in anchors:\n",
    "    if \"address_v4\" in anchor and \"geometry\" in anchor and \"coordinates\" in anchor[\"geometry\"]:\n",
    "        ip_v4_address = anchor[\"address_v4\"]\n",
    "        if ip_v4_address is None:\n",
    "            continue\n",
    "        country = anchor[\"country_code\"]\n",
    "        ip_per_country[ip_v4_address] = country\n",
    "\n",
    "country_per_ip = {}\n",
    "for ip, country in ip_per_country.items():\n",
    "    country_per_ip.setdefault(country, []).append(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute results per continent\n",
    "\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "\n",
    "continent_by_iso_2, country_by_iso_2 = iso_code_2_to_country()\n",
    "\n",
    "_, _, error_per_ip = compute_error_threshold_cdfs(errors_threshold_probes_to_anchors)\n",
    "\n",
    "error_per_continent_cdf = {}\n",
    "error_per_country_cdf = {}\n",
    "\n",
    "# Match the anchors of the second replicated paper\n",
    "anchors_second = list(set(load_json(ANCHORS_SECOND_PAPER_FILE)))\n",
    "for ip, error in error_per_ip.items():\n",
    "    if ip not in anchors_second:\n",
    "        continue\n",
    "    country = ip_per_country[ip]\n",
    "    continent = continent_by_iso_2[country]\n",
    "    error_per_continent_cdf.setdefault(continent, []).append(error)\n",
    "    error_per_country_cdf.setdefault(country, []).append(error)\n",
    "\n",
    "error_per_country_cdf_med = {country_by_iso_2[x]: (np.median(error_per_country_cdf[x]),\n",
    "                                                len(error_per_country_cdf[x]), len(country_per_ip[x])) for x in error_per_country_cdf}\n",
    "\n",
    "\n",
    "error_per_country_cdf_med_sorted = sorted(\n",
    "    error_per_country_cdf_med.items(), key=lambda x: x[1][0], reverse=True)\n",
    "print(error_per_country_cdf_med_sorted)\n",
    "\n",
    "Ys = [list(error_per_continent_cdf[c])\n",
    "        for c in error_per_continent_cdf]\n",
    "labels = [\n",
    "    f\"{c} ({len(error_per_continent_cdf[c])})\" for c in error_per_continent_cdf]\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "ofile = CBG_THRESHOLD_CONTINENT_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of the street level technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 5.a of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "error1 = []\n",
    "error2 = []\n",
    "error3 = []\n",
    "error4 = []\n",
    "\n",
    "filtered_error1 = []\n",
    "filtered_error2 = []\n",
    "filtered_error3 = []\n",
    "filtered_error4 = []\n",
    "for _, d in data.items():\n",
    "    errors = every_tier_result_and_errors(d)\n",
    "    error1.append(errors['error1'])\n",
    "    error2.append(errors['error2'])\n",
    "    error3.append(errors['error3'])\n",
    "    error4.append(errors['error4'])\n",
    "    if d['tier1:done'] and 'tier2:landmarks' in d and len(d['tier2:landmarks']) > 0:\n",
    "        filtered_error1.append(errors['error1'])\n",
    "        filtered_error2.append(errors['error2'])\n",
    "        filtered_error3.append(errors['error3'])\n",
    "        filtered_error4.append(errors['error4'])\n",
    "\n",
    "print(len(error1))\n",
    "print(len(error2))\n",
    "print(len(error3))\n",
    "print(len(error4))\n",
    "print(len([i for i in error4 if i <= 1]))\n",
    "\n",
    "street_lvl_count_cbg = 0\n",
    "street_lvl_count_tech = 0\n",
    "for e in error1:\n",
    "    if e <= 1:\n",
    "        street_lvl_count_cbg += 1\n",
    "for e in error3:\n",
    "    if e <= 1:\n",
    "        street_lvl_count_tech += 1\n",
    "print(f\"{street_lvl_count_cbg} targets are geolocated at street lvl using CBG {street_lvl_count_cbg/len(error1)}\")\n",
    "print(f\"{street_lvl_count_tech} targets are geolocated at street lvl using tech {street_lvl_count_tech/len(error3)}\")\n",
    "\n",
    "median1 = np.median(error1)\n",
    "median2 = np.median(error2)\n",
    "median3 = np.median(error3)\n",
    "median4 = np.median(error4)\n",
    "\n",
    "print(f\"tier 1 median error = {median1}\")\n",
    "print(f\"tier 2 median error = {median2}\")\n",
    "print(f\"tier 3 median error = {median3}\")\n",
    "print(f\"closest landmark distance median = {median4}\")\n",
    "\n",
    "fmedian1 = np.median(filtered_error1)\n",
    "fmedian2 = np.median(filtered_error2)\n",
    "fmedian3 = np.median(filtered_error3)\n",
    "fmedian4 = np.median(filtered_error4)\n",
    "\n",
    "print(f\"filtered tier 1 median error = {fmedian1}\")\n",
    "print(f\"filtered tier 2 median error = {fmedian2}\")\n",
    "print(f\"filtered tier 3 median error = {fmedian3}\")\n",
    "print(f\"filtered closest landmark distance median = {fmedian4}\")\n",
    "\n",
    "less_then_1 = 0\n",
    "less_then_1_lm = 0\n",
    "for e in error3:\n",
    "    if e <= 1:\n",
    "        less_then_1 += 1\n",
    "for e in error4:\n",
    "    if e <= 1:\n",
    "        less_then_1_lm += 1\n",
    "print(f\"{less_then_1} targets are geolocated at street lvl out of {len(error3)} or {less_then_1*100/len(error3)}%\")\n",
    "print(f\"{less_then_1_lm} targets has a landmark at street lvl out of {len(error4)} or {less_then_1_lm*100/len(error4)}%\")\n",
    "\n",
    "\n",
    "plot_multiple_cdf([error3, error1, error4], 10000, 0.1, None, 'Geolocation error (km)',\n",
    "                    'CDF of targets', [\"Street Level\", \"CBG\", \"Closest Landmark\"], xscale=\"log\")\n",
    "plt.legend(fontsize=\"14\")\n",
    "plot_save(CLOSE_LANDMARK_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 5.c of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "correlations = []\n",
    "mdvd = {}\n",
    "scater_plot_data = {}\n",
    "for target_ip, d in data.items():\n",
    "    tmp_landmarks = {}\n",
    "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "        if f in d:\n",
    "            for t in d[f]:\n",
    "                if t[4] < 0:\n",
    "                    continue\n",
    "                landmarks_ip = t[2]\n",
    "                measured_distance = rtt_to_km(t[4], 4/9, 300)\n",
    "                distance = haversine(\n",
    "                    (t[5], t[6]), (d['RIPE:lat'], d['RIPE:lon']))\n",
    "                if landmarks_ip not in tmp_landmarks:\n",
    "                    tmp_landmarks[landmarks_ip] = (\n",
    "                        measured_distance, distance)\n",
    "                if measured_distance < tmp_landmarks[landmarks_ip][0]:\n",
    "                    tmp_landmarks[landmarks_ip] = (\n",
    "                        measured_distance, distance)\n",
    "    if len(tmp_landmarks) != 0:\n",
    "        tmp_dict = {'md': [], 'd': []}\n",
    "        for k, v in tmp_landmarks.items():\n",
    "            all_diff = True\n",
    "            for i in range(len(tmp_dict['d'])):\n",
    "                if v[1] == tmp_dict['d'][i]:\n",
    "                    all_diff = False\n",
    "            if all_diff:\n",
    "                tmp_dict['md'].append(v[0])\n",
    "                tmp_dict['d'].append(v[1])\n",
    "        if len(tmp_dict['md']) > 1:\n",
    "            correlation = pearsonr(tmp_dict['md'], tmp_dict['d'])[0]\n",
    "            tmp_dict['correlation'] = correlation\n",
    "            correlations.append(correlation)\n",
    "            mdvd[d['target_ip']] = tmp_dict\n",
    "        if len(tmp_dict['md']) >= 5:  # and len(tmp_dict['md']) <= 15:\n",
    "            error = every_tier_result_and_errors(d)\n",
    "            if error['error3'] < 45:\n",
    "                scater_plot_data[target_ip] = {\n",
    "                    'geo_loc_data': d, 'error_data': error, 'mdvd_data': tmp_dict}\n",
    "\n",
    "medianc = np.median(correlations)\n",
    "minc = min(correlations)\n",
    "maxc = max(correlations)\n",
    "\n",
    "print(f\"Measured Distance vs Distance median correlation = {medianc}\")\n",
    "print(f\"Measured Distance vs Distance min correlation = {minc}\")\n",
    "print(f\"Measured Distance vs Distance max correlation = {maxc}\")\n",
    "\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "x3 = []\n",
    "x4 = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "y4 = []\n",
    "for _, d in scater_plot_data.items():\n",
    "    if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 1:\n",
    "        if len(x1) == 0:\n",
    "            x1 = d['mdvd_data']['d']\n",
    "            y1 = d['mdvd_data']['md']\n",
    "        if len(x1) > len(d['mdvd_data']['d']):\n",
    "            x1 = d['mdvd_data']['d']\n",
    "            y1 = d['mdvd_data']['md']\n",
    "    if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 6 and d['error_data']['error3'] > 4:\n",
    "        if len(x2) == 0:\n",
    "            x2 = d['mdvd_data']['d']\n",
    "            y2 = d['mdvd_data']['md']\n",
    "        if len(x2) > len(d['mdvd_data']['d']):\n",
    "            x2 = d['mdvd_data']['d']\n",
    "            y2 = d['mdvd_data']['md']\n",
    "    if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 11 and d['error_data']['error3'] > 9:\n",
    "        if len(x3) == 0:\n",
    "            x3 = d['mdvd_data']['d']\n",
    "            y3 = d['mdvd_data']['md']\n",
    "        if len(x3) > len(d['mdvd_data']['d']):\n",
    "            x3 = d['mdvd_data']['d']\n",
    "            y3 = d['mdvd_data']['md']\n",
    "    if d['error_data']['error3'] != d['error_data']['error1'] and d['error_data']['error3'] < 41 and d['error_data']['error3'] > 39:\n",
    "        if len(x4) == 0:\n",
    "            x4 = d['mdvd_data']['d']\n",
    "            y4 = d['mdvd_data']['md']\n",
    "        if len(x4) > len(d['mdvd_data']['d']):\n",
    "            x4 = d['mdvd_data']['d']\n",
    "            y4 = d['mdvd_data']['md']\n",
    "\n",
    "list_color = ['r', 'b', 'g', 'y']\n",
    "list_mak = ['o', '*', 'x', '+']\n",
    "list_lab = ['< 1 km error', '5 km error', '10 km error', '40 km error']\n",
    "plot_scatter_multiple([x1, x2, x3, x4], [y1, y2, y3, y4], None, None, 1, None, \"log\", \"log\",\n",
    "                        'Geographical distance (km)', 'Measured distance (km)', list_mak, list_color, [10, 10, 10, 10])\n",
    "plt.legend(list_lab, fontsize=\"14\")\n",
    "plot_save(SCATTER_DISTANCE_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraction of landmarks per target with unusable delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 6.a of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "values = []\n",
    "all_traceroutes_count = 0\n",
    "no_r1_traceroutes_count = 0\n",
    "\n",
    "for _, d in data.items():\n",
    "    good = 0\n",
    "    bad = 0\n",
    "\n",
    "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "        if f in d:\n",
    "            for t in d[f]:\n",
    "                if t[4] < 0:\n",
    "                    bad += 1\n",
    "                else:\n",
    "                    good += 1\n",
    "\n",
    "                all_traceroutes_count += 1\n",
    "                if t[3] == None:\n",
    "                    no_r1_traceroutes_count += 1\n",
    "\n",
    "\n",
    "\n",
    "    if good != 0 or bad != 0:\n",
    "        values.append(bad/(bad+good))\n",
    "\n",
    "print(f\"{no_r1_traceroutes_count} no r1 found out of {all_traceroutes_count}\")\n",
    "plot_multiple_cdf([values], 10000, 0, 1,\n",
    "                    'Fraction of landmarks with\\nD1 + D2 < 0', 'CDF of targets', None)\n",
    "plot_save(INVALID_RTT_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error distance vs population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "pop_data = load_json(POPULATION_CITY_FILE)\n",
    "\n",
    "dens_lst = []\n",
    "error_lst = []\n",
    "for d in pop_data:\n",
    "    ip = d['target_ip']\n",
    "    if ip not in data:\n",
    "        continue\n",
    "    pop = d['density']\n",
    "    dens_lst.append(pop)\n",
    "    errors = every_tier_result_and_errors(data[ip])\n",
    "    error_lst.append(errors['error3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 6.b of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_scatter_multiple([error_lst], [dens_lst], 0.1, 10000, 0.1, 100000, \"log\",\n",
    "                                \"log\", 'Error distance (km)', 'Population Density (people/km²)', [\"x\"], [\"b\"], [10])\n",
    "degree = 1\n",
    "coef = np.polyfit(error_lst, dens_lst, deg=degree)\n",
    "xseq = np.linspace(0, 10000, num=100)\n",
    "yseq = [0 for i in range(len(xseq))]\n",
    "for i in range(len(coef)):\n",
    "    power = len(coef) - i - 1\n",
    "    yseq = [(xseq[j]**power)*coef[i]+yseq[j] for j in range(len(xseq))]\n",
    "ax.plot(xseq, yseq, color=\"k\", lw=2.5)\n",
    "plot_save(SCATTER_DENSITY_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 8 of the replication paper (appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_cdf([dens_lst], 10000, None, None,\n",
    "                    'Population Density (people/km²)', 'CDF of targets', None, xscale=\"log\")\n",
    "plot_save(CDF_DENSITY_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to geolocate targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 6.c of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= load_json(ANALYZABLE_FILE)\n",
    "\n",
    "time1 = []\n",
    "time2 = []\n",
    "time3 = []\n",
    "values = []\n",
    "for _, d in data.items():\n",
    "    if d['tier1:done'] and 'tier1:duration' in d:\n",
    "        time1.append(d['tier1:duration'])\n",
    "    if d['tier2:done'] and 'tier2:duration' in d:\n",
    "        time2.append(d['tier2:duration'])\n",
    "    if d['tier3:done'] and 'tier3:duration' in d:\n",
    "        time3.append(d['tier3:duration'])\n",
    "        values.append(d['tier1:duration'] +\n",
    "                        d['tier2:duration']+d['tier3:duration'])\n",
    "\n",
    "median1 = np.median(time1)\n",
    "median2 = np.median(time2)\n",
    "median3 = np.median(time3)\n",
    "median = np.median(values)\n",
    "\n",
    "print(f\"tier 1 median duration = {median1}\")\n",
    "print(f\"tier 2 median duration = {median2}\")\n",
    "print(f\"tier 3 median duration = {median3}\")\n",
    "print(f\"Street Level median duration = {median}\")\n",
    "\n",
    "plot_multiple_cdf([values], 1000, None, None,\n",
    "                    'Time to geolocate a target (sec)', 'CDF of targets', None)\n",
    "plot_save(TIME_TO_GEOLOCATE_FILE, is_tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocation of CBG with all the RIPE Atlas VPs versus geolocation databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 7 of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_info_geo = load_json(IP_INFO_GEO_FILE)\n",
    "mm_geo = load_json(MAXMIND_GEO_FILE)\n",
    "errors_threshold_probes_to_anchors = load_json(PROBES_TO_ANCHORS_RESULT_FILE)\n",
    "removed_probes = load_json(REMOVED_PROBES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_threshold_cdfs_p_to_a, circles_threshold_cdfs_p_to_a, _ = compute_error_threshold_cdfs(\n",
    "    errors_threshold_probes_to_anchors)\n",
    "\n",
    "maxmind_error = {}\n",
    "ip_info_error = {}\n",
    "for i, anchor in enumerate(sorted(anchors, key=lambda x: x[\"address_v4\"])):\n",
    "    ip = anchor[\"address_v4\"]\n",
    "    if ip in removed_probes:\n",
    "        continue\n",
    "\n",
    "    if \"geometry\" not in anchor:\n",
    "        continue\n",
    "\n",
    "    long, lat = anchor[\"geometry\"][\"coordinates\"]\n",
    "    if ip in mm_geo:\n",
    "        error = haversine(mm_geo[ip], (lat, long))\n",
    "        maxmind_error[ip] = error\n",
    "\n",
    "    if ip in ip_info_geo:\n",
    "        ipinfo_lat, ipinfo_long = ip_info_geo[ip][\"loc\"].split(\",\")\n",
    "        ipinfo_lat, ipinfo_long = float(ipinfo_lat), float(ipinfo_long)\n",
    "        error = haversine((ipinfo_lat, ipinfo_long), (lat, long))\n",
    "        ip_info_error[ip] = error\n",
    "\n",
    "Ys = [error_threshold_cdfs_p_to_a[0], list(\n",
    "    maxmind_error.values()), list(ip_info_error.values())]\n",
    "print([len(Y) for Y in Ys])\n",
    "labels = [\"All VPs\", \"Maxmind (Free)\", \"IPinfo\"]\n",
    "fig, ax = plot_multiple_cdf(Ys, 10000, 1, 10000,\n",
    "                            \"Geolocation error (km)\",\n",
    "                            \"CDF of targets\",\n",
    "                            xscale=\"log\",\n",
    "                            yscale=\"linear\",\n",
    "                            legend=labels)\n",
    "homogenize_legend(ax, \"lower right\")\n",
    "\n",
    "ofile = GEO_DATABASE_FILE\n",
    "plot_save(ofile, is_tight_layout=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review-fXCvvitn-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
