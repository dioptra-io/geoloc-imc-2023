{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) ATLAS DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from random import shuffle\n",
    "\n",
    "from utils.file_utils import load_json, dump_json\n",
    "from utils.atlas_api import get_atlas_anchors, get_atlas_probes\n",
    "from default import ANCHORS_FILE, PROBES_FILE, PROBES_AND_ANCHORS_FILE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve atlas probes and anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected probes: 10344 (22.82%), rejected: 34986, geoloc rejected: 1\n"
     ]
    }
   ],
   "source": [
    "# create and fill the probes.json dataset.\n",
    "probes, probes_rejected, probes_geoloc_disputed = get_atlas_probes()\n",
    "dump_json(probes, PROBES_FILE)\n",
    "\n",
    "# display number of selected and rejected probes.\n",
    "print(\n",
    "    f\"selected probes: {len(probes)} ({round(len(probes) * 100 / (probes_rejected+len(probes)), 2)}%), rejected: {probes_rejected}, geoloc rejected: {probes_geoloc_disputed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected anchors: 780 (62.4%), rejected: 470, geoloc rejected: 1\n"
     ]
    }
   ],
   "source": [
    "# create and fill the anchors.json dataset.\n",
    "anchors, anchors_rejected, anchors_geoloc_disputed = get_atlas_anchors()\n",
    "dump_json(anchors, ANCHORS_FILE)\n",
    "\n",
    "# display number of selected and rejected anchors.\n",
    "print(\n",
    "    f\"selected anchors: {len(anchors)} ({round(len(anchors) * 100 / (anchors_rejected+len(anchors)), 2)}%), rejected: {anchors_rejected}, geoloc rejected: {anchors_geoloc_disputed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two files will be filtered and updated step by step during the execution of this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected probes: 9996\n",
      "selected anchors: 731\n",
      "{'address_v4': '82.217.219.124',\n",
      " 'asn_v4': 33915,\n",
      " 'country_code': 'NL',\n",
      " 'geometry': {'coordinates': [6.0075, 51.2005], 'type': 'Point'}}\n",
      "{'address_v4': '83.81.82.33',\n",
      " 'asn_v4': 33915,\n",
      " 'country_code': 'NL',\n",
      " 'geometry': {'coordinates': [6.0375, 51.2315], 'type': 'Point'}}\n",
      "{'address_v4': '95.247.234.173',\n",
      " 'asn_v4': 3269,\n",
      " 'country_code': 'IT',\n",
      " 'geometry': {'coordinates': [12.4375, 41.8995], 'type': 'Point'}}\n",
      "{'address_v4': '193.0.0.78',\n",
      " 'asn_v4': 3333,\n",
      " 'country_code': 'NL',\n",
      " 'geometry': {'coordinates': [4.8975, 52.3795], 'type': 'Point'}}\n",
      "{'address_v4': '77.174.30.45',\n",
      " 'asn_v4': 1136,\n",
      " 'country_code': 'NL',\n",
      " 'geometry': {'coordinates': [5.9585, 52.0075], 'type': 'Point'}}\n",
      "{'address_v4': '93.108.219.48',\n",
      " 'asn_v4': 12353,\n",
      " 'country_code': 'PT',\n",
      " 'geometry': {'coordinates': [-9.1515, 38.7295], 'type': 'Point'}}\n",
      "{'address_v4': '86.89.224.211',\n",
      " 'asn_v4': 12414,\n",
      " 'country_code': 'NL',\n",
      " 'geometry': {'coordinates': [4.8485, 52.3605], 'type': 'Point'}}\n",
      "{'address_v4': '216.147.126.54',\n",
      " 'asn_v4': 14593,\n",
      " 'country_code': 'US',\n",
      " 'geometry': {'coordinates': [-115.0215, 35.9515], 'type': 'Point'}}\n",
      "{'address_v4': '76.82.152.84',\n",
      " 'asn_v4': 20001,\n",
      " 'country_code': 'US',\n",
      " 'geometry': {'coordinates': [-117.1815, 32.8885], 'type': 'Point'}}\n",
      "{'address_v4': '2.57.252.147',\n",
      " 'asn_v4': 213279,\n",
      " 'country_code': 'NL',\n",
      " 'geometry': {'coordinates': [4.8885, 52.4005], 'type': 'Point'}}\n",
      "{'address_v4': '81.220.141.64',\n",
      " 'asn_v4': 15557,\n",
      " 'country_code': 'FR',\n",
      " 'geometry': {'coordinates': [1.6185, 43.2915], 'type': 'Point'}}\n"
     ]
    }
   ],
   "source": [
    "probes = load_json(PROBES_FILE)\n",
    "anchors = load_json(ANCHORS_FILE)\n",
    "\n",
    "print(f\"selected probes: {len(probes)}\")\n",
    "print(f\"selected anchors: {len(anchors)}\")\n",
    "\n",
    "for i, probe in enumerate(probes):\n",
    "    if i > 10:\n",
    "        break\n",
    "    pprint(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10727\n"
     ]
    }
   ],
   "source": [
    "probes_and_anchors = probes + anchors\n",
    "print(len(probes_and_anchors))\n",
    "shuffle(probes_and_anchors)\n",
    "\n",
    "dump_json(probes_and_anchors, PROBES_AND_ANCHORS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) GEOGRAPHIC DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import load_json, dump_json\n",
    "from utils.common import country_filtering\n",
    "from default import COUNTRIES_TXT_FILE, COUNTRIES_JSON_FILE, ANCHORS_FILE, PROBES_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get country dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD : {'latitude': '42.546245', 'longitude': '1.601554', 'name': 'Andorra'}\n",
      "AE : {'latitude': '23.424076', 'longitude': '53.847818', 'name': 'United'}\n",
      "AF : {'latitude': '33.93911', 'longitude': '67.709953', 'name': 'Afghanistan'}\n",
      "AG : {'latitude': '17.060816', 'longitude': '-61.796428', 'name': 'Antigua'}\n",
      "AI : {'latitude': '18.220554', 'longitude': '-63.068615', 'name': 'Anguilla'}\n",
      "AL : {'latitude': '41.153332', 'longitude': '20.168331', 'name': 'Albania'}\n",
      "AM : {'latitude': '40.069099', 'longitude': '45.038189', 'name': 'Armenia'}\n",
      "AN : {'latitude': '12.226079', 'longitude': '-69.060087', 'name': 'Netherlands'}\n",
      "AO : {'latitude': '-11.202692', 'longitude': '17.873887', 'name': 'Angola'}\n",
      "AQ : {'latitude': '-75.250973', 'longitude': '-0.071389', 'name': 'Antarctica'}\n",
      "AR : {'latitude': '-38.416097', 'longitude': '-63.616672', 'name': 'Argentina'}\n"
     ]
    }
   ],
   "source": [
    "countries = {}\n",
    "with open(COUNTRIES_TXT_FILE, \"r\", encoding=\"utf8\") as f:\n",
    "    for i, row in enumerate(f.readlines()):\n",
    "\n",
    "        row = [value.strip() for value in row.split(\" \")]\n",
    "        countries[row[0]] = {\n",
    "            \"latitude\": row[1],\n",
    "            \"longitude\": row[2],\n",
    "            \"name\": row[3],\n",
    "        }\n",
    "\n",
    "for i, (country_code, geoloc) in enumerate(countries.items()):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(f\"{country_code} : {geoloc}\")\n",
    "\n",
    "# save results\n",
    "dump_json(countries, COUNTRIES_JSON_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate default geolocated probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = load_json(PROBES_FILE)\n",
    "\n",
    "anchors = load_json(ANCHORS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Atlas probes kept: 9996, rejected: 0\n",
      "Number of Atlas anchors kept: 731, rejected: 0\n"
     ]
    }
   ],
   "source": [
    "filtered_probes = country_filtering(probes, countries)\n",
    "filtered_anchors = country_filtering(anchors, countries)\n",
    "\n",
    "print(\n",
    "    f\"Number of Atlas probes kept: {len(filtered_probes)}, rejected: {len(probes) - len(filtered_probes)}\")\n",
    "print(\n",
    "    f\"Number of Atlas anchors kept: {len(filtered_anchors)}, rejected: {len(anchors) - len(filtered_anchors)}\")\n",
    "\n",
    "# save results\n",
    "dump_json(filtered_anchors, ANCHORS_FILE)\n",
    "dump_json(filtered_probes, PROBES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) OTHER VARIOUS FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from clickhouse_driver import Client\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from utils.file_utils import load_json, dump_json\n",
    "from utils.helpers import haversine\n",
    "from utils.common import compute_remove_wrongly_geolocated_probes, compute_rtts_per_dst_src\n",
    "from utils.clickhouse_query import get_min_rtt_per_src_dst_query_ping_table, get_min_rtt_per_src_dst_prefix_query_ping_table\n",
    "from default import ADDRESS_FILE, HITLIST_FILE, ANCHORS_FILE, PROBES_FILE, PAIRWISE_DISTANCE_ANCHOR_FILE, PAIRWISE_DISTANCE_PROBE_FILE, PROBES_AND_ANCHORS_FILE, REMOVED_PROBES_FILE, GREEDY_PROBES_FILE, IP_INFO_GEO_FILE, MAXMIND_GEO_FILE\n",
    "\n",
    "DB_HOST = \"localhost\"\n",
    "GEO_REPLICATION_DB = \"geolocation_replication\"\n",
    "ANCHORS_MESHED_PING_TABLE = f\"anchors_meshed_pings\"\n",
    "PROBES_TO_ANCHORS_PING_TABLE = f\"ping_10k_to_anchors\"\n",
    "\n",
    "LIMIT = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ip level target list for all /24 prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_per_prefix = {}\n",
    "\n",
    "with open(ADDRESS_FILE, \"r\") as f:\n",
    "    for i, row in enumerate(f.readlines()[1:]):\n",
    "        row = row.split(\"\\t\")\n",
    "\n",
    "        # get prefix from hex value\n",
    "        prefix_hex = row[0]\n",
    "        prefix = [\"\".join(x) for x in zip(*[iter(prefix_hex)]*2)]\n",
    "        prefix = [int(x, 16) for x in prefix]\n",
    "        prefix = \".\".join(str(x) for x in prefix)\n",
    "\n",
    "        target_list = row[-1].strip(\"\\n\")\n",
    "        target_list = target_list.split(\",\")\n",
    "\n",
    "        # parse and save targets\n",
    "        if target_list[0] != '-':\n",
    "            for i, target in enumerate(target_list):\n",
    "                target_list[i] = prefix.split(\".\")[:-1]\n",
    "                target_list[i].append(str(int(target, 16)))\n",
    "                target_list[i] = \".\".join(target_list[i])\n",
    "\n",
    "            try:\n",
    "                targets_per_prefix[prefix].extend(target_list)\n",
    "            except KeyError:\n",
    "                targets_per_prefix[prefix] = target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json(targets_per_prefix, HITLIST_FILE)\n",
    "\n",
    "print(\"target hitlist\")\n",
    "for i, prefix in enumerate(targets_per_prefix):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(\"prefix:\", prefix, \"target hitlist:\", targets_per_prefix[prefix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pairwise matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = load_json(ANCHORS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_coordinates_per_ip = {}\n",
    "\n",
    "for anchor in anchors:\n",
    "    if \"address_v4\" in anchor and \"geometry\" in anchor and \"coordinates\" in anchor[\"geometry\"]:\n",
    "        ip_v4_address = anchor[\"address_v4\"]\n",
    "        if ip_v4_address is None:\n",
    "            continue\n",
    "        long, lat = anchor[\"geometry\"][\"coordinates\"]\n",
    "        vp_coordinates_per_ip[ip_v4_address] = lat, long\n",
    "\n",
    "\n",
    "vp_distance_matrix = {}\n",
    "vp_coordinates_per_ip_l = sorted(\n",
    "    vp_coordinates_per_ip.items(), key=lambda x: x[0])\n",
    "for i in range(len(vp_coordinates_per_ip_l)):\n",
    "    vp_i, vp_i_coordinates = vp_coordinates_per_ip_l[i]\n",
    "    for j in range(len(vp_coordinates_per_ip)):\n",
    "        vp_j, vp_j_coordinates = vp_coordinates_per_ip_l[j]\n",
    "        distance = haversine(vp_i_coordinates, vp_j_coordinates)\n",
    "        vp_distance_matrix.setdefault(vp_i, {})[vp_j] = distance\n",
    "        vp_distance_matrix.setdefault(vp_j, {})[vp_i] = distance\n",
    "\n",
    "\n",
    "dump_json(vp_distance_matrix, PAIRWISE_DISTANCE_ANCHOR_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = load_json(PROBES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_coordinates_per_ip = {}\n",
    "\n",
    "for probe in probes:\n",
    "    if \"address_v4\" in probe and \"geometry\" in probe and \"coordinates\" in probe[\"geometry\"]:\n",
    "        ip_v4_address = probe[\"address_v4\"]\n",
    "        if ip_v4_address is None:\n",
    "            continue\n",
    "        long, lat = probe[\"geometry\"][\"coordinates\"]\n",
    "        vp_coordinates_per_ip[ip_v4_address] = lat, long\n",
    "\n",
    "\n",
    "vp_distance_matrix = {}\n",
    "vp_coordinates_per_ip_l = sorted(\n",
    "    vp_coordinates_per_ip.items(), key=lambda x: x[0])\n",
    "for i in range(len(vp_coordinates_per_ip_l)):\n",
    "    vp_i, vp_i_coordinates = vp_coordinates_per_ip_l[i]\n",
    "    for j in range(len(vp_coordinates_per_ip)):\n",
    "        vp_j, vp_j_coordinates = vp_coordinates_per_ip_l[j]\n",
    "        distance = haversine(vp_i_coordinates, vp_j_coordinates)\n",
    "        vp_distance_matrix.setdefault(vp_i, {})[vp_j] = distance\n",
    "        vp_distance_matrix.setdefault(vp_j, {})[vp_i] = distance\n",
    "\n",
    "\n",
    "dump_json(vp_distance_matrix, PAIRWISE_DISTANCE_PROBE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find wrongly geolocated probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = load_json(ANCHORS_FILE)\n",
    "\n",
    "vp_distance_matrix = load_json(PAIRWISE_DISTANCE_ANCHOR_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_coordinates_per_ip = {}\n",
    "\n",
    "for anchor in anchors:\n",
    "    if \"address_v4\" in anchor and \"geometry\" in anchor and \"coordinates\" in anchor[\"geometry\"]:\n",
    "        ip_v4_address = anchor[\"address_v4\"]\n",
    "        if ip_v4_address is None:\n",
    "            continue\n",
    "        long, lat = anchor[\"geometry\"][\"coordinates\"]\n",
    "        vp_coordinates_per_ip[ip_v4_address] = lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_anchors = set()\n",
    "filter = \"\"\n",
    "\n",
    "rtt_per_srcs_dst = compute_rtts_per_dst_src(ANCHORS_MESHED_PING_TABLE, filter, threshold=300)\n",
    "\n",
    "removed_anchors = compute_remove_wrongly_geolocated_probes(rtt_per_srcs_dst,\n",
    "                                                            vp_coordinates_per_ip,\n",
    "                                                            vp_distance_matrix,\n",
    "                                                            removed_anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = load_json(PROBES_FILE)\n",
    "\n",
    "vp_distance_matrix = load_json(PAIRWISE_DISTANCE_PROBE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_coordinates_per_ip = {}\n",
    "\n",
    "for probe in probes:\n",
    "    if \"address_v4\" in probe and \"geometry\" in probe and \"coordinates\" in probe[\"geometry\"]:\n",
    "        ip_v4_address = probe[\"address_v4\"]\n",
    "        if ip_v4_address is None:\n",
    "            continue\n",
    "        long, lat = probe[\"geometry\"][\"coordinates\"]\n",
    "        vp_coordinates_per_ip[ip_v4_address] = lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_probes = set()\n",
    "filter = \"\"\n",
    "\n",
    "vp_coordinates_per_ip = {ip: vp_coordinates_per_ip[ip]\n",
    "                            for ip in vp_coordinates_per_ip}\n",
    "\n",
    "rtt_per_srcs_dst = compute_rtts_per_dst_src(PROBES_TO_ANCHORS_PING_TABLE, filter, threshold=300)\n",
    "\n",
    "removed_probes = compute_remove_wrongly_geolocated_probes(rtt_per_srcs_dst,\n",
    "                                                            vp_coordinates_per_ip,\n",
    "                                                            vp_distance_matrix,\n",
    "                                                            removed_anchors)\n",
    "\n",
    "removed_probes.update(removed_anchors)\n",
    "\n",
    "print(f\"Removing {len(removed_probes)} probes\")\n",
    "dump_json(removed_probes, REMOVED_PROBES_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove bad results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731 total anchors\n"
     ]
    }
   ],
   "source": [
    "anchors = load_json(ANCHORS_FILE)\n",
    "\n",
    "print(f\"{len(anchors)} total anchors\")\n",
    "\n",
    "removed_probes = load_json(REMOVED_PROBES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_anchors = []\n",
    "incorrect_geolocation_count = 0\n",
    "not_enough_vps = 0\n",
    "client = Client('127.0.0.1')\n",
    "\n",
    "for anchor in anchors:\n",
    "    if anchor['address_v4'] in removed_probes:\n",
    "        incorrect_geolocation_count += 1\n",
    "        continue\n",
    "    \n",
    "    target_ip = anchor['address_v4']\n",
    "    tmp_res_db = client.execute(\n",
    "        f'select distinct src_addr from bgp_interdomain_te.street_lvl_traceroutes where resp_addr = \\'{target_ip}\\' and dst_addr = \\'{target_ip}\\' and src_addr <> \\'{target_ip}\\'')\n",
    "    if len(tmp_res_db) < 100:\n",
    "        not_enough_vps += 1\n",
    "        continue\n",
    "    good_anchors.append(anchor)\n",
    "\n",
    "print(f\"{len(good_anchors)} anchors to keep\")\n",
    "print(f\"{incorrect_geolocation_count} anchors removed because of incorrect geolocation\")\n",
    "print(f\"{not_enough_vps} anchors removed because the lack of traceroute data towards\")\n",
    "\n",
    "dump_json(good_anchors, ANCHORS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9996 total probes\n"
     ]
    }
   ],
   "source": [
    "probes = load_json(PROBES_FILE)\n",
    "\n",
    "print(f\"{len(probes)} total probes\")\n",
    "\n",
    "removed_probes = load_json(REMOVED_PROBES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9996 probes to keep\n",
      "0 probes removed because of incorrect geolocation\n"
     ]
    }
   ],
   "source": [
    "good_probes = []\n",
    "incorrect_geolocation_count = 0\n",
    "\n",
    "for probe in probes:\n",
    "    if probe['address_v4'] in removed_probes:\n",
    "        incorrect_geolocation_count += 1\n",
    "        continue\n",
    "    good_probes.append(probe)\n",
    "\n",
    "print(f\"{len(good_probes)} probes to keep\")\n",
    "print(f\"{incorrect_geolocation_count} probes removed because of incorrect geolocation\")\n",
    "\n",
    "dump_json(good_probes, PROBES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create removed probes file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_probes = load_json(PROBES_AND_ANCHORS_FILE)\n",
    "good_anchors = load_json(ANCHORS_FILE)\n",
    "good_probes = load_json(PROBES_FILE)\n",
    "\n",
    "final_probes = []\n",
    "removed_probes = []\n",
    "for probe in original_probes:\n",
    "    if probe in good_anchors or probe in good_probes:\n",
    "        final_probes.append(probe)\n",
    "    else:\n",
    "        removed_probes.append(probe)\n",
    "\n",
    "dump_json(final_probes, PROBES_AND_ANCHORS_FILE)\n",
    "dump_json(removed_probes, REMOVED_PROBES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select greedy probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_probes = load_json(REMOVED_PROBES_FILE)\n",
    "\n",
    "vp_distance_matrix = load_json(PAIRWISE_DISTANCE_PROBE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedily compute the probe with the greatest distance to other probes\n",
    "\n",
    "# First of all remove entries with removed probes\n",
    "for probe in removed_probes:\n",
    "    if probe in vp_distance_matrix:\n",
    "        del vp_distance_matrix[probe]\n",
    "\n",
    "for probe, distance_per_probe in vp_distance_matrix.items():\n",
    "    for removed_probe in removed_probes:\n",
    "        if removed_probe in distance_per_probe:\n",
    "            del distance_per_probe[removed_probe]\n",
    "\n",
    "\n",
    "print(\"Starting greedy algorithm\")\n",
    "selected_probes = []\n",
    "remaining_probes = set(vp_distance_matrix.keys())\n",
    "with Pool(12) as p:\n",
    "    while len(remaining_probes) > 0 and len(selected_probes) < LIMIT:\n",
    "        args = []\n",
    "        for probe in remaining_probes:\n",
    "            args.append(\n",
    "                (probe, vp_distance_matrix[probe], selected_probes))\n",
    "        \n",
    "        distances_log = [math.log(distance_per_probe[p]) for p in selected_probes\n",
    "                     if p in distance_per_probe and distance_per_probe[p] > 0]\n",
    "        total_distance = sum(distances_log)\n",
    "        impl = probe, total_distance\n",
    "        results = p.starmap(impl, args)\n",
    "\n",
    "        furthest_probe_from_selected, _ = max(results, key=lambda x: x[1])\n",
    "        selected_probes.append(furthest_probe_from_selected)\n",
    "        remaining_probes.remove(furthest_probe_from_selected)\n",
    "\n",
    "\n",
    "dump_json(selected_probes, GREEDY_PROBES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find IP info and maxmind results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_date = \"20230516\"\n",
    "token = \"4f6c895ec9224f\"\n",
    "\n",
    "maxmind_block_file = f\"GeoLite2-City-CSV_{snapshot_date}/GeoLite2-City-Blocks-IPv4.csv\"\n",
    "maxmind_tree_file = f\"{maxmind_block_file[:-4]}_{snapshot_date}.tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = load_json(ANCHORS_FILE)\n",
    "\n",
    "removed_probes = load_json(REMOVED_PROBES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(maxmind_tree_file, \"rb\") as f:\n",
    "    tree = pickle.load(f)\n",
    "\n",
    "ip_info_geo = {}\n",
    "maxmind_geo = {}\n",
    "\n",
    "for i, anchor in enumerate(sorted(anchors, key=lambda x: x[\"address_v4\"])):\n",
    "    ip = anchor[\"address_v4\"]\n",
    "\n",
    "    # ip_info\n",
    "    url = f\"https://ipinfo.io/{ip}?token={token}\"\n",
    "    result = requests.get(url).json()\n",
    "    ip_info_geo[ip] = result\n",
    "\n",
    "    # maxmind_results\n",
    "    node = tree.search_best(ip)\n",
    "    if node is not None:\n",
    "        if \"city\" in node.data:\n",
    "            maxmind_geo[ip] = node.data[\"coordinates\"]\n",
    "\n",
    "dump_json(ip_info_geo, IP_INFO_GEO_FILE)\n",
    "dump_json(maxmind_geo, MAXMIND_GEO_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) FINISH COMPLETING GEOGRAPHIC DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file needed the final version of probes and anchors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import geopandas as gpd\\nimport rasterio\\n\\nfrom rasterio.transform import from_bounds'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from pprint import pprint\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "from utils.file_utils import load_json, dump_json\n",
    "from default import ANCHORS_FILE, POPULATION_CITY_FILE, CITIES_500_FILE, POPULATION_THRESHOLD\n",
    "\n",
    "'''import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "from rasterio.transform import from_bounds'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get population data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### city coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = load_json(ANCHORS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_with_location = []\n",
    "\n",
    "for anchor in anchors:\n",
    "    ip = anchor['address_v4']\n",
    "    lat = anchor['geometry']['coordinates'][1]\n",
    "    lon = anchor['geometry']['coordinates'][0]\n",
    "    country_code = anchor['country_code']\n",
    "    anchors_with_location.append({'target_ip': ip, 'lat': lat, 'lon': lon,\n",
    "                'country_code': country_code})\n",
    "\n",
    "\n",
    "anchors_with_city = []\n",
    "for anchor in anchors_with_location:\n",
    "    url = f\"http://nominatim.openstreetmap.org/reverse?format=geojson&lat={anchor['lat']}&lon={anchor['lon']}\"\n",
    "    r = requests.get(url)\n",
    "    elem = r.json()\n",
    "    if 'features' not in elem or len(elem['features']) != 1:\n",
    "        continue\n",
    "    info = elem['features'][0]\n",
    "    if 'properties' not in info or 'address' not in info['properties']:\n",
    "        continue\n",
    "    address = info['properties']['address']\n",
    "    if 'city' in address:\n",
    "        anchor['city'] = address['city']\n",
    "    elif 'village' in address:\n",
    "        anchor['city'] = address['village']\n",
    "    elif 'town' in address:\n",
    "        anchor['city'] = address['town']\n",
    "    elif 'country' in address:\n",
    "        anchor['city'] = address['country']\n",
    "    else:\n",
    "        pprint(info)\n",
    "        break\n",
    "    anchors_with_city.append(anchor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### city population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2601\n"
     ]
    }
   ],
   "source": [
    "population_by_city = {}\n",
    "\n",
    "\n",
    "with open(CITIES_500_FILE, \"r\", encoding=\"utf8\") as f:\n",
    "    for row in f.readlines():\n",
    "        row = [value.strip() for value in row.split(\"\\t\")]\n",
    "        city = row[1]\n",
    "        ascii_city = row[2]\n",
    "        # Iso2 code\n",
    "        country = row[8]\n",
    "        population = row[14]\n",
    "        city_key = f\"{city}_{country}\"\n",
    "        ascii_city_key = f\"{ascii_city}_{country}\"\n",
    "        if population != \"\":\n",
    "            population = int(float(population))\n",
    "            if population >= POPULATION_THRESHOLD:\n",
    "                population_by_city[ascii_city_key] = population\n",
    "                population_by_city[city_key] = population\n",
    "\n",
    "\n",
    "print(len(population_by_city)//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat C:\\Users\\milo2\\Desktop\\review\\datasets\\geography\\cities500.txt | grep \"Wien\"\n",
      "UnicodeDecodeError('utf-8', b\"'cat' n'est pas reconnu en tant que commande interne\\r\\nou externe, un programme ex\\x82cutable ou un fichier de commandes.\\r\\n\", 81, 82, 'invalid start byte')\n",
      "['1106542',\n",
      " 'Chitungwiza',\n",
      " 'Chitungwiza',\n",
      " 'Chitungviza,Chitungwiza,Chytungviza,Citungviza,chytwngwyza,Čitungviza,Читунгвиза,Читунгвіза,Чытунгвіза,چیتونگویزا',\n",
      " '-18.01274',\n",
      " '31.07555',\n",
      " 'P',\n",
      " 'PPL',\n",
      " 'ZW',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '340360',\n",
      " '',\n",
      " '1435',\n",
      " 'Africa/Harare',\n",
      " '2019-09-05']\n",
      "{'city': 'Wien',\n",
      " 'country_code': 'AT',\n",
      " 'lat': 48.2085,\n",
      " 'lon': 16.3695,\n",
      " 'target_ip': '193.171.255.2'}\n",
      "48.2085, 16.3695\n",
      "cat C:\\Users\\milo2\\Desktop\\review\\datasets\\geography\\cities500.txt | grep \"Satigny\"\n",
      "UnicodeDecodeError('utf-8', b\"'cat' n'est pas reconnu en tant que commande interne\\r\\nou externe, un programme ex\\x82cutable ou un fichier de commandes.\\r\\n\", 81, 82, 'invalid start byte')\n",
      "['1106542',\n",
      " 'Chitungwiza',\n",
      " 'Chitungwiza',\n",
      " 'Chitungviza,Chitungwiza,Chytungviza,Citungviza,chytwngwyza,Čitungviza,Читунгвиза,Читунгвіза,Чытунгвіза,چیتونگویزا',\n",
      " '-18.01274',\n",
      " '31.07555',\n",
      " 'P',\n",
      " 'PPL',\n",
      " 'ZW',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '340360',\n",
      " '',\n",
      " '1435',\n",
      " 'Africa/Harare',\n",
      " '2019-09-05']\n",
      "{'city': 'Satigny',\n",
      " 'country_code': 'FR',\n",
      " 'lat': 46.2285,\n",
      " 'lon': 6.0495,\n",
      " 'target_ip': '192.65.184.54'}\n",
      "46.2285, 6.0495\n",
      "cat C:\\Users\\milo2\\Desktop\\review\\datasets\\geography\\cities500.txt | grep \"Stockholms kommun\"\n",
      "UnicodeDecodeError('utf-8', b\"'cat' n'est pas reconnu en tant que commande interne\\r\\nou externe, un programme ex\\x82cutable ou un fichier de commandes.\\r\\n\", 81, 82, 'invalid start byte')\n",
      "['1106542',\n",
      " 'Chitungwiza',\n",
      " 'Chitungwiza',\n",
      " 'Chitungviza,Chitungwiza,Chytungviza,Citungviza,chytwngwyza,Čitungviza,Читунгвиза,Читунгвіза,Чытунгвіза,چیتونگویزا',\n",
      " '-18.01274',\n",
      " '31.07555',\n",
      " 'P',\n",
      " 'PPL',\n",
      " 'ZW',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '340360',\n",
      " '',\n",
      " '1435',\n",
      " 'Africa/Harare',\n",
      " '2019-09-05']\n",
      "{'city': 'Stockholms kommun',\n",
      " 'country_code': 'SE',\n",
      " 'lat': 59.3315,\n",
      " 'lon': 18.0595,\n",
      " 'target_ip': '185.42.136.158'}\n",
      "59.3315, 18.0595\n",
      "cat C:\\Users\\milo2\\Desktop\\review\\datasets\\geography\\cities500.txt | grep \"West Chester\"\n",
      "UnicodeDecodeError('utf-8', b\"'cat' n'est pas reconnu en tant que commande interne\\r\\nou externe, un programme ex\\x82cutable ou un fichier de commandes.\\r\\n\", 81, 82, 'invalid start byte')\n",
      "['1106542',\n",
      " 'Chitungwiza',\n",
      " 'Chitungwiza',\n",
      " 'Chitungviza,Chitungwiza,Chytungviza,Citungviza,chytwngwyza,Čitungviza,Читунгвиза,Читунгвіза,Чытунгвіза,چیتونگویزا',\n",
      " '-18.01274',\n",
      " '31.07555',\n",
      " 'P',\n",
      " 'PPL',\n",
      " 'ZW',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '340360',\n",
      " '',\n",
      " '1435',\n",
      " 'Africa/Harare',\n",
      " '2019-09-05']\n",
      "{'city': 'West Chester',\n",
      " 'country_code': 'US',\n",
      " 'lat': 39.9585,\n",
      " 'lon': -75.6095,\n",
      " 'target_ip': '76.26.115.194'}\n",
      "39.9585, -75.6095\n",
      "cat C:\\Users\\milo2\\Desktop\\review\\datasets\\geography\\cities500.txt | grep \"الدوحة\"\n",
      "UnicodeDecodeError('utf-8', b\"'cat' n'est pas reconnu en tant que commande interne\\r\\nou externe, un programme ex\\x82cutable ou un fichier de commandes.\\r\\n\", 81, 82, 'invalid start byte')\n",
      "['1106542',\n",
      " 'Chitungwiza',\n",
      " 'Chitungwiza',\n",
      " 'Chitungviza,Chitungwiza,Chytungviza,Citungviza,chytwngwyza,Čitungviza,Читунгвиза,Читунгвіза,Чытунгвіза,چیتونگویزا',\n",
      " '-18.01274',\n",
      " '31.07555',\n",
      " 'P',\n",
      " 'PPL',\n",
      " 'ZW',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '340360',\n",
      " '',\n",
      " '1435',\n",
      " 'Africa/Harare',\n",
      " '2019-09-05']\n",
      "{'city': 'الدوحة',\n",
      " 'country_code': 'QA',\n",
      " 'lat': 25.2775,\n",
      " 'lon': 51.5215,\n",
      " 'target_ip': '82.148.114.58'}\n",
      "25.2775, 51.5215\n",
      "cat C:\\Users\\milo2\\Desktop\\review\\datasets\\geography\\cities500.txt | grep \"Ashburn\"\n",
      "UnicodeDecodeError('utf-8', b\"'cat' n'est pas reconnu en tant que commande interne\\r\\nou externe, un programme ex\\x82cutable ou un fichier de commandes.\\r\\n\", 81, 82, 'invalid start byte')\n",
      "['1106542',\n",
      " 'Chitungwiza',\n",
      " 'Chitungwiza',\n",
      " 'Chitungviza,Chitungwiza,Chytungviza,Citungviza,chytwngwyza,Čitungviza,Читунгвиза,Читунгвіза,Чытунгвіза,چیتونگویزا',\n",
      " '-18.01274',\n",
      " '31.07555',\n",
      " 'P',\n",
      " 'PPL',\n",
      " 'ZW',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '340360',\n",
      " '',\n",
      " '1435',\n",
      " 'Africa/Harare',\n",
      " '2019-09-05']\n",
      "{'city': 'Ashburn',\n",
      " 'country_code': 'US',\n",
      " 'lat': 39.0385,\n",
      " 'lon': -77.4885,\n",
      " 'target_ip': '208.80.155.69'}\n",
      "39.0385, -77.4885\n",
      "cat C:\\Users\\milo2\\Desktop\\review\\datasets\\geography\\cities500.txt | grep \"Milano\"\n",
      "UnicodeDecodeError('utf-8', b\"'cat' n'est pas reconnu en tant que commande interne\\r\\nou externe, un programme ex\\x82cutable ou un fichier de commandes.\\r\\n\", 81, 82, 'invalid start byte')\n",
      "['1106542',\n",
      " 'Chitungwiza',\n",
      " 'Chitungwiza',\n",
      " 'Chitungviza,Chitungwiza,Chytungviza,Citungviza,chytwngwyza,Čitungviza,Читунгвиза,Читунгвіза,Чытунгвіза,چیتونگویزا',\n",
      " '-18.01274',\n",
      " '31.07555',\n",
      " 'P',\n",
      " 'PPL',\n",
      " 'ZW',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '340360',\n",
      " '',\n",
      " '1435',\n",
      " 'Africa/Harare',\n",
      " '2019-09-05']\n",
      "{'city': 'Milano',\n",
      " 'country_code': 'IT',\n",
      " 'lat': 45.4795,\n",
      " 'lon': 9.1015,\n",
      " 'target_ip': '217.29.76.27'}\n",
      "45.4795, 9.1015\n",
      "9\n",
      "0\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "anchors_with_pop = []\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "c3 = 0\n",
    "\n",
    "for anchor in anchors_with_city:\n",
    "    try:\n",
    "        anchor['population'] = population_by_city[f\"{anchor['city']}_{anchor['country_code']}\"]\n",
    "        c1 += 1\n",
    "    except:\n",
    "        try:\n",
    "            cmd = f\"cat {CITIES_500_FILE} | grep \\\"{anchor['city']}\\\"\"\n",
    "            print(cmd)\n",
    "            process = Popen(cmd, stdout=PIPE, stderr=STDOUT, shell=True)\n",
    "            output, err = process.communicate()\n",
    "            row = output.decode().split(\"\\t\")\n",
    "            anchor['population'] = int(row[14])\n",
    "            anchors_with_pop.append(anchor)\n",
    "            c2 += 1\n",
    "        except Exception as e:\n",
    "            pprint(e)\n",
    "            pprint(row)\n",
    "            pprint(anchor)\n",
    "            print(f\"{anchor['lat']}, {anchor['lon']}\")\n",
    "            anchor['population'] = 0\n",
    "            anchors_with_pop.append(anchor)\n",
    "            c3 += 1\n",
    "\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)\n",
    "dump_json(anchors_with_pop, POPULATION_CITY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### city density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the population density data\n",
    "with rasterio.open(f'{resources_dir}/gpw_v4_population_density_rev11_2020_30_sec.tif') as dataset:\n",
    "    population_density = dataset.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_with_pop = load_json(POPULATION_CITY_FILE)\n",
    "\n",
    "anchors_with_density = []\n",
    "for anchor in anchors_with_pop:\n",
    "    lat, lon = anchor['lat'], anchor['lon']\n",
    "    point = gpd.GeoDataFrame(geometry=gpd.points_from_xy([lon], [lat]))\n",
    "\n",
    "    # Convert lat-lon to pixel coordinates\n",
    "    xmin, ymin, xmax, ymax = dataset.bounds\n",
    "    transform = from_bounds(\n",
    "        xmin, ymin, xmax, ymax, dataset.width, dataset.height)\n",
    "    row, col = dataset.index(lon, lat)\n",
    "\n",
    "    # Extract the population density value\n",
    "    population_density_value = population_density[row, col]\n",
    "    anchor['density'] = float(population_density_value)\n",
    "\n",
    "    anchors_with_density.append(anchor)\n",
    "\n",
    "dump_json(anchors_with_density, POPULATION_CITY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pop = 0\n",
    "max_v = None\n",
    "for p in anchors_with_density:\n",
    "    if p['density'] > max_pop:\n",
    "        max_pop = p['density']\n",
    "        max_v = p\n",
    "\n",
    "pprint(max_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
