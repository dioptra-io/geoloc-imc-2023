{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing part 4\n",
    "\n",
    "Vantage points will probe the targets in a 3-step method, either by doing pings or traceroutes.  \n",
    "\n",
    "Vantage points are the Ripe Atlas anchors, then indireclty some online landmarks.  \n",
    "As always, targets are the anchors.  \n",
    "\n",
    "This notebook is an implementation of the street level method. Check the paper for more information.  \n",
    "To do after create_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "from pprint import pprint\n",
    "from clickhouse_driver import Client\n",
    "\n",
    "from scripts.utils.file_utils import load_json, dump_json\n",
    "from scripts.utils.helpers import haversine\n",
    "from scripts.street_level.traceroutes_results import serialize\n",
    "from scripts.street_level.three_tiers import get_all_info_geoloc\n",
    "from default import USER_ANCHORS_FILE, ANALYZABLE_FILE, CLICKHOUSE_DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### database for traceroutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why create this db? no insert data\n",
    "\n",
    "# Create Database\n",
    "client = Client(CLICKHOUSE_DB)\n",
    "query = 'CREATE DATABASE street_lvl'\n",
    "client.execute(query)\n",
    "\n",
    "# Create Traceroutes table\n",
    "query = (\n",
    "        f\"create table street_lvl.street_lvl_traceroutes\"\n",
    "        f\"(src_addr String, dst_prefix String, dst_addr String, resp_addr String, proto Int16, hop Int16, rtt Float64, ttl Int16, prb_id Int64, msm_id Int64, tstamp Datetime('UTC'))\"\n",
    "        f\"ENGINE = MergeTree()\"\n",
    "        f\"order by (dst_addr, src_addr, tstamp)\"\n",
    ")\n",
    "client.execute(query)\n",
    "client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "\n",
    "This would take a lot of time (more than a day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchors are the targets and Vantage points\n",
    "anchors = load_json(USER_ANCHORS_FILE)\n",
    "try:\n",
    "    all_res = load_json(ANALYZABLE_FILE)\n",
    "except FileNotFoundError:\n",
    "    all_res = {}\n",
    "\n",
    "i = 0\n",
    "for target in anchors:\n",
    "    try:\n",
    "        target_ip = target['address_v4']\n",
    "        if target_ip in all_res: # we skip targets already geolocated\n",
    "            continue\n",
    "        print(f\"{i}:{target_ip}\")\n",
    "        i += 1\n",
    "\n",
    "        res = get_all_info_geoloc(target_ip)\n",
    "        res = serialize(res)\n",
    "        # We save the coordinates of the targets as given by RIPE Atlas\n",
    "        res['RIPE:lat'] = target['geometry']['coordinates'][1]\n",
    "        res['RIPE:lon'] = target['geometry']['coordinates'][0]\n",
    "\n",
    "        # We save the error of the estimated geolocation at each step\n",
    "        if res['lat'] != None and res['lon'] != None:\n",
    "            res['error'] = haversine(\n",
    "                (res['lat'], res['lon']), (res['RIPE:lat'], res['RIPE:lon']))\n",
    "        if 'tier1:lat' in res and 'tier1:lon' in res and res['tier1:lat'] != None and res['tier1:lon'] != None:\n",
    "            res['tier1:error'] = haversine(\n",
    "                (res['tier1:lat'], res['tier1:lon']), (res['RIPE:lat'], res['RIPE:lon']))\n",
    "        if 'tier2:lat' in res and 'tier2:lon' in res and res['tier2:lat'] != None and res['tier2:lon'] != None:\n",
    "            res['tier2:error'] = haversine(\n",
    "                (res['tier2:lat'], res['tier2:lon']), (res['RIPE:lat'], res['RIPE:lon']))\n",
    "        if 'tier3:lat' in res and 'tier3:lon' in res and res['tier3:lat'] != None and res['tier3:lon'] != None:\n",
    "            res['tier3:error'] = haversine(\n",
    "                (res['tier3:lat'], res['tier3:lon']), (res['RIPE:lat'], res['RIPE:lon']))\n",
    "\n",
    "        all_res[target_ip] = res\n",
    "        # We save the results\n",
    "        dump_json(all_res, ANALYZABLE_FILE)\n",
    "    except Exception:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocat one IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_ip': '195.83.132.129', 'tier1:done': False, 'tier2:done': False, 'tier3:done': False, 'negative_rtt_included': True, 'speed_threshold': 0.6666666666666666, 'tier1:lat': None, 'tier1:lon': None, 'vps': set(), 'tier1:duration': 1282.0457310676575, 'lat': None, 'lon': None}\n"
     ]
    }
   ],
   "source": [
    "target_ip = '195.83.132.129' # LAAS/CNRS\n",
    "geolocation = get_all_info_geoloc(target_ip)\n",
    "#geolocation = geoloc(target_ip)\n",
    "print(geolocation)\n",
    "geolocation = serialize(geolocation)\n",
    "dump_json(geolocation, 'res_tmp.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review-8XQ99qZ1-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
