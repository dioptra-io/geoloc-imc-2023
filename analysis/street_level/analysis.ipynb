{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyasn\n",
    "from env_project import ANALYZABLE_FILE_PATH, IP_TO_ASN_FILE_PATH, POPULATION_CITY_FILE_PATH\n",
    "\n",
    "from clickhouse_driver import Client\n",
    "from plot_utils.plot import plot_multiple_cdf, plot_scatter, plot_save, plot_scatter_multiple\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from geoloc_earth import get_points_in_poly, plot_circles_and_points\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import ujson as json\n",
    "from helpers import haversine, rtt_to_km, is_within_cirle, polygon_centroid, circle_intersections, distance\n",
    "import sys\n",
    "sys.path.insert(0, './geoloc-imc-2023/geoloc_imc_2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbg_evaluation(data):\n",
    "    bad = 0\n",
    "    good = 0\n",
    "    good_23_only = 0\n",
    "    empty_vps = 0\n",
    "    not_empty_vps = 0\n",
    "    targeted_traceroutes = 0\n",
    "    not_targeted_traceroutes = 0\n",
    "    vps_not_working = []\n",
    "    for _, d in data.items():\n",
    "        if d['tier1:done']:\n",
    "            good += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "            points = get_points_in_poly(d['vps'], 36, 5, 4/9)\n",
    "            if len(points) != 0:\n",
    "                print(len(points))\n",
    "            tmp_vps = []\n",
    "            for vp in d['vps']:\n",
    "                tmp_vps.append((vp[0], vp[1], vp[2], None, None))\n",
    "            points = get_points_in_poly(tmp_vps, 36, 5, 2/3)\n",
    "            if len(points) != 0:\n",
    "                good_23_only += 1\n",
    "            else:\n",
    "                if len(d['vps']) > 0:\n",
    "                    not_empty_vps += 1\n",
    "                    vps_not_working.append(d['target_ip'])\n",
    "                else:\n",
    "                    empty_vps += 1\n",
    "                    client = Client('127.0.0.1')\n",
    "                    tmp_row = client.execute(\n",
    "                        f'select src_addr, rtt, tstamp from bgp_interdomain_te.street_lvl_traceroutes where dst_addr = \\'{d[\"target_ip\"]}\\'')\n",
    "                    if len(tmp_row) != 0:\n",
    "                        targeted_traceroutes += 1\n",
    "                        print(f\"{d['target_ip']} was targeted by traceroutes\")\n",
    "                    else:\n",
    "                        not_targeted_traceroutes += 1\n",
    "\n",
    "    print(vps_not_working)\n",
    "    print(f\"{bad} no intersection out or {bad+good} = {bad/(bad+good)}\")\n",
    "    print(\n",
    "        f\"If the speed where to be 2/3 CBG would have worked for {good_23_only} more targets\")\n",
    "    print(f\"{empty_vps} no vp, {not_empty_vps} some vps\")\n",
    "    print(\n",
    "        f\"When no vp found {targeted_traceroutes} target had a traceroute dedecated to it and {not_targeted_traceroutes} did not\")\n",
    "\n",
    "    position_in = 0\n",
    "    position_out = 0\n",
    "    would_be_in = 0\n",
    "    for _, d in data.items():\n",
    "        if not d['tier1:done']:\n",
    "            continue\n",
    "        all_in = True\n",
    "        candidate_geo = (d['lat_c'], d['lon_c'])\n",
    "        for vp in d['vps']:\n",
    "            if not is_within_cirle((vp[0], vp[1]), vp[2], candidate_geo, speed_threshold=4/9):\n",
    "                all_in = False\n",
    "        if all_in:\n",
    "            position_in += 1\n",
    "        else:\n",
    "            position_out += 1\n",
    "            all_in = True\n",
    "            for vp in d['vps']:\n",
    "                if not is_within_cirle((vp[0], vp[1]), vp[2], candidate_geo, speed_threshold=2/3):\n",
    "                    all_in = False\n",
    "            if all_in:\n",
    "                would_be_in += 1\n",
    "            else:\n",
    "                print(f\"{d['target_ip']} is always outside the CBG area\")\n",
    "\n",
    "    print(f\"the target was in the CBG area {position_in} times\")\n",
    "    print(f\"the target was out of the CBG area {position_out} times\")\n",
    "    print(f\"CBG failed {position_out*100/(position_in+position_out)}%\")\n",
    "    print(\n",
    "        f\"If we would use 2/3 {would_be_in} extra targets would be in the CBG area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def success_rate(data):\n",
    "    feilds_count = {'tier1:done': 0, 'tier2:done': 0, 'tier3:done': 0}\n",
    "    for _, d in data.items():\n",
    "        for feild in feilds_count:\n",
    "            if d[feild]:\n",
    "                feilds_count[feild] += 1\n",
    "    print(f\"{len(data)} Total targets done\")\n",
    "    for k, v in feilds_count.items():\n",
    "        print(f\"{v} {k}\")\n",
    "\n",
    "    dict_reasons = {\n",
    "        'tier2_failed_because_no_zipcodes': 0,\n",
    "        'tier2_failed_because_no_landmark': 0,\n",
    "        'tier2_failed_because_no_valid_traceroute': 0,\n",
    "        'tier2_failed_because_other': 0,\n",
    "        'tier3_failed_because_no_zipcodes': 0,\n",
    "        'tier3_failed_because_no_landmark': 0,\n",
    "        'tier3_failed_because_no_valid_traceroute': 0,\n",
    "        'tier3_failed_because_other': 0\n",
    "    }\n",
    "\n",
    "    for _, d in data.items():\n",
    "        if not d['tier1:done']:  # Here you should analyse tier1\n",
    "            continue\n",
    "        if not d['tier2:done']:\n",
    "            if d['tier2:inspected_points_count'] == 0:\n",
    "                dict_reasons['tier2_failed_because_no_zipcodes'] += 1\n",
    "                continue\n",
    "            if d['tier2:landmark_count'] == 0:\n",
    "                dict_reasons['tier2_failed_because_no_landmark'] += 1\n",
    "                continue\n",
    "            one_traceroute_found = False\n",
    "            for t in d['tier2:traceroutes']:\n",
    "                if t[4] > 0:\n",
    "                    one_traceroute_found = True\n",
    "                    break\n",
    "            if not one_traceroute_found:\n",
    "                dict_reasons['tier2_failed_because_no_valid_traceroute'] += 1\n",
    "                continue\n",
    "            dict_reasons['tier2_failed_because_other'] += 1\n",
    "            continue\n",
    "        if not d['tier3:done']:\n",
    "            if d['tier3:inspected_points_count'] == 0:\n",
    "                dict_reasons['tier3_failed_because_no_zipcodes'] += 1\n",
    "                # if d['target_ip'] not in ['185.28.221.65', '46.183.219.225']:\n",
    "                #    print(d['target_ip'])\n",
    "                #    exit()\n",
    "                continue\n",
    "            if d['tier3:landmark_count'] == 0:\n",
    "                dict_reasons['tier3_failed_because_no_landmark'] += 1\n",
    "                continue\n",
    "            one_traceroute_found = False\n",
    "            for t in d['tier3:traceroutes']:\n",
    "                if t[4] > 0:\n",
    "                    one_traceroute_found = True\n",
    "                    break\n",
    "            if not one_traceroute_found:\n",
    "                dict_reasons['tier3_failed_because_no_valid_traceroute'] += 1\n",
    "                continue\n",
    "            dict_reasons['tier3_failed_because_other'] += 1\n",
    "            continue\n",
    "\n",
    "    for k, v in dict_reasons.items():\n",
    "        print(f\"{k} {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API calls count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_calles_count(data):\n",
    "    zipcodes_counts = []\n",
    "    landmarks_counts = []\n",
    "    traceroutes_counts = []\n",
    "    for _, d in data.items():\n",
    "        zipcodes_count = 0\n",
    "        landmarks_count = 0\n",
    "        traceroutes_count = 0\n",
    "\n",
    "        for f in ['tier2:inspected_points_count', 'tier3:inspected_points_count']:\n",
    "            if f in d:\n",
    "                zipcodes_count += d[f]\n",
    "        if zipcodes_count != 0:\n",
    "            zipcodes_counts.append(zipcodes_count)\n",
    "\n",
    "        for f in [\"tier2:failed_dns_count\", \"tier2:failed_asn_count\", \"tier2:cdn_count\", \"tier2:non_cdn_count\", \"tier3:failed_dns_count\", \"tier3:failed_asn_count\", \"tier3:cdn_count\", \"tier3:non_cdn_count\"]:\n",
    "            if f in d:\n",
    "                landmarks_count += d[f]\n",
    "        if landmarks_count != 0:\n",
    "            landmarks_counts.append(landmarks_count)\n",
    "\n",
    "        for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "            if f in d:\n",
    "                traceroutes_count += len(d[f])\n",
    "        if traceroutes_count != 0:\n",
    "            traceroutes_counts.append(traceroutes_count)\n",
    "\n",
    "    print(f\"{np.median(zipcodes_counts)} Zipcode to check (median)\")\n",
    "    print(f\"{np.median(traceroutes_counts)} traceroutes to check (median)\")\n",
    "    print(f\"{np.median(landmarks_counts)} landmarks to check (median)\")\n",
    "\n",
    "    total = 0\n",
    "    for zip in zipcodes_counts:\n",
    "        total += zip\n",
    "    print(f\"{total} Overpass queries\")\n",
    "    total = 0\n",
    "    for x in landmarks_counts:\n",
    "        total += x\n",
    "    print(f\"{total} landmarks verification\")\n",
    "    total = 0\n",
    "    for x in traceroutes_counts:\n",
    "        total += x\n",
    "    print(f\"{total} traceroutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation same network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_same_network(data):\n",
    "    asn_coef_lst = []\n",
    "    bgp_coef_lst = []\n",
    "    asndb = pyasn.pyasn(IP_TO_ASN_FILE_PATH)\n",
    "    bgp_prefixes = get_all_bgp_prefixes()\n",
    "    for _, d in data.items():\n",
    "        same_bgp_x = []\n",
    "        same_bgp_y = []\n",
    "        same_asn_x = []\n",
    "        same_asn_y = []\n",
    "        for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "            if f in d:\n",
    "                for t in d[f]:\n",
    "                    if t[4] < 0:\n",
    "                        continue\n",
    "                    distance = haversine(\n",
    "                        (t[5], t[6]), (d['lat_c'], d['lon_c']))\n",
    "                    ipt = t[1]\n",
    "                    ipl = t[2]\n",
    "                    asnt = asndb.lookup(ipt)[0]\n",
    "                    asnl = asndb.lookup(ipl)[0]\n",
    "                    if asnl != None and asnt != None:\n",
    "                        if asnt == asnl and distance not in same_asn_y:\n",
    "                            same_asn_y.append(distance)\n",
    "                            same_asn_x.append(t[4])\n",
    "\n",
    "                    if is_same_bgp_prefix(ipt, ipl, bgp_prefixes):\n",
    "                        if distance not in same_bgp_y:\n",
    "                            same_bgp_y.append(distance)\n",
    "                            same_bgp_x.append(t[4])\n",
    "        if len(same_asn_x) > 1:\n",
    "            correlation = pearsonr(same_asn_x, same_asn_y)[0]\n",
    "            asn_coef_lst.append(correlation)\n",
    "        if len(same_bgp_x) > 1:\n",
    "            correlation = pearsonr(same_bgp_x, same_bgp_y)[0]\n",
    "            bgp_coef_lst.append(correlation)\n",
    "\n",
    "    print(f\"{len(asn_coef_lst)} targets with correlation asn\")\n",
    "    print(f\"{len(bgp_coef_lst)} targets with correlation bgp\")\n",
    "    print(f\"{np.median(bgp_coef_lst)} median bgp correlation\")\n",
    "    print(f\"{np.median(asn_coef_lst)} median asn correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ping_go_do(data):\n",
    "    res_dct = {}\n",
    "    res = []\n",
    "    for _, d in data.items():\n",
    "        for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
    "            target_geo = (d['lat_c'], d['lon_c'])\n",
    "            if f in d:\n",
    "                for l in d[f]:\n",
    "                    landmark_geo = (l[2], l[3])\n",
    "                    distance = haversine(target_geo, landmark_geo)\n",
    "                    if distance <= 40:\n",
    "                        res_dct[(d['target_ip'], l[0])] = float(distance)\n",
    "    print(len(res_dct))\n",
    "    for k, v in res_dct.items():\n",
    "        res.append({'target_ip': k[0], 'landmark_ip': k[1], 'distance': v})\n",
    "\n",
    "    with open(\"ping_todo.json\", 'w') as outfile:\n",
    "        json.dump(res, outfile)\n",
    "\n",
    "    return res\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
