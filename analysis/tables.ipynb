{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Print tables\n",
                "\n",
                "Print all the tables of the replication paper  \n",
                "To do after analysis/million_scale.ipynb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyasn\n",
                "\n",
                "from ipaddress import ip_network\n",
                "from clickhouse_driver import Client\n",
                "\n",
                "from scripts.utils.file_utils import load_json\n",
                "from scripts.utils.clickhouse import Clickhouse\n",
                "from scripts.analysis.analysis import get_all_bgp_prefixes, is_same_bgp_prefix, every_tier_result_and_errors\n",
                "from scripts.utils.helpers import haversine\n",
                "from default import IP_TO_ASN_FILE, ANALYZABLE_FILE, ROUND_BASED_ALGORITHM_FILE, TARGET_TO_LANDMARKS_PING_TABLE"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Measurement overhead"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Figure 3.c of the replication paper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "round_based_algorithm_results = load_json(ROUND_BASED_ALGORITHM_FILE)\n",
                "\n",
                "round_based_algorithm_results = {\n",
                "int(x): round_based_algorithm_results[x] for x in round_based_algorithm_results}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "10 5785182\n",
                        "100 4459050\n",
                        "300 3205290\n",
                        "500 2800245\n",
                        "1000 2817933\n"
                    ]
                }
            ],
            "source": [
                "for tier1_vps, results in sorted(round_based_algorithm_results.items()):\n",
                "        tier1_vps = int(tier1_vps)\n",
                "        n_vps_cdf = [r[2] + tier1_vps for r in results if r[2] is not None]\n",
                "        print(tier1_vps, 3 * sum(n_vps_cdf))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Number of landmarks within a certain radius"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Figure 5.b of the replication paper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 78.128.211.119 with a landmark in the same /24\n",
                        "Found 77.109.180.62 with a landmark in the same /24\n",
                        "Found 103.143.136.43 with a landmark in the same /24\n"
                    ]
                }
            ],
            "source": [
                "data = load_json(ANALYZABLE_FILE)\n",
                "\n",
                "valid_landmarks_count = 0\n",
                "unvalid_landmarks_count = 0\n",
                "same_asn_lst = []\n",
                "same_24_lst = []\n",
                "same_bgp_lst = []\n",
                "distances_to_landmarks = []\n",
                "all_landmarks = []\n",
                "asndb = pyasn.pyasn(str(IP_TO_ASN_FILE))\n",
                "bgp_prefixes = get_all_bgp_prefixes()\n",
                "\n",
                "for _, d in data.items():\n",
                "    same_asn = 0\n",
                "    diff_asn = 0\n",
                "    same_bgp = 0\n",
                "    diff_bgp = 0\n",
                "    same_24 = 0\n",
                "    diff_24 = 0\n",
                "    all_landmarks.append(0)\n",
                "    if \"tier2:cdn_count\" in d and \"tier2:landmark_count\" in d and \"tier2:failed_header_test_count\" in d:\n",
                "        all_landmarks[-1] += d['tier2:landmark_count'] + \\\n",
                "            d['tier2:cdn_count'] + d['tier2:failed_header_test_count']\n",
                "        valid_landmarks_count += d['tier2:landmark_count']\n",
                "        unvalid_landmarks_count += d['tier2:cdn_count'] + \\\n",
                "            d['tier2:failed_header_test_count']\n",
                "    if \"tier3:cdn_count\" in d and \"tier3:landmark_count\" in d and \"tier3:failed_header_test_count\" in d:\n",
                "        all_landmarks[-1] += d['tier3:landmark_count'] + \\\n",
                "            d['tier3:cdn_count'] + d['tier3:failed_header_test_count']\n",
                "        valid_landmarks_count += d['tier3:landmark_count']\n",
                "        unvalid_landmarks_count += d['tier3:cdn_count'] + \\\n",
                "            d['tier3:failed_header_test_count']\n",
                "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
                "        if f in d:\n",
                "            for t in d[f]:\n",
                "\n",
                "                ipt = t[1]\n",
                "                ipl = t[2]\n",
                "                asnt = asndb.lookup(ipt)[0]\n",
                "                asnl = asndb.lookup(ipl)[0]\n",
                "                if asnl != None and asnt != None:\n",
                "                    if asnt == asnl:\n",
                "                        same_asn += 1\n",
                "                    else:\n",
                "                        diff_asn += 1\n",
                "\n",
                "                nt = ip_network(ipt+\"/24\", strict=False).network_address\n",
                "                nl = ip_network(ipl+\"/24\", strict=False).network_address\n",
                "                if nt == nl:\n",
                "                    same_24 += 1\n",
                "                else:\n",
                "                    diff_24 += 1\n",
                "\n",
                "                if is_same_bgp_prefix(ipt, ipl, bgp_prefixes):\n",
                "                    same_bgp += 1\n",
                "                else:\n",
                "                    diff_bgp += 1\n",
                "\n",
                "    distances = []\n",
                "    for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
                "        target_geo = (d['RIPE:lat'], d['RIPE:lon'])\n",
                "        if f in d:\n",
                "            for l in d[f]:\n",
                "                landmark_geo = (l[2], l[3])\n",
                "                distances.append(haversine(target_geo, landmark_geo))\n",
                "    distances_to_landmarks.append(distances)\n",
                "\n",
                "    if same_asn != 0 or diff_asn != 0:\n",
                "        same_asn_lst.append(same_asn/(same_asn+diff_asn))\n",
                "\n",
                "    if same_24 != 0 or diff_24 != 0:\n",
                "        same_24_lst.append(same_24/(same_24+diff_24))\n",
                "        if same_24 != 0:\n",
                "            print(\n",
                "                f\"Found {d['target_ip']} with a landmark in the same /24\")\n",
                "    if same_bgp != 0 or diff_bgp != 0:\n",
                "        same_bgp_lst.append(same_bgp/(diff_bgp+same_bgp))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "713 target have potentail landmarks or 0.9861687413554634\n",
                        "677 target have valid landmarks or 0.9363762102351314\n",
                        "207 target with a landmark within 1 km or 0.2863070539419087\n",
                        "419 target with a landmark within 5 km or 0.5795297372060858\n",
                        "464 target with a landmark within 10 km or 0.6417704011065007\n",
                        "552 target with a landmark within 40 km or 0.7634854771784232\n"
                    ]
                }
            ],
            "source": [
                "landmarks_all = []\n",
                "landmarks_less_1 = []\n",
                "landmarks_less_5 = []\n",
                "landmarks_less_10 = []\n",
                "landmarks_less_40 = []\n",
                "\n",
                "for landmark_distances in distances_to_landmarks:\n",
                "    landmarks_all.append(len(landmark_distances))\n",
                "    landmarks_less_1.append(len([i for i in landmark_distances if i <= 1]))\n",
                "    landmarks_less_5.append(len([i for i in landmark_distances if i <= 5]))\n",
                "    landmarks_less_10.append(\n",
                "        len([i for i in landmark_distances if i <= 10]))\n",
                "    landmarks_less_40.append(\n",
                "        len([i for i in landmark_distances if i <= 40]))\n",
                "\n",
                "lm_a_0 = len([i for i in all_landmarks if i > 0])\n",
                "lmv_a_0 = len([i for i in landmarks_all if i > 0])\n",
                "lm1_0 = len([i for i in landmarks_less_1 if i > 0])\n",
                "lm5_0 = len([i for i in landmarks_less_5 if i > 0])\n",
                "lm10_0 = len([i for i in landmarks_less_10 if i > 0])\n",
                "lm40_0 = len([i for i in landmarks_less_40 if i > 0])\n",
                "\n",
                "\n",
                "len_all = len(data)\n",
                "print(f\"{lm_a_0} target have potentail landmarks or {lm_a_0/len_all}\")\n",
                "print(f\"{lmv_a_0} target have valid landmarks or {lmv_a_0/len_all}\")\n",
                "print(f\"{lm1_0} target with a landmark within 1 km or {lm1_0/len_all}\")\n",
                "print(f\"{lm5_0} target with a landmark within 5 km or {lm5_0/len_all}\")\n",
                "print(f\"{lm10_0} target with a landmark within 10 km or {lm10_0/len_all}\")\n",
                "print(f\"{lm40_0} target with a landmark within 40 km or {lm40_0/len_all}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-09-14 13:19:51::INFO:root:analysis:: Tier1 Failed\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "207 targets with landmarks (ping <= 1) or 0.2863070539419087\n",
                        "419 targets with landmarks (ping <= 5) or 0.5795297372060858\n",
                        "464 targets with landmarks (ping <= 10) or 0.6417704011065007\n",
                        "552 targets with landmarks (ping <= 40) or 0.7634854771784232\n",
                        "723 targets with landmarks (ping <= 9999999999) or 1.0\n"
                    ]
                }
            ],
            "source": [
                "clickhouse_driver = Clickhouse()\n",
                "query = clickhouse_driver.get_min_rtt_per_src_dst_prefix_query(TARGET_TO_LANDMARKS_PING_TABLE, filter=\"\", threshold=1000000)\n",
                "db_table = clickhouse_driver.execute(query)\n",
                "\n",
                "rtts = []\n",
                "remove_dict = {}\n",
                "for l in db_table:\n",
                "    rtts.append(l[2])\n",
                "    remove_dict[(l[0], l[1])] = l[2]\n",
                "\n",
                "error1 = []\n",
                "error2 = []\n",
                "error3 = []\n",
                "error4 = []\n",
                "error1ms = []\n",
                "error2ms = []\n",
                "error5ms = []\n",
                "error10ms = []\n",
                "\n",
                "for _, d in data.items():\n",
                "    errors = every_tier_result_and_errors(d)\n",
                "    error1.append(errors['error1'])\n",
                "    error2.append(errors['error2'])\n",
                "    error3.append(errors['error3'])\n",
                "    error4.append(errors['error4'])\n",
                "    err1ms = 50000\n",
                "    err2ms = 50000\n",
                "    err5ms = 50000\n",
                "    err10ms = 50000\n",
                "    for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
                "        if f in d:\n",
                "            for l_ip, _, l_lat, l_lon in d[f]:\n",
                "                dist = haversine((l_lat, l_lon), (d['RIPE:lat'], d['RIPE:lon']))\n",
                "                key_rtt = (l_ip, d['target_ip'])\n",
                "                if dist < err1ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 1):\n",
                "                    err1ms = dist\n",
                "                if dist < err2ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 2):\n",
                "                    err2ms = dist\n",
                "                if dist < err5ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 5):\n",
                "                    err5ms = dist\n",
                "                if dist < err10ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 10):\n",
                "                    err10ms = dist\n",
                "    if err1ms != 50000:\n",
                "        error1ms.append(err1ms)\n",
                "    else:\n",
                "        error1ms.append(error1[-1])\n",
                "    if err2ms != 50000:\n",
                "        error2ms.append(err2ms)\n",
                "    else:\n",
                "        error2ms.append(error1[-1])\n",
                "    if err5ms != 50000:\n",
                "        error5ms.append(err5ms)\n",
                "    else:\n",
                "        error5ms.append(error1[-1])\n",
                "    if err10ms != 50000:\n",
                "        error10ms.append(err10ms)\n",
                "    else:\n",
                "        error10ms.append(error1[-1])\n",
                "\n",
                "for i in [1, 5, 10, 40, 9999999999]:\n",
                "    c = len([j for j in error1ms if j <= i])\n",
                "    print(f\"{c} targets with landmarks (ping <= {i}) or {c/len(error1ms)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "review-8XQ99qZ1-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
