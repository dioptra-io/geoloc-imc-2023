{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyasn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from ipaddress import ip_network\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "from clickhouse_driver import Client\n",
    "\n",
    "from scripts.utils.file_utils import load_json, dump_json\n",
    "from scripts.analysis.analysis import get_all_bgp_prefixes, is_same_bgp_prefix, every_tier_result_and_errors\n",
    "from scripts.utils.helpers import haversine, rtt_to_km, is_within_cirle, polygon_centroid, circle_intersections, distance, get_points_in_poly\n",
    "from default import IP_TO_ASN_FILE, ANALYZABLE_FILE, ROUND_BASED_ALGORITHM_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(ANALYZABLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['92.38.184.82']\n",
      "1 no intersection out or 723 = 0.0013831258644536654\n",
      "If the speed where to be 2/3 CBG would have worked for 0 more targets\n",
      "0 no vp, 1 some vps\n",
      "When no vp found 0 target had a traceroute dedecated to it and 0 did not\n",
      "the target was in the CBG area 677 times\n",
      "the target was out of the CBG area 45 times\n",
      "CBG failed 6.232686980609419%\n",
      "If we would use 2/3 45 extra targets would be in the CBG area\n"
     ]
    }
   ],
   "source": [
    "bad = 0\n",
    "good = 0\n",
    "good_23_only = 0\n",
    "empty_vps = 0\n",
    "not_empty_vps = 0\n",
    "targeted_traceroutes = 0\n",
    "not_targeted_traceroutes = 0\n",
    "vps_not_working = []\n",
    "for _, d in data.items():\n",
    "    if d['tier1:done']:\n",
    "        good += 1\n",
    "    else:\n",
    "        bad += 1\n",
    "        points = get_points_in_poly(d['vps'], 36, 5, 4/9)\n",
    "        if len(points) != 0:\n",
    "            print(len(points))\n",
    "        tmp_vps = []\n",
    "        for vp in d['vps']:\n",
    "            tmp_vps.append((vp[0], vp[1], vp[2], None, None))\n",
    "        points = get_points_in_poly(tmp_vps, 36, 5, 2/3)\n",
    "        if len(points) != 0:\n",
    "            good_23_only += 1\n",
    "        else:\n",
    "            if len(d['vps']) > 0:\n",
    "                not_empty_vps += 1\n",
    "                vps_not_working.append(d['target_ip'])\n",
    "            else:\n",
    "                empty_vps += 1\n",
    "                client = Client('127.0.0.1')\n",
    "                tmp_row = client.execute(\n",
    "                    f'select src_addr, rtt, tstamp from bgp_interdomain_te.street_lvl_traceroutes where dst_addr = \\'{d[\"target_ip\"]}\\'')\n",
    "                if len(tmp_row) != 0:\n",
    "                    targeted_traceroutes += 1\n",
    "                    print(f\"{d['target_ip']} was targeted by traceroutes\")\n",
    "                else:\n",
    "                    not_targeted_traceroutes += 1\n",
    "\n",
    "print(vps_not_working)\n",
    "print(f\"{bad} no intersection out or {bad+good} = {bad/(bad+good)}\")\n",
    "print(\n",
    "    f\"If the speed where to be 2/3 CBG would have worked for {good_23_only} more targets\")\n",
    "print(f\"{empty_vps} no vp, {not_empty_vps} some vps\")\n",
    "print(\n",
    "    f\"When no vp found {targeted_traceroutes} target had a traceroute dedecated to it and {not_targeted_traceroutes} did not\")\n",
    "\n",
    "position_in = 0\n",
    "position_out = 0\n",
    "would_be_in = 0\n",
    "for _, d in data.items():\n",
    "    if not d['tier1:done']:\n",
    "        continue\n",
    "    all_in = True\n",
    "    candidate_geo = (d['lat_c'], d['lon_c'])\n",
    "    for vp in d['vps']:\n",
    "        if not is_within_cirle((vp[0], vp[1]), vp[2], candidate_geo, speed_threshold=4/9):\n",
    "            all_in = False\n",
    "    if all_in:\n",
    "        position_in += 1\n",
    "    else:\n",
    "        position_out += 1\n",
    "        all_in = True\n",
    "        for vp in d['vps']:\n",
    "            if not is_within_cirle((vp[0], vp[1]), vp[2], candidate_geo, speed_threshold=2/3):\n",
    "                all_in = False\n",
    "        if all_in:\n",
    "            would_be_in += 1\n",
    "        else:\n",
    "            print(f\"{d['target_ip']} is always outside the CBG area\")\n",
    "\n",
    "print(f\"the target was in the CBG area {position_in} times\")\n",
    "print(f\"the target was out of the CBG area {position_out} times\")\n",
    "print(f\"CBG failed {position_out*100/(position_in+position_out)}%\")\n",
    "print(\n",
    "    f\"If we would use 2/3 {would_be_in} extra targets would be in the CBG area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723 Total targets done\n",
      "722 tier1:done\n",
      "660 tier2:done\n",
      "425 tier3:done\n",
      "tier2_failed_because_no_zipcodes 0\n",
      "tier2_failed_because_no_landmark 45\n",
      "tier2_failed_because_no_valid_traceroute 17\n",
      "tier2_failed_because_other 0\n",
      "tier3_failed_because_no_zipcodes 161\n",
      "tier3_failed_because_no_landmark 74\n",
      "tier3_failed_because_no_valid_traceroute 0\n",
      "tier3_failed_because_other 0\n"
     ]
    }
   ],
   "source": [
    "feilds_count = {'tier1:done': 0, 'tier2:done': 0, 'tier3:done': 0}\n",
    "for _, d in data.items():\n",
    "    for feild in feilds_count:\n",
    "        if d[feild]:\n",
    "            feilds_count[feild] += 1\n",
    "print(f\"{len(data)} Total targets done\")\n",
    "for k, v in feilds_count.items():\n",
    "    print(f\"{v} {k}\")\n",
    "\n",
    "dict_reasons = {\n",
    "    'tier2_failed_because_no_zipcodes': 0,\n",
    "    'tier2_failed_because_no_landmark': 0,\n",
    "    'tier2_failed_because_no_valid_traceroute': 0,\n",
    "    'tier2_failed_because_other': 0,\n",
    "    'tier3_failed_because_no_zipcodes': 0,\n",
    "    'tier3_failed_because_no_landmark': 0,\n",
    "    'tier3_failed_because_no_valid_traceroute': 0,\n",
    "    'tier3_failed_because_other': 0\n",
    "}\n",
    "\n",
    "for _, d in data.items():\n",
    "    if not d['tier1:done']:  # Here you should analyse tier1\n",
    "        continue\n",
    "    if not d['tier2:done']:\n",
    "        if d['tier2:inspected_points_count'] == 0:\n",
    "            dict_reasons['tier2_failed_because_no_zipcodes'] += 1\n",
    "            continue\n",
    "        if d['tier2:landmark_count'] == 0:\n",
    "            dict_reasons['tier2_failed_because_no_landmark'] += 1\n",
    "            continue\n",
    "        one_traceroute_found = False\n",
    "        for t in d['tier2:traceroutes']:\n",
    "            if t[4] > 0:\n",
    "                one_traceroute_found = True\n",
    "                break\n",
    "        if not one_traceroute_found:\n",
    "            dict_reasons['tier2_failed_because_no_valid_traceroute'] += 1\n",
    "            continue\n",
    "        dict_reasons['tier2_failed_because_other'] += 1\n",
    "        continue\n",
    "    if not d['tier3:done']:\n",
    "        if d['tier3:inspected_points_count'] == 0:\n",
    "            dict_reasons['tier3_failed_because_no_zipcodes'] += 1\n",
    "            # if d['target_ip'] not in ['185.28.221.65', '46.183.219.225']:\n",
    "            #    print(d['target_ip'])\n",
    "            #    exit()\n",
    "            continue\n",
    "        if d['tier3:landmark_count'] == 0:\n",
    "            dict_reasons['tier3_failed_because_no_landmark'] += 1\n",
    "            continue\n",
    "        one_traceroute_found = False\n",
    "        for t in d['tier3:traceroutes']:\n",
    "            if t[4] > 0:\n",
    "                one_traceroute_found = True\n",
    "                break\n",
    "        if not one_traceroute_found:\n",
    "            dict_reasons['tier3_failed_because_no_valid_traceroute'] += 1\n",
    "            continue\n",
    "        dict_reasons['tier3_failed_because_other'] += 1\n",
    "        continue\n",
    "\n",
    "for k, v in dict_reasons.items():\n",
    "    print(f\"{k} {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API calls count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920.0 Zipcode to check (median)\n",
      "111.0 traceroutes to check (median)\n",
      "2419.0 landmarks to check (median)\n",
      "753428 Overpass queries\n",
      "2755315 landmarks verification\n",
      "143601 traceroutes\n"
     ]
    }
   ],
   "source": [
    "zipcodes_counts = []\n",
    "landmarks_counts = []\n",
    "traceroutes_counts = []\n",
    "for _, d in data.items():\n",
    "    zipcodes_count = 0\n",
    "    landmarks_count = 0\n",
    "    traceroutes_count = 0\n",
    "\n",
    "    for f in ['tier2:inspected_points_count', 'tier3:inspected_points_count']:\n",
    "        if f in d:\n",
    "            zipcodes_count += d[f]\n",
    "    if zipcodes_count != 0:\n",
    "        zipcodes_counts.append(zipcodes_count)\n",
    "\n",
    "    for f in [\"tier2:failed_dns_count\", \"tier2:failed_asn_count\", \"tier2:cdn_count\", \"tier2:non_cdn_count\", \"tier3:failed_dns_count\", \"tier3:failed_asn_count\", \"tier3:cdn_count\", \"tier3:non_cdn_count\"]:\n",
    "        if f in d:\n",
    "            landmarks_count += d[f]\n",
    "    if landmarks_count != 0:\n",
    "        landmarks_counts.append(landmarks_count)\n",
    "\n",
    "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "        if f in d:\n",
    "            traceroutes_count += len(d[f])\n",
    "    if traceroutes_count != 0:\n",
    "        traceroutes_counts.append(traceroutes_count)\n",
    "\n",
    "print(f\"{np.median(zipcodes_counts)} Zipcode to check (median)\")\n",
    "print(f\"{np.median(traceroutes_counts)} traceroutes to check (median)\")\n",
    "print(f\"{np.median(landmarks_counts)} landmarks to check (median)\")\n",
    "\n",
    "total = 0\n",
    "for zip in zipcodes_counts:\n",
    "    total += zip\n",
    "print(f\"{total} Overpass queries\")\n",
    "total = 0\n",
    "for x in landmarks_counts:\n",
    "    total += x\n",
    "print(f\"{total} landmarks verification\")\n",
    "total = 0\n",
    "for x in traceroutes_counts:\n",
    "    total += x\n",
    "print(f\"{total} traceroutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation same network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IP_TO_ASN_FILE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m asn_coef_lst \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m bgp_coef_lst \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m asndb \u001b[39m=\u001b[39m pyasn\u001b[39m.\u001b[39mpyasn(IP_TO_ASN_FILE_PATH)\n\u001b[0;32m      4\u001b[0m bgp_prefixes \u001b[39m=\u001b[39m get_all_bgp_prefixes()\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _, d \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IP_TO_ASN_FILE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "asn_coef_lst = []\n",
    "bgp_coef_lst = []\n",
    "asndb = pyasn.pyasn(IP_TO_ASN_FILE)\n",
    "bgp_prefixes = get_all_bgp_prefixes()\n",
    "\n",
    "for _, d in data.items():\n",
    "    same_bgp_x = []\n",
    "    same_bgp_y = []\n",
    "    same_asn_x = []\n",
    "    same_asn_y = []\n",
    "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "        if f in d:\n",
    "            for t in d[f]:\n",
    "                if t[4] < 0:\n",
    "                    continue\n",
    "                distance = haversine(\n",
    "                    (t[5], t[6]), (d['lat_c'], d['lon_c']))\n",
    "                ipt = t[1]\n",
    "                ipl = t[2]\n",
    "                asnt = asndb.lookup(ipt)[0]\n",
    "                asnl = asndb.lookup(ipl)[0]\n",
    "                if asnl != None and asnt != None:\n",
    "                    if asnt == asnl and distance not in same_asn_y:\n",
    "                        same_asn_y.append(distance)\n",
    "                        same_asn_x.append(t[4])\n",
    "\n",
    "                if is_same_bgp_prefix(ipt, ipl, bgp_prefixes):\n",
    "                    if distance not in same_bgp_y:\n",
    "                        same_bgp_y.append(distance)\n",
    "                        same_bgp_x.append(t[4])\n",
    "    if len(same_asn_x) > 1:\n",
    "        correlation = pearsonr(same_asn_x, same_asn_y)[0]\n",
    "        asn_coef_lst.append(correlation)\n",
    "    if len(same_bgp_x) > 1:\n",
    "        correlation = pearsonr(same_bgp_x, same_bgp_y)[0]\n",
    "        bgp_coef_lst.append(correlation)\n",
    "\n",
    "print(f\"{len(asn_coef_lst)} targets with correlation asn\")\n",
    "print(f\"{len(bgp_coef_lst)} targets with correlation bgp\")\n",
    "print(f\"{np.median(bgp_coef_lst)} median bgp correlation\")\n",
    "print(f\"{np.median(asn_coef_lst)} median asn correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round_based_algorithm_results = load_json(ROUND_BASED_ALGORITHM_FILE)\n",
    "round_based_algorithm_results = load_json(\"test.json\")\n",
    "round_based_algorithm_results = {\n",
    "int(x): round_based_algorithm_results[x] for x in round_based_algorithm_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5485404\n",
      "100 4280667\n",
      "300 3029640\n",
      "500 2640603\n",
      "1000 2639484\n"
     ]
    }
   ],
   "source": [
    "for tier1_vps, results in sorted(round_based_algorithm_results.items()):\n",
    "        tier1_vps = int(tier1_vps)\n",
    "        n_vps_cdf = [r[2] + tier1_vps for r in results if r[2] is not None]\n",
    "        print(tier1_vps, 3 * sum(n_vps_cdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of landmarks within a certain radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78.128.211.119 with a landmark in the same /24\n",
      "Found 77.109.180.62 with a landmark in the same /24\n",
      "Found 103.143.136.43 with a landmark in the same /24\n"
     ]
    }
   ],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "valid_landmarks_count = 0\n",
    "unvalid_landmarks_count = 0\n",
    "same_asn_lst = []\n",
    "same_24_lst = []\n",
    "same_bgp_lst = []\n",
    "distances_to_landmarks = []\n",
    "all_landmarks = []\n",
    "asndb = pyasn.pyasn(IP_TO_ASN_FILE)\n",
    "bgp_prefixes = get_all_bgp_prefixes()\n",
    "\n",
    "for _, d in data.items():\n",
    "    same_asn = 0\n",
    "    diff_asn = 0\n",
    "    same_bgp = 0\n",
    "    diff_bgp = 0\n",
    "    same_24 = 0\n",
    "    diff_24 = 0\n",
    "    all_landmarks.append(0)\n",
    "    if \"tier2:cdn_count\" in d and \"tier2:landmark_count\" in d and \"tier2:failed_header_test_count\" in d:\n",
    "        all_landmarks[-1] += d['tier2:landmark_count'] + \\\n",
    "            d['tier2:cdn_count'] + d['tier2:failed_header_test_count']\n",
    "        valid_landmarks_count += d['tier2:landmark_count']\n",
    "        unvalid_landmarks_count += d['tier2:cdn_count'] + \\\n",
    "            d['tier2:failed_header_test_count']\n",
    "    if \"tier3:cdn_count\" in d and \"tier3:landmark_count\" in d and \"tier3:failed_header_test_count\" in d:\n",
    "        all_landmarks[-1] += d['tier3:landmark_count'] + \\\n",
    "            d['tier3:cdn_count'] + d['tier3:failed_header_test_count']\n",
    "        valid_landmarks_count += d['tier3:landmark_count']\n",
    "        unvalid_landmarks_count += d['tier3:cdn_count'] + \\\n",
    "            d['tier3:failed_header_test_count']\n",
    "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "        if f in d:\n",
    "            for t in d[f]:\n",
    "\n",
    "                ipt = t[1]\n",
    "                ipl = t[2]\n",
    "                asnt = asndb.lookup(ipt)[0]\n",
    "                asnl = asndb.lookup(ipl)[0]\n",
    "                if asnl != None and asnt != None:\n",
    "                    if asnt == asnl:\n",
    "                        same_asn += 1\n",
    "                    else:\n",
    "                        diff_asn += 1\n",
    "\n",
    "                nt = ip_network(ipt+\"/24\", strict=False).network_address\n",
    "                nl = ip_network(ipl+\"/24\", strict=False).network_address\n",
    "                if nt == nl:\n",
    "                    same_24 += 1\n",
    "                else:\n",
    "                    diff_24 += 1\n",
    "\n",
    "                if is_same_bgp_prefix(ipt, ipl, bgp_prefixes):\n",
    "                    same_bgp += 1\n",
    "                else:\n",
    "                    diff_bgp += 1\n",
    "\n",
    "    distances = []\n",
    "    for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
    "        target_geo = (d['lat_c'], d['lon_c'])\n",
    "        if f in d:\n",
    "            for l in d[f]:\n",
    "                landmark_geo = (l[2], l[3])\n",
    "                distances.append(haversine(target_geo, landmark_geo))\n",
    "    distances_to_landmarks.append(distances)\n",
    "\n",
    "    if same_asn != 0 or diff_asn != 0:\n",
    "        same_asn_lst.append(same_asn/(same_asn+diff_asn))\n",
    "\n",
    "    if same_24 != 0 or diff_24 != 0:\n",
    "        same_24_lst.append(same_24/(same_24+diff_24))\n",
    "        if same_24 != 0:\n",
    "            print(\n",
    "                f\"Found {d['target_ip']} with a landmark in the same /24\")\n",
    "    if same_bgp != 0 or diff_bgp != 0:\n",
    "        same_bgp_lst.append(same_bgp/(diff_bgp+same_bgp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65325 total valid landmarks\n",
      "2519202 unvalid landmarks\n",
      "2.5275417900451416% valid landmarks\n",
      "537 targets has all its landmarks outside its AS out of 669 80.26905829596413%\n",
      "674 targets has all its landmarks outside its /24 out of 677 99.55686853766618%\n",
      "618 targets has all its landmarks outside its BGP prefix out of 677 91.2850812407681%\n"
     ]
    }
   ],
   "source": [
    "only_outside_asn = 0\n",
    "for x in same_asn_lst:\n",
    "    if x == 0:\n",
    "        only_outside_asn += 1\n",
    "only_outside_24 = 0\n",
    "for x in same_24_lst:\n",
    "    if x == 0:\n",
    "        only_outside_24 += 1\n",
    "only_outside_bgp = 0\n",
    "for x in same_bgp_lst:\n",
    "    if x == 0:\n",
    "        only_outside_bgp += 1\n",
    "\n",
    "print(f\"{valid_landmarks_count} total valid landmarks\")\n",
    "print(f\"{unvalid_landmarks_count} unvalid landmarks\")\n",
    "print(f\"{(valid_landmarks_count*100)/(valid_landmarks_count+unvalid_landmarks_count)}% valid landmarks\")\n",
    "\n",
    "print(f\"{only_outside_asn} targets has all its landmarks outside its AS out of {len(same_asn_lst)} {only_outside_asn*100/(len(same_asn_lst))}%\")\n",
    "print(f\"{only_outside_24} targets has all its landmarks outside its /24 out of {len(same_24_lst)} {only_outside_24*100/(len(same_24_lst))}%\")\n",
    "print(f\"{only_outside_bgp} targets has all its landmarks outside its BGP prefix out of {len(same_bgp_lst)} {only_outside_bgp*100/(len(same_bgp_lst))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39853 ping measurement to do\n",
      "207\n",
      "713 target have potentail landmarks or 0.9861687413554634\n",
      "677 target have valid landmarks or 0.9363762102351314\n",
      "207 target with a landmark within 1 km or 0.2863070539419087\n",
      "419 target with a landmark within 5 km or 0.5795297372060858\n",
      "464 target with a landmark within 10 km or 0.6417704011065007\n",
      "552 target with a landmark within 40 km or 0.7634854771784232\n"
     ]
    }
   ],
   "source": [
    "landmarks_all = []\n",
    "landmarks_less_1 = []\n",
    "landmarks_less_5 = []\n",
    "landmarks_less_10 = []\n",
    "landmarks_less_40 = []\n",
    "total_count_ping = 0\n",
    "\n",
    "for landmark_distances in distances_to_landmarks:\n",
    "    # if len(landmark_distances) == 0:\n",
    "    #     continue\n",
    "    landmarks_all.append(len(landmark_distances))\n",
    "    landmarks_less_1.append(len([i for i in landmark_distances if i <= 1]))\n",
    "    landmarks_less_5.append(len([i for i in landmark_distances if i <= 5]))\n",
    "    landmarks_less_10.append(\n",
    "        len([i for i in landmark_distances if i <= 10]))\n",
    "    landmarks_less_40.append(\n",
    "        len([i for i in landmark_distances if i <= 40]))\n",
    "    total_count_ping += len([i for i in landmark_distances if i <= 40])\n",
    "\n",
    "print(f\"{total_count_ping} ping measurement to do\")\n",
    "\n",
    "lm_a_0 = len([i for i in all_landmarks if i > 0])\n",
    "lmv_a_0 = len([i for i in landmarks_all if i > 0])\n",
    "lm1_0 = len([i for i in landmarks_less_1 if i > 0])\n",
    "lm5_0 = len([i for i in landmarks_less_5 if i > 0])\n",
    "lm10_0 = len([i for i in landmarks_less_10 if i > 0])\n",
    "lm40_0 = len([i for i in landmarks_less_40 if i > 0])\n",
    "\n",
    "lm1_1 = len([i for i in landmarks_less_1 if i >= 1])\n",
    "print(lm1_1)\n",
    "\n",
    "len_all = len(data)\n",
    "print(f\"{lm_a_0} target have potentail landmarks or {lm_a_0/len_all}\")\n",
    "print(f\"{lmv_a_0} target have valid landmarks or {lmv_a_0/len_all}\")\n",
    "print(f\"{lm1_0} target with a landmark within 1 km or {lm1_0/len_all}\")\n",
    "print(f\"{lm5_0} target with a landmark within 5 km or {lm5_0/len_all}\")\n",
    "print(f\"{lm10_0} target with a landmark within 10 km or {lm10_0/len_all}\")\n",
    "print(f\"{lm40_0} target with a landmark within 40 km or {lm40_0/len_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27026\n",
      "27026\n",
      "Tier1 Failed\n",
      "119 targets with landmarks (ping <= 1) or 0.16459197786998617\n",
      "356 targets with landmarks (ping <= 5) or 0.49239280774550487\n",
      "423 targets with landmarks (ping <= 10) or 0.5850622406639004\n",
      "517 targets with landmarks (ping <= 40) or 0.715076071922545\n",
      "723 targets with landmarks (ping <= 9999999999) or 1.0\n"
     ]
    }
   ],
   "source": [
    "query = get_min_rtt_per_src_dst_query_ping_table(\n",
    "    'geolocation_replication', 'targets_to_landmarks_pings', '', 1000000)\n",
    "client = Client('127.0.0.1')\n",
    "db_table = client.execute(query)\n",
    "\n",
    "rtts = []\n",
    "remove_dict = {}\n",
    "print(len(db_table))\n",
    "for l in db_table:\n",
    "    rtts.append(l[2])\n",
    "    remove_dict[(l[0], l[1])] = l[2]\n",
    "print(len(rtts))\n",
    "\n",
    "error1 = []\n",
    "error2 = []\n",
    "error3 = []\n",
    "error4 = []\n",
    "error1ms = []\n",
    "error2ms = []\n",
    "error5ms = []\n",
    "error10ms = []\n",
    "\n",
    "for _, d in data.items():\n",
    "    errors = every_tier_result_and_errors(d)\n",
    "    error1.append(errors['error1'])\n",
    "    error2.append(errors['error2'])\n",
    "    error3.append(errors['error3'])\n",
    "    error4.append(errors['error4'])\n",
    "    err1ms = 50000\n",
    "    err2ms = 50000\n",
    "    err5ms = 50000\n",
    "    err10ms = 50000\n",
    "    for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
    "        if f in d:\n",
    "            for l_ip, _, l_lat, l_lon in d[f]:\n",
    "                dist = haversine((l_lat, l_lon), (d['lat_c'], d['lon_c']))\n",
    "                key_rtt = (l_ip, d['target_ip'])\n",
    "                if dist < err1ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 1):\n",
    "                    err1ms = dist\n",
    "                if dist < err2ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 2):\n",
    "                    err2ms = dist\n",
    "                if dist < err5ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 5):\n",
    "                    err5ms = dist\n",
    "                if dist < err10ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 10):\n",
    "                    err10ms = dist\n",
    "    if err1ms != 50000:\n",
    "        error1ms.append(err1ms)\n",
    "    else:\n",
    "        error1ms.append(error1[-1])\n",
    "    if err2ms != 50000:\n",
    "        error2ms.append(err2ms)\n",
    "    else:\n",
    "        error2ms.append(error1[-1])\n",
    "    if err5ms != 50000:\n",
    "        error5ms.append(err5ms)\n",
    "    else:\n",
    "        error5ms.append(error1[-1])\n",
    "    if err10ms != 50000:\n",
    "        error10ms.append(err10ms)\n",
    "    else:\n",
    "        error10ms.append(error1[-1])\n",
    "\n",
    "for i in [1, 5, 10, 40, 9999999999]:\n",
    "    c = len([j for j in error1ms if j <= i])\n",
    "    print(f\"{c} targets with landmarks (ping <= {i}) or {c/len(error1ms)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review-8XQ99qZ1-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
