{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print tables\n",
    "\n",
    "Print all the tables of the replication paper  \n",
    "To do after analysis/million_scale.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyasn\n",
    "\n",
    "from ipaddress import ip_network\n",
    "from clickhouse_driver import Client\n",
    "\n",
    "from scripts.utils.file_utils import load_json\n",
    "from scripts.utils.clickhouse_utils import get_min_rtt_per_src_dst_query_ping_table\n",
    "from scripts.analysis.analysis import get_all_bgp_prefixes, is_same_bgp_prefix, every_tier_result_and_errors\n",
    "from scripts.utils.helpers import haversine\n",
    "from default import IP_TO_ASN_FILE, ANALYZABLE_FILE, ROUND_BASED_ALGORITHM_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3.c of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_based_algorithm_results = load_json(ROUND_BASED_ALGORITHM_FILE)\n",
    "\n",
    "round_based_algorithm_results = {\n",
    "int(x): round_based_algorithm_results[x] for x in round_based_algorithm_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tier1_vps, results in sorted(round_based_algorithm_results.items()):\n",
    "        tier1_vps = int(tier1_vps)\n",
    "        n_vps_cdf = [r[2] + tier1_vps for r in results if r[2] is not None]\n",
    "        print(tier1_vps, 3 * sum(n_vps_cdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of landmarks within a certain radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5.b of the replication paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lat_c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m distances \u001b[39m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtier2:landmarks\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtier3:landmarks\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> 61\u001b[0m     target_geo \u001b[39m=\u001b[39m (d[\u001b[39m'\u001b[39;49m\u001b[39mlat_c\u001b[39;49m\u001b[39m'\u001b[39;49m], d[\u001b[39m'\u001b[39m\u001b[39mRIPE:lon\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39min\u001b[39;00m d:\n\u001b[1;32m     63\u001b[0m         \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m d[f]:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lat_c'"
     ]
    }
   ],
   "source": [
    "data = load_json(ANALYZABLE_FILE)\n",
    "\n",
    "valid_landmarks_count = 0\n",
    "unvalid_landmarks_count = 0\n",
    "same_asn_lst = []\n",
    "same_24_lst = []\n",
    "same_bgp_lst = []\n",
    "distances_to_landmarks = []\n",
    "all_landmarks = []\n",
    "asndb = pyasn.pyasn(str(IP_TO_ASN_FILE))\n",
    "bgp_prefixes = get_all_bgp_prefixes()\n",
    "\n",
    "for _, d in data.items():\n",
    "    same_asn = 0\n",
    "    diff_asn = 0\n",
    "    same_bgp = 0\n",
    "    diff_bgp = 0\n",
    "    same_24 = 0\n",
    "    diff_24 = 0\n",
    "    all_landmarks.append(0)\n",
    "    if \"tier2:cdn_count\" in d and \"tier2:landmark_count\" in d and \"tier2:failed_header_test_count\" in d:\n",
    "        all_landmarks[-1] += d['tier2:landmark_count'] + \\\n",
    "            d['tier2:cdn_count'] + d['tier2:failed_header_test_count']\n",
    "        valid_landmarks_count += d['tier2:landmark_count']\n",
    "        unvalid_landmarks_count += d['tier2:cdn_count'] + \\\n",
    "            d['tier2:failed_header_test_count']\n",
    "    if \"tier3:cdn_count\" in d and \"tier3:landmark_count\" in d and \"tier3:failed_header_test_count\" in d:\n",
    "        all_landmarks[-1] += d['tier3:landmark_count'] + \\\n",
    "            d['tier3:cdn_count'] + d['tier3:failed_header_test_count']\n",
    "        valid_landmarks_count += d['tier3:landmark_count']\n",
    "        unvalid_landmarks_count += d['tier3:cdn_count'] + \\\n",
    "            d['tier3:failed_header_test_count']\n",
    "    for f in ['tier2:traceroutes', 'tier3:traceroutes']:\n",
    "        if f in d:\n",
    "            for t in d[f]:\n",
    "\n",
    "                ipt = t[1]\n",
    "                ipl = t[2]\n",
    "                asnt = asndb.lookup(ipt)[0]\n",
    "                asnl = asndb.lookup(ipl)[0]\n",
    "                if asnl != None and asnt != None:\n",
    "                    if asnt == asnl:\n",
    "                        same_asn += 1\n",
    "                    else:\n",
    "                        diff_asn += 1\n",
    "\n",
    "                nt = ip_network(ipt+\"/24\", strict=False).network_address\n",
    "                nl = ip_network(ipl+\"/24\", strict=False).network_address\n",
    "                if nt == nl:\n",
    "                    same_24 += 1\n",
    "                else:\n",
    "                    diff_24 += 1\n",
    "\n",
    "                if is_same_bgp_prefix(ipt, ipl, bgp_prefixes):\n",
    "                    same_bgp += 1\n",
    "                else:\n",
    "                    diff_bgp += 1\n",
    "\n",
    "    distances = []\n",
    "    for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
    "        target_geo = (d['RIPE:lat'], d['RIPE:lon'])\n",
    "        if f in d:\n",
    "            for l in d[f]:\n",
    "                landmark_geo = (l[2], l[3])\n",
    "                distances.append(haversine(target_geo, landmark_geo))\n",
    "    distances_to_landmarks.append(distances)\n",
    "\n",
    "    if same_asn != 0 or diff_asn != 0:\n",
    "        same_asn_lst.append(same_asn/(same_asn+diff_asn))\n",
    "\n",
    "    if same_24 != 0 or diff_24 != 0:\n",
    "        same_24_lst.append(same_24/(same_24+diff_24))\n",
    "        if same_24 != 0:\n",
    "            print(\n",
    "                f\"Found {d['target_ip']} with a landmark in the same /24\")\n",
    "    if same_bgp != 0 or diff_bgp != 0:\n",
    "        same_bgp_lst.append(same_bgp/(diff_bgp+same_bgp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_all = []\n",
    "landmarks_less_1 = []\n",
    "landmarks_less_5 = []\n",
    "landmarks_less_10 = []\n",
    "landmarks_less_40 = []\n",
    "\n",
    "for landmark_distances in distances_to_landmarks:\n",
    "    landmarks_all.append(len(landmark_distances))\n",
    "    landmarks_less_1.append(len([i for i in landmark_distances if i <= 1]))\n",
    "    landmarks_less_5.append(len([i for i in landmark_distances if i <= 5]))\n",
    "    landmarks_less_10.append(\n",
    "        len([i for i in landmark_distances if i <= 10]))\n",
    "    landmarks_less_40.append(\n",
    "        len([i for i in landmark_distances if i <= 40]))\n",
    "\n",
    "lm_a_0 = len([i for i in all_landmarks if i > 0])\n",
    "lmv_a_0 = len([i for i in landmarks_all if i > 0])\n",
    "lm1_0 = len([i for i in landmarks_less_1 if i > 0])\n",
    "lm5_0 = len([i for i in landmarks_less_5 if i > 0])\n",
    "lm10_0 = len([i for i in landmarks_less_10 if i > 0])\n",
    "lm40_0 = len([i for i in landmarks_less_40 if i > 0])\n",
    "\n",
    "\n",
    "len_all = len(data)\n",
    "print(f\"{lm_a_0} target have potentail landmarks or {lm_a_0/len_all}\")\n",
    "print(f\"{lmv_a_0} target have valid landmarks or {lmv_a_0/len_all}\")\n",
    "print(f\"{lm1_0} target with a landmark within 1 km or {lm1_0/len_all}\")\n",
    "print(f\"{lm5_0} target with a landmark within 5 km or {lm5_0/len_all}\")\n",
    "print(f\"{lm10_0} target with a landmark within 10 km or {lm10_0/len_all}\")\n",
    "print(f\"{lm40_0} target with a landmark within 40 km or {lm40_0/len_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = get_min_rtt_per_src_dst_query_ping_table(\n",
    "    'geolocation_replication', 'targets_to_landmarks_pings', '', 1000000)\n",
    "client = Client('127.0.0.1')\n",
    "db_table = client.execute(query)\n",
    "\n",
    "rtts = []\n",
    "remove_dict = {}\n",
    "for l in db_table:\n",
    "    rtts.append(l[2])\n",
    "    remove_dict[(l[0], l[1])] = l[2]\n",
    "\n",
    "error1 = []\n",
    "error2 = []\n",
    "error3 = []\n",
    "error4 = []\n",
    "error1ms = []\n",
    "error2ms = []\n",
    "error5ms = []\n",
    "error10ms = []\n",
    "\n",
    "for _, d in data.items():\n",
    "    errors = every_tier_result_and_errors(d)\n",
    "    error1.append(errors['error1'])\n",
    "    error2.append(errors['error2'])\n",
    "    error3.append(errors['error3'])\n",
    "    error4.append(errors['error4'])\n",
    "    err1ms = 50000\n",
    "    err2ms = 50000\n",
    "    err5ms = 50000\n",
    "    err10ms = 50000\n",
    "    for f in ['tier2:landmarks', 'tier3:landmarks']:\n",
    "        if f in d:\n",
    "            for l_ip, _, l_lat, l_lon in d[f]:\n",
    "                dist = haversine((l_lat, l_lon), (d['RIPE:lat'], d['RIPE:lon']))\n",
    "                key_rtt = (l_ip, d['target_ip'])\n",
    "                if dist < err1ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 1):\n",
    "                    err1ms = dist\n",
    "                if dist < err2ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 2):\n",
    "                    err2ms = dist\n",
    "                if dist < err5ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 5):\n",
    "                    err5ms = dist\n",
    "                if dist < err10ms and (key_rtt not in remove_dict or remove_dict[key_rtt] <= 10):\n",
    "                    err10ms = dist\n",
    "    if err1ms != 50000:\n",
    "        error1ms.append(err1ms)\n",
    "    else:\n",
    "        error1ms.append(error1[-1])\n",
    "    if err2ms != 50000:\n",
    "        error2ms.append(err2ms)\n",
    "    else:\n",
    "        error2ms.append(error1[-1])\n",
    "    if err5ms != 50000:\n",
    "        error5ms.append(err5ms)\n",
    "    else:\n",
    "        error5ms.append(error1[-1])\n",
    "    if err10ms != 50000:\n",
    "        error10ms.append(err10ms)\n",
    "    else:\n",
    "        error10ms.append(error1[-1])\n",
    "\n",
    "for i in [1, 5, 10, 40, 9999999999]:\n",
    "    c = len([j for j in error1ms if j <= i])\n",
    "    print(f\"{c} targets with landmarks (ping <= {i}) or {c/len(error1ms)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review-8XQ99qZ1-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
